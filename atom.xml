<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[程序员说]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.devtalking.com/"/>
  <updated>2018-08-27T02:38:01.918Z</updated>
  <id>http://www.devtalking.com/</id>
  
  <author>
    <name><![CDATA[DevTalking]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[机器学习笔记十七之集成学习、随机森林]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-17/"/>
    <id>http://www.devtalking.com//articles/machine-learning-17/</id>
    <published>2018-07-31T16:00:00.000Z</published>
    <updated>2018-08-27T02:38:01.918Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>到目前为止，我们已经学习了大概有八种机器学习的算法，其中有解决分类问题的，有解决回归问题的。这些算法其实没有谁是最好的，谁不好之说，反而应该将这些算法集合起来，发挥他们的最大价值。比如我们买东西或看电影之前，多少都会咨询身边的朋友，或去网上看看买家的评价，然后我们才会根据口碑好坏，或评价好坏决定买还是不买，看还是不看。在机器学习中，同样有这样的思路，这就是重要的集成学习。</p>
<h2 id="u96C6_u6210_u5B66_u4E60"><a href="#u96C6_u6210_u5B66_u4E60" class="headerlink" title="集成学习"></a>集成学习</h2><p>机器学习中的集成学习就是将选择若干算法，针对同一样本数据训练模型，然后看看结果，使用投票机制，少数服从多数，用多数算法给出的结果当作最终的决策依据，这就是集成学习的核心思路。下面我们先手动模拟一个使用集成学习解决回归问题的的示例：</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建500个点的样本数据</span></span><br><span class="line">X, y = datasets.make_moons(n_samples=<span class="number">500</span>, noise=<span class="number">0.3</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制样本数据</span></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9c07fb9e783815c1557d2a2e01163bd9.jpg" alt=""></p>
<p>分别使用逻辑回归、SVM、决策树针对上面的样本数据训练模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拆分样本数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用逻辑回归</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_clf = LogisticRegression()</span><br><span class="line">log_clf.fit(X_train, y_train)</span><br><span class="line">log_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.872</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SVM</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">svc_clf = SVC()</span><br><span class="line">svc_clf.fit(X_train, y_train)</span><br><span class="line">svc_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.89600000000000002</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用决策树</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt_clf = DecisionTreeClassifier()</span><br><span class="line">dt_clf.fit(X_train, y_train)</span><br><span class="line">dt_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.85599999999999998</span></span><br></pre></td></tr></table></figure>
<p>可以看到，使用三种不同的分类算法训练出的模型，最后的$R^2$评分都不尽相同。下面我们使用投票的方式，选择出最终预测值，具体思路是先求出三种模型对测试数据的预测结果，将三个结果向量相加，得到新的结果向量，因为分类只有0和1，所以新的结果向量里的值最大为3，最小为0。然后通过Fancy Index的方式，求出三种模型预测中至少有2种预测为1的，才真正认为是1的分类，那么也就是新结果向量里大于等于2的结果，其余小于2的都认为是0的分类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求出三种模型对测试数据的预测结果</span></span><br><span class="line">y_predict1 = log_clf.predict(X_test)</span><br><span class="line">y_predict2 = svc_clf.predict(X_test)</span><br><span class="line">y_predict3 = dt_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">y_predict = np.array((y_predict1 + y_predict2 + y_predict3) &gt;= <span class="number">2</span>, dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算最终的评分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.88800000000000001</span></span><br></pre></td></tr></table></figure>
<p>上面的示例是我们手动使用三种算法的结果通过投票方式求得了最终的决策依据。其实Scikit Learn中已经为我们封装了这种方式，名为<code>VotingClassifier</code>，既投票分类器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 引入投票分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># VotingClassifier和Pipeline的用法非常类似，这里的voting="hard"可以先忽略</span></span><br><span class="line">voting_clf = VotingClassifier(estimators=[</span><br><span class="line">	(<span class="string">"log_clf"</span>, LogisticRegression()),</span><br><span class="line">	(<span class="string">"svm_clf"</span>, SVC()),</span><br><span class="line">	(<span class="string">"dt_clf"</span>, DecisionTreeClassifier(random_state=<span class="number">666</span>))</span><br><span class="line">], voting=<span class="string">"hard"</span>)</span><br><span class="line"></span><br><span class="line">voting_clf.fit(X_train, y_train)</span><br><span class="line">voting_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.88800000000000001</span></span><br></pre></td></tr></table></figure>
<p>可以看到，使用<code>VotingClassifier</code>最后的评分和我们手动模拟的是一致的。</p>
<h2 id="Soft_Voting"><a href="#Soft_Voting" class="headerlink" title="Soft Voting"></a>Soft Voting</h2><p>在上一节中，Scikit Learn提供的<code>VotingClassifier</code>有一个参数<code>voting</code>，我们传了<code>hard</code>这个值。其实这个参数就表示投票的方式，<code>hard</code>代表的就是少数服从多数的机制。</p>
<p>但是，其实在很多时候少数服从多数得到的结果并不是正确的，这个在日常生活中其实很常见，所谓真理掌握在少数人手里就是这个意思。所以更合理的投票机制应该是对投票人加以权重值，投票人越专业，越权威，那么权重值就应该高一些。就好比歌唱比赛，评委有三类人，第一类是音乐制作人，特点是人少，但权重值高，第二类是职业歌手，人数次之，权重值也次之，第三类是普通观众，这类人人数最多，但是权重值也最低。那么决定选手去留还是掌握在少数的音乐制作人和职业歌手这些评委。这个思路其实就是Soft Voting。</p>
<p>再举一个示例，假设有5个模型，针对同一个二分类问题，将每种类别都计算出了概率：</p>
<ul>
<li>模型1 A-99%，B-1%</li>
<li>模型2 A-49%，B-51%</li>
<li>模型3 A-40%，B-60%</li>
<li>模型4 A-90%，B-10%</li>
<li>模型5 A-30%，B-70%</li>
</ul>
<p>从上面的数据，明显可以得到，整体的分类应该B，因为模型2、模型3、模型5的结论都是B，所以按照Hard Voting方式，少数服从多数，那整体的类别会定为B。</p>
<p>但是我们可以换个角度去看问题，模型1和模型4对判定为类别A的概率都在90%以上，说明非常笃定。而模型2、模型3、模型5虽然结论为类别B，但是类别A和类别B的判定概率相差并不是很大。而我们将五种模型对类别A、类别B的概率加起来就可以明显的看到，判定为类别A的总概率为：</p>
<p>$$\frac {(0.99 + 0.49 + 0.4 + 0.9 + 0.3)} 5 = 0.616$$</p>
<p>而判定为类别B的总概率为：</p>
<p>$$\frac {(0.01 + 0.51 + 0.6 + 0.1 + 0.7)} 5 = 0.384$$</p>
<p>显然判定为类别A的总概率要远高于类别B，那么整体类别应该是A。</p>
<p>以上模型判定类别的概率其实就可以理解为权重值。所以Soft Voting要求集合里的每一个模型都能估计出类别的概率。那么我们来看看我们已经了解过的机器学习算法哪些是支持概率的：</p>
<ul>
<li>逻辑回归算法本身就是基于概率模型的，通过Sigmoid函数计算概率。</li>
<li>kNN算法也是支持估计概率的，如果和预测点相邻的3个点，有2个点是红色，1个是蓝色，那么可以很容计算出红色类别的概率是$\frac 2 3$，蓝色类别的概率是$\frac 1 3$。</li>
<li>决策树算法也是支持估计概率的，它的思路和kNN的很相近，每个叶子节点中如果信息熵或基尼系数不为0，那么就肯定至少包含2种以上的类别，那么用一个类别的数量除以所有类别的数据就能得到概率。</li>
<li>SVM算法本身是不能够天然支持估计概率的。不过Scikit Learn中提供的<code>SVC</code>通过其他方式实现了估计概率的能力，代价就是增加了算法的时间复杂度和训练时间。它有一个<code>probability</code>参数，默认为<code>false</code>既不支持估计概率，如果显示传入<code>true</code>，那么就会启用估计概率的能力。</li>
</ul>
<p>下面来看看Soft Voting如何使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 还是使用上一节的数据，同样构建VotingClassifier，voiting参数传入soft。这里注意SVC要显示传入probability=True</span></span><br><span class="line">voting_clf1 = VotingClassifier(estimators=[</span><br><span class="line">	(<span class="string">"log_clf"</span>, LogisticRegression()),</span><br><span class="line">	(<span class="string">"svm_clf"</span>, SVC(probability=<span class="keyword">True</span>)),</span><br><span class="line">	(<span class="string">"dt_clf"</span>, DecisionTreeClassifier(random_state=<span class="number">666</span>))</span><br><span class="line">], voting=<span class="string">"soft"</span>)</span><br><span class="line"></span><br><span class="line">voting_clf1.fit(X_train, y_train)</span><br><span class="line">voting_clf1.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.89600000000000002</span></span><br></pre></td></tr></table></figure>
<p>可以看到Soft Voting相比较Hard Voting，预测评分是有提高的。</p>
<h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>上两节主要介绍了集成学习的原理。那么就这个原理而言，它的好坏是有一个基本的先决条件的。对于投票机制而言，5个人投票和1000个人投票得到的结果，毫无疑问是后者更具有说服力。所以前两节我们只使用了三个机器学习算法训练的模型去投票是不具有很强的说服力的。那么问题来了，我们如何能有更多的模型，来作为投票者呢？这就是这一节要说的取样问题。</p>
<p>我们知道机器学习算法是有限的，有几十个就顶破天了，所以用不同的算法这条路是行不通的，那么我们就从同一种算法的不同模型这个思路入手。基本思路就是使用一种机器学习算法，创建更多的子模型，然后集成这些子模型的意见进行投票，有个前提是子模型之间不能一致，要有差异性。这一点大家应该很好理解，模型之间的差异性越大，投票才有意义。</p>
<p>那么如何创建子模型的差异性呢？通常的做法是训练每个子模型时只看样本数据的一部分，比如一共有1000个样本数据，每个子模型只看100个样本数据，因为样本数据有差异性，所以训练出的子模型之间就自然存在差异性了。这时问题又来了，训练子模型时只这么少的样本数据，那么每个子模型的准确率自然会比较低。此时就应征了，人多力量大，一把筷子折不断的道理。</p>
<p>假设有三个子模型，每个子模型的准确率只有51%，为什么要用51%作为示例呢，因为投硬币的概率都有50%，所以比它只高一点，算是很低的准确率了。那么整体的准确率为：</p>
<p>$$0.51^3 + C_3^2 \cdot 0.51^2 \cdot 0.49 = 0.515$$</p>
<p>三个准确率为51%的子模型，可以使整体的准确率提高至51.5%。那如果是500个子模型，整体的准确率会提升至：</p>
<p>$$\sum_{i=251}^{500}C_{500}^i \cdot 0.51^i \cdot 0.49^{500-i} = 0.656$$</p>
<p>可见当子模型的数量增加时，同时会增加整体的准确率。所以其实子模型并不需要太高的准确率。</p>
<h3 id="u5B50_u6A21_u578B_u53D6_u6837_u65B9_u5F0F"><a href="#u5B50_u6A21_u578B_u53D6_u6837_u65B9_u5F0F" class="headerlink" title="子模型取样方式"></a>子模型取样方式</h3><p>子模型的取样方式有两种：</p>
<ul>
<li>放回取样：每次取完训练子模型的部分样本数据后，再放回样本数据池里，训练下一个子模型使用同样的方式。这样的方式，训练不同的子模型会可能会用到小部分相同的样本数据。</li>
<li>不放回取样：每次取完训练子模型的部分样本数据后，这部分样本数据不再放回样本数据池里，训练下一个子模型使用同样的方式。这样的方式，训练不同的子模型的样本数据不会重复。</li>
</ul>
<p>通常使用放回取样的方式更多，举个例子，假如有500个样本数据，训练子模型时使用50条数据，那么使用不放回取样只能训练出10个子模型。而如果使用放回取样的话，理论上可以训练出成千上万个子模型。在机器学习中，将取样称为Bagging。而在统计学中，放回取样称为Bootstrap。</p>
<p>下面我们用代码来看看如何使用取样的方式训练子模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 还是使用之前构造的样本数据</span></span><br><span class="line"><span class="comment"># 我们使用决策树这个算法，然后导入BaggingClassifier</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># BaggingClassifier的第一个参数是给定一个算法类型</span></span><br><span class="line"><span class="comment"># n_estimators参数是创建多少个子模型</span></span><br><span class="line"><span class="comment"># max_samples参数是每个子模型使用多少样本数据训练</span></span><br><span class="line"><span class="comment"># bootstrap为True表示为放回取样方式，为False表示为不放回取样方式</span></span><br><span class="line">bagging_clf = BaggingClassifier(</span><br><span class="line">	DecisionTreeClassifier(), n_estimators=<span class="number">500</span>, max_samples=<span class="number">100</span>, bootstrap=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">bagging_clf.fit(X_train, y_train)</span><br><span class="line">bagging_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.88</span></span><br></pre></td></tr></table></figure>
<h3 id="OOB"><a href="#OOB" class="headerlink" title="OOB"></a>OOB</h3><p>使用放回取样方式虽然可以构建更多的子模型，但是它有一个问题，那就是在有限次的放回取样过程中，有一部分样本数据可能根本没有取到，按严格的数学计算，这个比例大概是37%。这个情况称为OOB（Out of Bag）换个思路思考，这37%根本没有被用到的样本数据恰好可以作为测试数据来用，所以在使用这种方式时，我们可以不用<code>train_test_split</code>对样本数据进行拆分，直接使用没有被用到的这37%的样本数据既可。来看看<code>BaggingClassifier</code>如何使用OOB：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多加一个参数oob_score，True为使用OOB，既记住哪些样本取过，哪些没取过</span></span><br><span class="line">bagging_clf = BaggingClassifier(</span><br><span class="line">	DecisionTreeClassifier(), </span><br><span class="line">	n_estimators=<span class="number">500</span>, </span><br><span class="line">	max_samples=<span class="number">100</span>, </span><br><span class="line">	bootstrap=<span class="keyword">True</span>,</span><br><span class="line">	oob_score=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">bagging_clf.fit(X, y)</span><br><span class="line"><span class="comment"># 使用没取过样本作为测试数据</span></span><br><span class="line">bagging_clf.oob_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.91000000000000003</span></span><br></pre></td></tr></table></figure>
<p>可以看到准确率是有所提高的。</p>
<h3 id="u5E76_u53D1_u53D6_u6837"><a href="#u5E76_u53D1_u53D6_u6837" class="headerlink" title="并发取样"></a>并发取样</h3><p>按照Bagging的思路，因为不需要保证每次取样的唯一性，所以每次取样是可以并行处理的。我们可以使用<code>n_jobs</code>指定运行的CPU核数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先看看之前的训练时间</span></span><br><span class="line">%%time</span><br><span class="line">bagging_clf = BaggingClassifier(</span><br><span class="line">	DecisionTreeClassifier(), </span><br><span class="line">	n_estimators=<span class="number">500</span>, </span><br><span class="line">	max_samples=<span class="number">100</span>, </span><br><span class="line">	bootstrap=<span class="keyword">True</span>,</span><br><span class="line">	oob_score=<span class="keyword">True</span>)</span><br><span class="line">bagging_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">717</span> ms, sys: <span class="number">6.41</span> ms, total: <span class="number">724</span> ms</span><br><span class="line">Wall time: <span class="number">723</span> ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再看看加了n_jobs参数后的训练时间，n_jobs=-1表示使用电脑的所有CPU核数</span></span><br><span class="line">%%time</span><br><span class="line">bagging_clf = BaggingClassifier(</span><br><span class="line">	DecisionTreeClassifier(), </span><br><span class="line">	n_estimators=<span class="number">500</span>, </span><br><span class="line">	max_samples=<span class="number">100</span>, </span><br><span class="line">	bootstrap=<span class="keyword">True</span>,</span><br><span class="line">	oob_score=<span class="keyword">True</span>,</span><br><span class="line">	n_jobs=-<span class="number">1</span>)</span><br><span class="line">bagging_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">264</span> ms, sys: <span class="number">57.6</span> ms, total: <span class="number">322</span> ms</span><br><span class="line">Wall time: <span class="number">571</span> ms</span><br></pre></td></tr></table></figure>
<p>可以看到，训练时间缩短了150多毫秒。</p>
<blockquote>
<p>在<a href="http://www.devtalking.com/articles/machine-learning-4/"> 机器学习笔记四之kNN算法、超参数、数据归一化 </a>中的网格搜索超参数一节介绍过n_jobs参数。</p>
</blockquote>
<h3 id="u7279_u5F81_u53D6_u6837"><a href="#u7279_u5F81_u53D6_u6837" class="headerlink" title="特征取样"></a>特征取样</h3><p>我们之前讲的都是对样本数据条数进行随机取样，<code>BaggingClassifier</code>还可以对特征进行随机取样，这样更能增加子模型的差异性，称为Random Subspaces。另外还有既对样本条数取样，又针对特征随机取样的方式，称为Random Patches。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># max_features 表示随机取几个特征</span></span><br><span class="line"><span class="comment"># bootstrap_features为True表示对特征取样是放回取样方式</span></span><br><span class="line">random_subspaces_clf = BaggingClassifier(</span><br><span class="line">	DecisionTreeClassifier(), </span><br><span class="line">	n_estimators=<span class="number">500</span>, </span><br><span class="line">	max_samples=<span class="number">500</span>, </span><br><span class="line">	bootstrap=<span class="keyword">True</span>,</span><br><span class="line">	oob_score=<span class="keyword">True</span>,</span><br><span class="line">	n_jobs=-<span class="number">1</span>,</span><br><span class="line">	max_features=<span class="number">1</span>,</span><br><span class="line">	bootstrap_features=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">random_subspaces_clf.fit(X, y)</span><br><span class="line">random_subspaces_clf.oob_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.82999999999999996</span></span><br></pre></td></tr></table></figure>
<p>首先将<code>max_samples</code>设置为500，意在取消对样本数据条数随机取样，因为一共有500个样本数据，要创建500个子模型，如果每个子模型都使用500个样本数据，那相当于对样本条数取样是没有意义的。又因为我们的样本特征只有2个，所以<code>max_features</code>设置为1。</p>
<p>如果将<code>max_samples</code>设回100的话，那就是既对样本条数随机取样，又对特征随机取样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_patches_clf = BaggingClassifier(</span><br><span class="line">	DecisionTreeClassifier(), </span><br><span class="line">	n_estimators=<span class="number">100</span>, </span><br><span class="line">	max_samples=<span class="number">500</span>, </span><br><span class="line">	bootstrap=<span class="keyword">True</span>,</span><br><span class="line">	oob_score=<span class="keyword">True</span>,</span><br><span class="line">	n_jobs=-<span class="number">1</span>,</span><br><span class="line">	max_features=<span class="number">1</span>,</span><br><span class="line">	bootstrap_features=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">random_patches_clf.fit(X, y)</span><br><span class="line">random_patches_clf.oob_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.79400000000000004</span></span><br></pre></td></tr></table></figure>
<h2 id="u968F_u673A_u68EE_u6797"><a href="#u968F_u673A_u68EE_u6797" class="headerlink" title="随机森林"></a>随机森林</h2><p>前面几个章节介绍了集成学习的原理。在集成学习中，如果使用决策树，通过取样的方式创建子模型，这些子模型就是一个个随机的决策树。我们管这种方式形象的称为随机森林。在Scikit Learn中，也为我们封装好了随机森林的类，它的原理和上一小节示例中通过<code>BaggingClassifier</code>和<code>DecisionTreeClassifier</code>构建的分类器基本是一样的。我们来看看如何使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rf_clf = RandomForestClassifier(n_estimators=<span class="number">500</span>, oob_score=<span class="keyword">True</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还是使用之前构建的样本数据</span></span><br><span class="line">rf_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">RandomForestClassifier(bootstrap=<span class="keyword">True</span>, class_weight=<span class="keyword">None</span>, criterion=<span class="string">'gini'</span>,</span><br><span class="line">			max_depth=<span class="keyword">None</span>, max_features=<span class="string">'auto'</span>, max_leaf_nodes=<span class="keyword">None</span>,</span><br><span class="line">			min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="keyword">None</span>,</span><br><span class="line">			min_samples_leaf=<span class="number">1</span>, min_samples_split=<span class="number">2</span>,</span><br><span class="line">			min_weight_fraction_leaf=<span class="number">0.0</span>, n_estimators=<span class="number">500</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">			oob_score=<span class="keyword">True</span>, random_state=<span class="number">666</span>, verbose=<span class="number">0</span>, warm_start=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">rf_clf.oob_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.89200000000000002</span></span><br></pre></td></tr></table></figure>
<p><code>fit</code>之后，从返回结果里可以看到，<code>RandomForestClassifier</code>的参数综合了<code>BaggingClassifier</code>及<code>DecisionTreeClassifier</code>的参数。我们可以对不同的参数进行调优，训练出更好的模型。</p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>我们之前介绍的集成学习中子模型之间是相互独立的，差异越大越好。那么集成学习中还有一种创建子模型的方式，就是每个子模型之间有关联，都在尝试增强整体的效果。这种方式称为Boosting方式。</p>
<h3 id="Ada_Boosting"><a href="#Ada_Boosting" class="headerlink" title="Ada Boosting"></a>Ada Boosting</h3><p>在Boosting方式中，有一种方式称为Ada Boosting，我们用网络上的一幅解决回归问题的图来做以解释：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/dc245dddc9e81012b48a1edc4ef88f9e.jpg" alt=""></p>
<p>我们先来看第一个齿轮上下连接的图，下面的图是原始样本数据，每个样本点的权重值都是一样的，上面的图是第一个子模型预测出的结果，那势必会有没有被准确预测的样本点，将这些样本点的权重值加大。</p>
<p>第二列下图中展示的深色点就是权重值增大的样本点，浅色点是上一个子模型预测出的样本点。那么训练第二个子模型时会优先考虑权重大的样本点进行拟合，拟合出的结果如第二列上图所示。</p>
<p>然后再将第二个子模型没有预测出的样本点的权重值增大，如第三列下图所示，在训练第三个子模型时优先考虑第二个子模型没有预测出的样本点进行拟合。以此类推，这样就可以训练出很多子模型，不同于取样方式，Boosting方式的所有子模型使用全量的样本数据进行训练，不过样本数据有权重值的概念，而且后一个子模型是在完善上一个子模型的错误，从而所有子模型达到增强整体的作用。这就是Ada Boosting的原理。</p>
<p>下面来看看Scikit Learn为我们提供的<code>AdaBoostClassifier</code>如何使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=<span class="number">2</span>), n_estimators=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boosting方式没有OOB的概念，所以还是需要使用拆分后的样本数据</span></span><br><span class="line">ada_clf.fit(X_train, y_train)</span><br><span class="line">ada_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.86399999999999999</span></span><br></pre></td></tr></table></figure>
<h3 id="Gradient_Boosting"><a href="#Gradient_Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h3><p>还有一种Boosting的方式称为Gradient Boosting。它的原理是我们训练第一个子模型M1，它肯定会有没有准确预测到的样本，我们称为错误E1。然后我们将E1这些样本点作为训练第二个子模型的样本数据，训练出第二个子模型M2，然后它必然还会产生错误E2。那么再将E2作为训练第三个子模型的样本数据，产生错误E3，以此类推，训练出多个子模型。最终预测的结果是M1+M2+M3+…的结果。</p>
<p>下面来看看Scikit Learn为我们提供的<code>GradientBoostingClassifier</code>如何使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># GradientBoostingClassifier本身就是使用决策树作为算法实现的，所以不再需要传入算法实例</span></span><br><span class="line">gd_clf = GradientBoostingClassifier(max_depth=<span class="number">2</span>, n_estimators=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boosting方式没有OOB的概念，所以还是需要使用拆分后的样本数据</span></span><br><span class="line">gd_clf.fit(X_train, y_train)</span><br><span class="line">gd_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.90400000000000003</span></span><br></pre></td></tr></table></figure>
<h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>这一小节我们再来认识一个集成学习创建子模型的思路，Stacking。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/906458322308a046b8ed9a5054bea0ee.jpg" alt=""></p>
<p>上图也是网络上的一幅图，我们先看中间那层，Subset1和Subset2是将原始样本数据分成两部分后的数据，我们先使用Subset1训练出三个子模型，这三个子模型会产生错误，既没有预测到的样本数据。然后将这三个子模型的三个错误结果和Subset2组成新的样本数据，训练出第四个子模型。整体的预测结果以第四个子模型的结果为准。这就是Stacking的基本原理，通过Stacking方式可以构建出比较复杂的子模型关系网：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/5694b519f162aa165a3c99ba2b0d8e39.jpg" alt=""></p>
<p>上图有三层，一共7个子模型，就需要将原始样本数据分成三份，第一份作为训练第一层三个子模型的样本数据，第二份作为训练第二层子模型的样本数据其中一部分，以此类推。</p>
<p>不过在Scikit Learn中没有提供任何Stacking的类供我们使用，Stacking的原理已经有神经网络的雏形了，里面涉及到的调参环节非常多，大家有兴趣可以自己尝试实现Stacking算法。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>到目前为止，我们已经学习了大概有八种机器学习的算法，其中有解决分类问题的，有解决回归问题的。这些算法其实没有谁是最好的，谁不好之说，反而应该将这些算法集合起来，发挥他们的最大价值。比如我们买东西或看电影之前，多少都会咨询身边的朋友，或去网上看看买家的评价，然后我们才会根据口碑好坏，或评价好坏决定买还是不买，看还是不看。在机器学习中，同样有这样的思路，这就是重要的集成学习。</p>
<h2 id="u96C6_u6210_u5B66_u4E60"><a href="#u96C6_u6210_u5B66_u4E60" class="headerlink" title="集成学习"></a>集成学习</h2><p>机器学习中的集成学习就是将选择若干算法，针对同一样本数据训练模型，然后看看结果，使用投票机制，少数服从多数，用多数算法给出的结果当作最终的决策依据，这就是集成学习的核心思路。下面我们先手动模拟一个使用集成学习解决回归问题的的示例：</p>]]>
    
    </summary>
    
      <category term="Bagging" scheme="http://www.devtalking.com/tags/Bagging/"/>
    
      <category term="Boosting" scheme="http://www.devtalking.com/tags/Boosting/"/>
    
      <category term="Stacking" scheme="http://www.devtalking.com/tags/Stacking/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="随机森林" scheme="http://www.devtalking.com/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
      <category term="集成学习" scheme="http://www.devtalking.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十六之基尼系数、CART]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-16/"/>
    <id>http://www.devtalking.com//articles/machine-learning-16/</id>
    <published>2018-07-14T16:00:00.000Z</published>
    <updated>2018-08-27T02:37:31.384Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记我们来看看决策树的另一种划分方式基尼系数和决策树中的超参数，以及决策树的缺陷。</p>
<h2 id="u57FA_u5C3C_u7CFB_u6570"><a href="#u57FA_u5C3C_u7CFB_u6570" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>在一开始我们使用Scikit Learn中提供的决策树分类器时，<code>DecisionTreeClassifier</code>有一个参数<code>criterion</code>，我们之前传入了<code>entropy</code>，也就是表示此时决策树使用信息熵方式。由此可知，决策树应该不止信息熵一种方式，所以这一节来看看决策树的另一种方式，基尼系数。</p>
<p>其实基尼系数和信息熵的思路基本是一致的，只是判定数据随机性度量的公式不一样，那么基尼系数的公式为：</p>
<p>$$G = 1 - \sum_{i=1}^k p_i^2$$</p>
<p>同样用之前的例子代入公式看一下：</p>
<p>$$\{\frac 1 3,\frac 1 3,\frac 1 3\}$$</p>
<p>将上面的数据类别占比信息代入公式后可得：</p>
<p>$$G = 1-(\frac 1 3)^2-(\frac 1 3)^2-(\frac 1 3)^2=0.6666$$</p>
<p>再换一组数据类别占比信息：</p>
<p>$$\{\frac 1 {10},\frac 2 {10},\frac 7 {10}\}$$</p>
<p>代入公式后可得：</p>
<p>$$G = 1-(\frac 1 {10})^2-(\frac 2 {10})^2-(\frac 7 {10})^2=0.46$$</p>
<p>可见基尼系数同样反应了数据不确定性的度量。这里就不再使用代码对基尼系数的方式进行模拟了，其实只需要将信息熵的公式换成基尼系数的既可。</p>
<a id="more"></a>
<h2 id="u4FE1_u606F_u71B5_u548C_u57FA_u5C3C_u7CFB_u6570_u7684_u6BD4_u8F83"><a href="#u4FE1_u606F_u71B5_u548C_u57FA_u5C3C_u7CFB_u6570_u7684_u6BD4_u8F83" class="headerlink" title="信息熵和基尼系数的比较"></a>信息熵和基尼系数的比较</h2><p>信息熵和基尼系数都是决策树中根节点划分的依据，本质上这两种方式没有太大的差别，具体的比较在这列一下：</p>
<ul>
<li>信息熵的计算比基尼系数的稍慢一些，因为信息熵的公式里是要求$log$的，而基尼系数公式中只是平方求和而已。</li>
<li>Scikit Learn中的决策树默认使用基尼系数方式，所以当我们不传入<code>criterion</code>参数时，默认使用<code>gini</code>方式。</li>
<li>信息熵和基尼系数没有特别的效果优劣。只是大家需要了解决策树根节点划分的方式原理。</li>
</ul>
<h2 id="u51B3_u7B56_u6811_u4E2D_u7684_u8D85_u53C2_u6570"><a href="#u51B3_u7B56_u6811_u4E2D_u7684_u8D85_u53C2_u6570" class="headerlink" title="决策树中的超参数"></a>决策树中的超参数</h2><p>在信息熵那一节中，我们使用代码模拟了决策树的根节点划分过程，从中可以可知道决策树最极端的就是每个叶子节点的不确定性都为0，也就是每个叶子节点都是包含一种类别的数据。这样一来虽然对样本数据的分类准确度非常高，但是却是典型的过拟合情况，或者说模型的泛化能力非常差。</p>
<p>另外一点是如果做到极端情况，那么模型训练过程的时间复杂度也会非常高，达到了$O(n<em>m</em>logm)$，$m$是样本数据行数，$n$是样本数据特征数。</p>
<p>综上，我们期望训练出的模型泛化能力要好，并且训练时间复杂度要适中，所以关键就是要对决策树剪枝，从而降低复杂度，解决过拟合问题。这就需要用到决策树的超参数。</p>
<p>在之前我们使用Scikit Learn中的决策树时用到了一个参数<code>max_depth</code>，既决策树深度，就是限定了决策数的层数，这就是一种可以剪枝的超参数。除此之外，还有一些超参数可以达到剪枝作用，我们一一来看一看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line">X, y = datasets.make_moons(noise=<span class="number">0.25</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制样本数据</span></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/bd5cc9311e40f765b64194f49538ceb3.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不传入任何参数，既criterion默认使用gini，决策树深度划分到不能划分为止</span></span><br><span class="line">dt_clf = DecisionTreeClassifier()</span><br><span class="line">dt_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># meshgrid函数用两个坐标轴上的点在平面上画格，返回坐标矩阵</span></span><br><span class="line">	X0, X1 = np.meshgrid(</span><br><span class="line">		<span class="comment"># 随机两组数，起始值和密度由坐标轴的起始值决定</span></span><br><span class="line">		np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line">	<span class="comment"># ravel()方法将高维数组降为一维数组，c_[]将两个数组以列的形式拼接起来，形成矩阵</span></span><br><span class="line">	X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 通过训练好的逻辑回归模型，预测平面上这些点的分类</span></span><br><span class="line">	y_predict = model.predict(X_grid_matrix)</span><br><span class="line">	y_predict_matrix = y_predict.reshape(X0.shape)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 设置色彩表</span></span><br><span class="line">	<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">	my_colormap = ListedColormap([<span class="string">'#EF9A9A'</span>, <span class="string">'#40E0D0'</span>, <span class="string">'#FFFF00'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 绘制等高线，并且填充等高区域的颜色</span></span><br><span class="line">	plt.contourf(X0, X1, y_predict_matrix, linewidth=<span class="number">5</span>, cmap=my_colormap)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plot_decision_boundary(dt_clf, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d39713171c7b11bb167b07a5f6b6be9f.jpg" alt=""></p>
<p>从上图的决策边界可以看出，模型是明显处于过拟合的状态，既能为了个别点就开辟出分界区域，泛化能力是很差的。下面当我们对决策树的深度做限制后看看效果是怎样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 限定决策树深度为2</span></span><br><span class="line">dt_clf2 = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span><br><span class="line">dt_clf2.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plot_decision_boundary(dt_clf2, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/531a8cefcbfb14d809ff7ec71cd31ce8.jpg" alt=""></p>
<p>可以看到当限定了决策树深度后，模型的过拟合情况明显得到了改善，泛化能力有明显提高。下面再来看看其他几个超参数。</p>
<p>我们可以指定每个节点当它至少有多少个数据时才继续拆分下去：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每个节点至少有10个数据时才会继续拆分下去</span></span><br><span class="line">dt_clf3 = DecisionTreeClassifier(min_samples_split=<span class="number">10</span>)</span><br><span class="line">dt_clf3.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plot_decision_boundary(dt_clf3, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/64bb0b9e97043f2a2f8433575ce973cd.jpg" alt=""></p>
<p>我们还可以指定划分后决策树每个叶子节点至少要多少个样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每个叶子节点至少要有6个样本数据</span></span><br><span class="line">dt_clf4 = DecisionTreeClassifier(min_samples_leaf=<span class="number">6</span>)</span><br><span class="line">dt_clf4.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plot_decision_boundary(dt_clf4, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/6f85167f913255c7f76e81c1bb017865.jpg" alt=""></p>
<p>对于一棵决策树而言，叶子节点越多，决策树肯定约复杂。所以我们也可以指定最大叶子节点个数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 决策树的最大叶子节点个数为4</span></span><br><span class="line">dt_clf5 = DecisionTreeClassifier(max_leaf_nodes=<span class="number">4</span>)</span><br><span class="line">dt_clf5.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plot_decision_boundary(dt_clf5, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8b12c5d67fef11675afec389a5543297.jpg" alt=""></p>
<p>决策树还有许多超参数，这里我们只是举了几个例子，大家可以去Scikit Learn官网去看看<a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" target="_blank" rel="external">DecisionTreeClassifier</a>的其他超参数，然后使用网格搜索方式选出最优超参数的组合，使得模型达到最优效果。</p>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART是Classification And Regression Tree的缩写，字面意思已经很明确了，分类和回归树，说明我们这篇笔记中介绍的基于信息熵和基尼系数划分方式的决策树既可以解决分类问题，还可以解决回归问题。Scikit Learn中封装的决策树也是CART，当然还有一些其他实现方式的决策树，大家有兴趣可以在网上查阅ID3，C4.5，C5.0等实现方式的决策树。</p>
<p>下面我们来看看Scikit Learn中封装的解决回归问题的决策树：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用波士顿房价数据</span></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">dt_reg = DecisionTreeRegressor()</span><br><span class="line">dt_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">dt_reg.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.59192423637607194</span></span><br></pre></td></tr></table></figure>
<p>可以看到Scikit Learn中提供了决策树回归器<code>DecisionTreeRegressor</code>，虽然模型的评分不是很高，但是可以通过超参数来调节，提升评分。<code>DecisionTreeRegressor</code>和超参数和<code>DecisionTreeClassifier</code>的超参数基本是一样的。</p>
<h2 id="u51B3_u7B56_u6811_u7684_u7F3A_u9677"><a href="#u51B3_u7B56_u6811_u7684_u7F3A_u9677" class="headerlink" title="决策树的缺陷"></a>决策树的缺陷</h2><p>这一小节来看看决策树存在的两个缺陷。</p>
<h3 id="u51B3_u7B56_u8FB9_u754C_u7684_u5C40_u9650_u6027"><a href="#u51B3_u7B56_u8FB9_u754C_u7684_u5C40_u9650_u6027" class="headerlink" title="决策边界的局限性"></a>决策边界的局限性</h3><p>因为决策树的根节点，也就是划分节点的判断条件都是在某个维度，判断小于或大于某个阈值，所以每个根节点的决策边界必然都是平行于某个维度的，对于二维数据来说，决策边界不是平行于横轴就是平行于纵轴的。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b80637514c0594b18bdf840a331ed7eb.jpg" alt=""></p>
<p>比如上图显示的示例，如果使用逻辑回归训练模型，绘制出的决策边界应该是上图中的斜虚线。但如果是使用决策树训练模型，那么绘制出的决策边界应该首先是从中间进行划分：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/02172dbea2507b7d0abf5f5107c86dfc.jpg" alt=""></p>
<p>此时上半部分的信息熵为0，因全部是蓝色点，然后会从中间再分一下：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ea52f77ca640de25f81e9c5f2030be33.jpg" alt=""></p>
<p>所以最终决策边界为：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/7d071b1897316f224733e8d528fa03da.jpg" alt=""></p>
<p>那么这样的横平竖直的决策边界有什么局限性呢？我们再来看一个示例：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ad4993998fc46677ffbfbba0b6e485a8.jpg" alt=""></p>
<p>如果是上图显示的样本数据，使用决策树训练的模型绘制出的决策边界是一个阶梯状，中间的阶梯部分划分倒是问题不大，但问题出在两头于横轴平行的部分。如果我们再来一个样本点，看看会被划分到哪一类：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/92145d7d0e9fb92d40efe2a4f684710f.jpg" alt=""></p>
<p>假如新来一个样本点A，它是蓝色点，但是按照决策树的决策边界就被归为了红色点。但如果是逻辑回归模型绘制出的决策边界，点A就能被正确的分类：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/db2d20d13c6768910a3a3e90dcff0413.jpg" alt=""></p>
<p>这就是决策树决策边界的局限性所在。</p>
<h3 id="u5BF9_u6570_u636E_u654F_u611F"><a href="#u5BF9_u6570_u636E_u654F_u611F" class="headerlink" title="对数据敏感"></a>对数据敏感</h3><p>决策树的另一个缺陷是对样本数据中的个别数据非常敏感，这个敏感体现在如果对样本数据进行少许改动，决策边界都会发生巨大改变。我们来举例看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还是使用鸢尾花数据</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt_clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>, criterion=<span class="string">"entropy"</span>)</span><br><span class="line">dt_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(dt_clf, axis=[<span class="number">0.5</span>, <span class="number">7.5</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/88a63811729dd486eb8498ca3dd53020.jpg" alt=""></p>
<p>然后我们删除一个鸢尾花数据，再来看看决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除样本数据中索引为138的数据</span></span><br><span class="line">X_new = np.delete(X, <span class="number">138</span>, axis=<span class="number">0</span>)</span><br><span class="line">y_new = np.delete(y, <span class="number">138</span>)</span><br><span class="line"></span><br><span class="line">dt_clf2 = DecisionTreeClassifier(max_depth=<span class="number">2</span>, criterion=<span class="string">"entropy"</span>)</span><br><span class="line">dt_clf2.fit(X_new, y_new)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(dt_clf2, axis=[<span class="number">0.5</span>, <span class="number">7.5</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9b2316f2e266fc14fbe65c230e781766.jpg" alt=""></p>
<p>可以看到当删除了一个数据后，决策边界发生了巨大的变化。不过对于样本数据中的个别数据很敏感这也是非参数机器学习算法的通病，都比较依赖于调参，才能得到一个比较好的模型。</p>
<p>但是决策树的更多的体现在集成学习中，在下一篇笔记中大家将会看到决策树在集成学习和随机森林里的作用。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记我们来看看决策树的另一种划分方式基尼系数和决策树中的超参数，以及决策树的缺陷。</p>
<h2 id="u57FA_u5C3C_u7CFB_u6570"><a href="#u57FA_u5C3C_u7CFB_u6570" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>在一开始我们使用Scikit Learn中提供的决策树分类器时，<code>DecisionTreeClassifier</code>有一个参数<code>criterion</code>，我们之前传入了<code>entropy</code>，也就是表示此时决策树使用信息熵方式。由此可知，决策树应该不止信息熵一种方式，所以这一节来看看决策树的另一种方式，基尼系数。</p>
<p>其实基尼系数和信息熵的思路基本是一致的，只是判定数据随机性度量的公式不一样，那么基尼系数的公式为：</p>
<p>$$G = 1 - \sum_{i=1}^k p_i^2$$</p>
<p>同样用之前的例子代入公式看一下：</p>
<p>$$\{\frac 1 3,\frac 1 3,\frac 1 3\}$$</p>
<p>将上面的数据类别占比信息代入公式后可得：</p>
<p>$$G = 1-(\frac 1 3)^2-(\frac 1 3)^2-(\frac 1 3)^2=0.6666$$</p>
<p>再换一组数据类别占比信息：</p>
<p>$$\{\frac 1 {10},\frac 2 {10},\frac 7 {10}\}$$</p>
<p>代入公式后可得：</p>
<p>$$G = 1-(\frac 1 {10})^2-(\frac 2 {10})^2-(\frac 7 {10})^2=0.46$$</p>
<p>可见基尼系数同样反应了数据不确定性的度量。这里就不再使用代码对基尼系数的方式进行模拟了，其实只需要将信息熵的公式换成基尼系数的既可。</p>]]>
    
    </summary>
    
      <category term="基尼系数" scheme="http://www.devtalking.com/tags/%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十五之决策树、信息熵]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-15/"/>
    <id>http://www.devtalking.com//articles/machine-learning-15/</id>
    <published>2018-06-29T16:00:00.000Z</published>
    <updated>2018-08-27T02:37:08.085Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记我们来看看机器学习中一个重要的非参数学习算法，决策树。</p>
<h2 id="u4EC0_u4E48_u662F_u51B3_u7B56_u6811"><a href="#u4EC0_u4E48_u662F_u51B3_u7B56_u6811" class="headerlink" title="什么是决策树"></a>什么是决策树</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/84205d722c59ef40b62804d2672f9435.jpg" alt=""></p>
<p>上图是一个向银行申请信用卡的示例，图中的树状图展示了申请人需要在银行过几道关卡后才能成功申请到一张信用卡的流程图。在图中树状图的根节点是申请人输入的信息，叶子节点是银行作出的决策，也就相当于是对申请者输入信息作出的分类决策。从第一个根节点到最后一个叶子节点经过的根节点数量称为树状图的深度（depth）。上图示例中的树状图从第一个根节点<strong>申请人是否办理过信用卡</strong>，到最后一个<strong>发放信用卡</strong>叶子节点共经过了三个根节点，所以深度为3。那么像这样使用树状图对输入信息一步步分类的方式就称为决策树方式。</p>
<p>我们再来看一个问题，上图中每一个根节点的输入信息都可以用<strong>是</strong>或<strong>否</strong>来做判断分类，但是机器学习的样本数据都是数字，那么此时如果做判断呢？我们先来使用Scikit Learn中提供的决策树直观的看一下通过决策树对样本数据的分类过程和分类结果。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们使用Scikit Learn提供的鸢尾花数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了绘图方便，我们只使用鸢尾花数据的后两个特征</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将样本点绘制出来</span></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8ba5000e2b864692e02f12e153e1d220.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 引入Scikit Learn中的决策树分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># max_depth参数是决策树深度，criterion参数这里我们先不用管</span></span><br><span class="line">dt_clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>, criterion=<span class="string">"entropy"</span>)</span><br><span class="line">dt_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># meshgrid函数用两个坐标轴上的点在平面上画格，返回坐标矩阵</span></span><br><span class="line">	X0, X1 = np.meshgrid(</span><br><span class="line">		<span class="comment"># 随机两组数，起始值和密度由坐标轴的起始值决定</span></span><br><span class="line">		np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line">	<span class="comment"># ravel()方法将高维数组降为一维数组，c_[]将两个数组以列的形式拼接起来，形成矩阵</span></span><br><span class="line">	X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 通过训练好的逻辑回归模型，预测平面上这些点的分类</span></span><br><span class="line">	y_predict = model.predict(X_grid_matrix)</span><br><span class="line">	y_predict_matrix = y_predict.reshape(X0.shape)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 设置色彩表</span></span><br><span class="line">	<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">	my_colormap = ListedColormap([<span class="string">'#EF9A9A'</span>, <span class="string">'#40E0D0'</span>, <span class="string">'#FFFF00'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 绘制等高线，并且填充等高区域的颜色</span></span><br><span class="line">	plt.contourf(X0, X1, y_predict_matrix, linewidth=<span class="number">5</span>, cmap=my_colormap)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(dt_clf, axis=[<span class="number">0.5</span>, <span class="number">7.5</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/88a63811729dd486eb8498ca3dd53020.jpg" alt=""></p>
<p>从决策边界图中可以看到决策树分类器将鸢尾花的数据较好的进行的归类区分。那么再回到刚才的问题，那就是在对样本数据进行决策树分类时，根节点是怎么做判断的呢？</p>
<p>我们仔细看上图，假设我们将横轴表示的特征记为$x$，纵轴表示的特征记为$y$，那么红色的区域就是$x&lt;2.4$的区域，所以第一个根节点就直接可定义为$x$是否小于2.4，既样本数据的某一个特征数值是否小于2.4，如果小于2.4，那么将该样本点判定为蓝色分类的点。</p>
<p>然后再来看纵轴，观察可得，$y=1.8$是蓝色和黄色区域的分界点，所以可将$y&lt;1.8$作为第二层的根节点，如果样本数据的另一个特征数值小于1.8，则判定为黄色分类的点，大于1.8则判定为绿色分类的点。那么此时的决策树为：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/85a1b8f21073e15f99d2a2f015762392.jpg" alt=""></p>
<p>也就是决策树在对样本数据分类时，会先选定一个维度，或者说一个特征，再选定和这个维度想对应阈值构成一个根节点，既判断条件。</p>
<p>通过上面的示例，大家应该什么是决策树有了直观的了解，并且也能看出来决策树的一些特点：</p>
<ul>
<li>决策树算法是非参数学习算法。</li>
<li>决策树算法可以解决分类问题。</li>
<li>决策树算法可以天然解决多分类问题。</li>
<li>决策树算法处理样本数据的结果具有非常好的可解释性。</li>
</ul>
<p>了解了决策树后，那么问题来了，对于决策树来说核心的工作是确定根节点，那么这个根节点该如何确定呢？在哪个维度做划分？某个维度在哪个值上做划分？我们往后看。</p>
<h2 id="u4FE1_u606F_u71B5"><a href="#u4FE1_u606F_u71B5" class="headerlink" title="信息熵"></a>信息熵</h2><p>这一节我们先来看看什么是信息熵。其实信息熵的概念很简单，<strong>熵在信息论中代表随机变量不确定的度量</strong>。</p>
<ul>
<li>熵越大，数据的不确定性越高。</li>
<li>熵越小，数据的不确定性越低。</li>
</ul>
<h3 id="u4FE1_u606F_u71B5_u7684_u516C_u5F0F"><a href="#u4FE1_u606F_u71B5_u7684_u516C_u5F0F" class="headerlink" title="信息熵的公式"></a>信息熵的公式</h3><p>上面解释了信息熵的定义，那么这一小节我们使用信息熵的公式来解释它的定义。</p>
<p>$$H = - \sum_{i=1}^k p_i log(p_i)$$</p>
<p>上面的公式就是香农提出的信息熵的公式。逐个解释一下：</p>
<ul>
<li>假如一组数据有k类信息，那么每一个信息所占的比例就是$p_i$。比如鸢尾花数据包含三种鸢尾花的数据，那么每种鸢尾花所占的比例就是$\frac 1 3$，那么$p_1$、$p_2$、$p_3$就分别为$\frac 1 3$。</li>
<li>因为$p_i$只可能是小于1的，所以$log(p_i)$始终是负数。所以需要在公式最前面加负号，让整个熵的值大于0。</li>
</ul>
<p>我们来举几个例子看一下，首先用鸢尾花的例子，三种鸢尾花各占$\frac 1 3$：</p>
<p>$$\{\frac 1 3,\frac 1 3,\frac 1 3\}$$</p>
<p>那么代入信息熵的公式可得：</p>
<p>$$H=-\frac 1 3 log(\frac 1 3)-\frac 1 3log(\frac 1 3)-\frac 1 3log(\frac 1 3)=1.0986$$</p>
<p>再来看一个例子：</p>
<p>$$\{\frac 1 {10},\frac 2 {10},\frac 7 {10}\}$$</p>
<p>代入公式可得：</p>
<p>$$H=-\frac 1 {10} log(\frac 1 {10})-\frac 2 {10}log(\frac 2 {10})-\frac 7 {10}log(\frac 7 {10})=0.8018$$</p>
<p>从上面两个例子可以看出，第二个例子的信息熵比一个例子的小，那么意味着第二个示例的数据不确定性要低于第一个示例的数据。其实从数据中也能看出，其中有一类信息占全部信息的$\frac 7 {10}$，所以大多数据是能确定在某一类中的，故而不确定性低。而第一个示例中每类信息都占了全部信息的$\frac 1 3$，所以数据不能很明确的确定是哪类，故而不确定性高。</p>
<p>再来看一个极端的例子，$\{1,0,0\}$，将其代入信息熵公式后得到的值是0。因为整个数据中就一种类型的数据，所以不确定性达到了最低，既信息熵的最小值为0。</p>
<h3 id="u4FE1_u606F_u71B5_u66F2_u7EBF"><a href="#u4FE1_u606F_u71B5_u66F2_u7EBF" class="headerlink" title="信息熵曲线"></a>信息熵曲线</h3><p>这一小节我们在Jupyter Notebook中将信息熵的曲线绘制出来再让大家感性的理解一下。假设一组信息中有两个类别，那么当一个类别所占比例为$x$时，另一个类别的所占比例肯定是$1-x$，将其代入信息熵的公式展开后可得：</p>
<p>$$H=-xlog(x)-(1-x)log(1-x)$$</p>
<p>下面我们用代码来绘制一下这个曲线：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(p)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> -p * np.log(p) - (<span class="number">1</span>-p) * np.log(<span class="number">1</span>-p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建不同的比例，这里避免log(0)的计算，所以要避免p=0和1-p=0</span></span><br><span class="line">x = np.linspace(<span class="number">0.01</span>, <span class="number">0.99</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将不同比例和不同信息熵的值绘制出来</span></span><br><span class="line">plt.plot(x, entropy(x))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/662595f933f20b24d3dbefb277f3cae3.jpg" alt=""></p>
<p>从图中可以看到，曲线是以横轴0.5为中心的抛物线。当比例$x$为0.5时，纵轴达到最大值，也就是信息熵达到了最大值，表示此时变量不确定性最大。因为两个类别各以0.5的比例出现，很难确定是哪个类型，所以不确定性大。</p>
<p>当以0.5为中心向两边延展后，可以看到信息熵在逐渐减小，意味着不确定性在减小。因为此时要么有一个比例在减小，要么有一个比例在增大。这两种情况都可以表明其中一种类别的比例在增大，所以更容易确定是哪种类别，故而不确定性减小。</p>
<h2 id="u4F7F_u7528_u4FE1_u606F_u71B5_u5BFB_u627E_u6700_u4F18_u5212_u5206"><a href="#u4F7F_u7528_u4FE1_u606F_u71B5_u5BFB_u627E_u6700_u4F18_u5212_u5206" class="headerlink" title="使用信息熵寻找最优划分"></a>使用信息熵寻找最优划分</h2><p>之前我们有带着两个问题进入到了信息熵小节。那么这一节就来解答这两个问题：</p>
<ul>
<li>决策树每个节点在哪个维度做划分？</li>
<li>某个维度在哪个值上做划分？</li>
</ul>
<p>那么我们要做的事情就是找到一个维度和一个阈值，使得通过该维度和阈值划分后的信息熵最低，此时这个划分才是最好的划分。</p>
<p>用大白话解释一下就是，我们在所有数据中寻找到信息熵最低的维度和阈值，然后将数据划分为多个部分，再寻找划分后每部分信息熵最低的维度和阈值，继续划分下去，最终形成完整的决策树。这一节就来看看如何使用信息熵寻找最优划分。</p>
<h3 id="u5212_u5206_u51FD_u6570"><a href="#u5212_u5206_u51FD_u6570" class="headerlink" title="划分函数"></a>划分函数</h3><p>我们先定义一个划分函数，也就是相当于构建决策树根节点的作用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 四个参数分别为样本特征数据、样本目标数据、维度、阈值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(X, y, d, v)</span>:</span></span><br><span class="line">	<span class="comment"># 划分后左侧和右侧的索引数组</span></span><br><span class="line">	index_l = (X[:, d] &lt;= v)</span><br><span class="line">	index_r = (X[:, d] &gt; v)</span><br><span class="line">	<span class="keyword">return</span> X[index_l], X[index_r], y[index_l], y[index_r]</span><br></pre></td></tr></table></figure>
<p>举个例子来看看上面的划分函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构建一个五行四列的样本数据</span></span><br><span class="line">X = np.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">X = X.reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">y = <span class="number">2</span> * X + <span class="number">3</span></span><br><span class="line"></span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[  <span class="number">1.</span>        ,   <span class="number">1.47368421</span>,   <span class="number">1.94736842</span>,   <span class="number">2.42105263</span>],</span><br><span class="line">	   [  <span class="number">2.89473684</span>,   <span class="number">3.36842105</span>,   <span class="number">3.84210526</span>,   <span class="number">4.31578947</span>],</span><br><span class="line">	   [  <span class="number">4.78947368</span>,   <span class="number">5.26315789</span>,   <span class="number">5.73684211</span>,   <span class="number">6.21052632</span>],</span><br><span class="line">	   [  <span class="number">6.68421053</span>,   <span class="number">7.15789474</span>,   <span class="number">7.63157895</span>,   <span class="number">8.10526316</span>],</span><br><span class="line">	   [  <span class="number">8.57894737</span>,   <span class="number">9.05263158</span>,   <span class="number">9.52631579</span>,  <span class="number">10.</span>        ]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 期望以第1个维度，既第1列特征为划分维度，以5为划分阈值</span></span><br><span class="line">X_l, X_r, y_l, y_r = split(X, y, <span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">X_l</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">1.</span>        ,  <span class="number">1.47368421</span>,  <span class="number">1.94736842</span>,  <span class="number">2.42105263</span>],</span><br><span class="line">	   [ <span class="number">2.89473684</span>,  <span class="number">3.36842105</span>,  <span class="number">3.84210526</span>,  <span class="number">4.31578947</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果以第0个维度，既第0列特征为划分维度，以5为划分阈值，那么X_l为三行四列矩阵，因为第0列的第三行值也小于5</span></span><br><span class="line">X_l, X_r, y_l, y_r = split(X, y, <span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">X_l</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">1.</span>        ,  <span class="number">1.47368421</span>,  <span class="number">1.94736842</span>,  <span class="number">2.42105263</span>],</span><br><span class="line">	   [ <span class="number">2.89473684</span>,  <span class="number">3.36842105</span>,  <span class="number">3.84210526</span>,  <span class="number">4.31578947</span>],</span><br><span class="line">	   [ <span class="number">4.78947368</span>,  <span class="number">5.26315789</span>,  <span class="number">5.73684211</span>,  <span class="number">6.21052632</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u8BA1_u7B97_u4FE1_u606F_u71B5_u51FD_u6570"><a href="#u8BA1_u7B97_u4FE1_u606F_u71B5_u51FD_u6570" class="headerlink" title="计算信息熵函数"></a>计算信息熵函数</h3><p>然后定义计算信息熵的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="comment"># 计算信息熵时不关心样本特征，只关心样本目标数据的类别和每个类别的数量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entropy</span><span class="params">(y)</span>:</span></span><br><span class="line">	<span class="comment"># 使用Counter生成字典，key为y的值，value为等于该值的元素数量</span></span><br><span class="line">	counter_y = Counter(y)</span><br><span class="line">	entropy_result = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> num <span class="keyword">in</span> counter_y.values():</span><br><span class="line">		p = num / len(y)</span><br><span class="line">		<span class="comment"># 将所有类别的占比加起来，得到信息熵</span></span><br><span class="line">		entropy_result += -p * log(p)</span><br><span class="line">		</span><br><span class="line">	<span class="keyword">return</span> entropy_result</span><br></pre></td></tr></table></figure>
<p>举个例子验证一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_y = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">test_y1 = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># test_y信息熵应该比较大，因为有4种类别</span></span><br><span class="line">entropy(test_y)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.3421257227487469</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># test_y1信息熵应该比较小，因为有2种类别</span></span><br><span class="line">entropy(test_y1)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.5982695885852573</span></span><br></pre></td></tr></table></figure>
<h3 id="u5BFB_u627E_u7EF4_u5EA6_u548C_u9608_u503C_u51FD_u6570"><a href="#u5BFB_u627E_u7EF4_u5EA6_u548C_u9608_u503C_u51FD_u6570" class="headerlink" title="寻找维度和阈值函数"></a>寻找维度和阈值函数</h3><p>最后我们来定义寻找最优信息熵、维度、阈值的函数，该函数的最基本思路就是遍历样本数据的每个维度，对该维度中每两个相邻的值求均值作为阈值，然后求出信息熵，最终找到最小的信息熵，此时计算出该信息熵的维度和阈值既是最优维度和最优阈值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 寻找最优信息熵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">try_split</span><span class="params">(X, y)</span>:</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 最优信息熵初始取无穷大</span></span><br><span class="line">	best_entropy = float(<span class="string">'inf'</span>)</span><br><span class="line">	<span class="comment"># 最优维度和最优阈值，初始值为-1</span></span><br><span class="line">	best_d, best_v = -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 对样本特征数据的每个维度，既每个特征进行搜索</span></span><br><span class="line">	<span class="keyword">for</span> d <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">		<span class="comment"># 在d这个维度上，将每两个样本点中间的值作为阈值</span></span><br><span class="line">		<span class="comment"># 对样本数据在d这个维度上进行排序，返回排序索引</span></span><br><span class="line">		sorted_index = np.argsort(X[:, d])</span><br><span class="line">		<span class="comment"># 遍历每行样本数据，注意从第一行开始，因为需要用上一行的值和该行的值求均值</span></span><br><span class="line">		<span class="keyword">for</span> row <span class="keyword">in</span> range(<span class="number">1</span>, len(X)):</span><br><span class="line">			<span class="comment"># 如果两个值相等，那么均值无法区分这两个值，所以忽略这种情况</span></span><br><span class="line">			<span class="keyword">if</span> X[sorted_index[row-<span class="number">1</span>], d] != X[sorted_index[row], d]:</span><br><span class="line">				v = (X[sorted_index[row-<span class="number">1</span>], d] + X[sorted_index[row], d]) / <span class="number">2</span></span><br><span class="line">				<span class="comment"># 使用split()函数做划分</span></span><br><span class="line">				X_l, X_r, y_l, y_r = split(X, y, d, v)</span><br><span class="line">				<span class="comment"># 求划分后两部分的信息熵</span></span><br><span class="line">				e = entropy(y_l) + entropy(y_r)</span><br><span class="line">				<span class="comment"># 保存最优信息熵、维度、阈值</span></span><br><span class="line">				<span class="keyword">if</span> e &lt; best_entropy:</span><br><span class="line">					best_entropy, best_d, best_v = e, d, v</span><br><span class="line">					</span><br><span class="line">	<span class="keyword">return</span> best_entropy, best_d, best_v</span><br></pre></td></tr></table></figure>
<p>我们同样使用鸢尾花的数据进行验证，在验证之前，我们先将之前通过Scikit Learn的决策树求出的鸢尾花决策边界的图贴出来，做以对比：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/88a63811729dd486eb8498ca3dd53020.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">best_entropy, best_d, best_v = try_split(X, y)</span><br><span class="line">print(<span class="string">"best_entropy = "</span>, best_entropy)</span><br><span class="line">print(<span class="string">"best_d = "</span>, best_d)</span><br><span class="line">print(<span class="string">"best_v = "</span>, best_v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">best_entropy =  <span class="number">0.6931471805599453</span></span><br><span class="line">best_d =  <span class="number">0</span></span><br><span class="line">best_v =  <span class="number">2.45</span></span><br></pre></td></tr></table></figure>
<p>此时样本数据的第一个根节点的判断条件就求出来了，从上面的决策边界图中可以看到，第一次划分确实是从第0个维度，既横轴开始，以2.4左右为阈值进行的。我们来看看通过这个根节点划分后的数据是什么样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_l1, X_r1, y_l1, y_r1 = split(X, y, best_d, best_v)</span><br><span class="line">entropy(y_l1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">entropy(y_r1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.6931471805599453</span></span><br></pre></td></tr></table></figure>
<p>从上面结果可以看到，经过第一个根节点划分后，对左侧数据求信息熵的结果为0，说明左侧的数据现在只有一个类型了。从上图也可以清晰的看到，以横轴2.4往左的区域全部是蓝色的点。那么对右侧而言，它的信息熵是0.69，说明对右侧还可以继续划分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_entropy2, best_d2, best_v2 = try_split(X_r1, y_r1)</span><br><span class="line">print(<span class="string">"best_entropy2 = "</span>, best_entropy2)</span><br><span class="line">print(<span class="string">"best_d2 = "</span>, best_d2)</span><br><span class="line">print(<span class="string">"best_v2 = "</span>, best_v2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">best_entropy2 =  <span class="number">0.4132278899361904</span></span><br><span class="line">best_d2 =  <span class="number">1</span></span><br><span class="line">best_v2 =  <span class="number">1.75</span></span><br></pre></td></tr></table></figure>
<p>第二次划分以第一个维度上的1.75为阈值进行，这和上图中的纵轴的划分界限基本是一致的。再来看看划分后的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_l2, X_r2, y_l2, y_r2 = split(X_r1, y_r1, best_d2, best_v2)</span><br><span class="line">entropy(y_l2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.30849545083110386</span></span><br><span class="line"></span><br><span class="line">entropy(y_r2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.10473243910508653</span></span><br></pre></td></tr></table></figure>
<p>可以看到第二次划分后，两部分的信息熵都不为零，其实还可以对每部分再进行划分。不过在之前使用Scikit Learn的决策树划分时，我们将深度设为了2，所以就只划分到了目前的阶段，如果将深度设的更大的话，那么就会继续划分下去。在我们模拟寻找最优划分的过程中，就不再继续划分下去了，大家理解了划分过程就可以了。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记我们来看看机器学习中一个重要的非参数学习算法，决策树。</p>
<h2 id="u4EC0_u4E48_u662F_u51B3_u7B56_u6811"><a href="#u4EC0_u4E48_u662F_u51B3_u7B56_u6811" class="headerlink" title="什么是决策树"></a>什么是决策树</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/84205d722c59ef40b62804d2672f9435.jpg" alt=""></p>
<p>上图是一个向银行申请信用卡的示例，图中的树状图展示了申请人需要在银行过几道关卡后才能成功申请到一张信用卡的流程图。在图中树状图的根节点是申请人输入的信息，叶子节点是银行作出的决策，也就相当于是对申请者输入信息作出的分类决策。从第一个根节点到最后一个叶子节点经过的根节点数量称为树状图的深度（depth）。上图示例中的树状图从第一个根节点<strong>申请人是否办理过信用卡</strong>，到最后一个<strong>发放信用卡</strong>叶子节点共经过了三个根节点，所以深度为3。那么像这样使用树状图对输入信息一步步分类的方式就称为决策树方式。</p>
<p>我们再来看一个问题，上图中每一个根节点的输入信息都可以用<strong>是</strong>或<strong>否</strong>来做判断分类，但是机器学习的样本数据都是数字，那么此时如果做判断呢？我们先来使用Scikit Learn中提供的决策树直观的看一下通过决策树对样本数据的分类过程和分类结果。</p>]]>
    
    </summary>
    
      <category term="信息熵" scheme="http://www.devtalking.com/tags/%E4%BF%A1%E6%81%AF%E7%86%B5/"/>
    
      <category term="决策树" scheme="http://www.devtalking.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十四之核函数（Kernel Function）]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-14/"/>
    <id>http://www.devtalking.com//articles/machine-learning-14/</id>
    <published>2018-06-14T16:00:00.000Z</published>
    <updated>2018-08-27T02:36:49.069Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="SVM_u5904_u7406_u975E_u7EBF_u6027_u95EE_u9898"><a href="#SVM_u5904_u7406_u975E_u7EBF_u6027_u95EE_u9898" class="headerlink" title="SVM处理非线性问题"></a>SVM处理非线性问题</h2><p>我们在<a href="http://www.devtalking.com/articles/machine-learning-8/"> 机器学习笔记八之多项式回归、拟合程度、模型泛化 </a>中讲过线性回归通过多项式回归方法处理非线性问题，同样SVM也可以使用多项式方法处理非线性问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用datasets提供的方法构建非线性数据</span></span><br><span class="line">X, y = datasets.make_moons()</span><br></pre></td></tr></table></figure>
<p>我们可以使用<code>datasets</code>提供的<code>make_moons()</code>函数生成非线性数据，默认为100行2列的矩阵，既100个样本数据，每个样本数据2个特征。可以使用<code>n_samples</code>参数指定样本数据数量。我们将其绘制出来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/df044e91c5054a81223e83ab5e339d9d.jpg" alt=""></p>
<p>可以看到<code>make_moons()</code>默认生成的数据绘制出的图像是两个规整的半月牙曲线。但是作为样本数据有点太规整了，所以我们可以使用<code>noise</code>参数给生成的数据增加一点噪音：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = datasets.make_moons(noise=<span class="number">0.15</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/e1409e9a5bd253d605c321d1edf4aa9c.jpg" alt=""></p>
<p>可以看到，增加了噪音后虽然数据点较之前的分布分散随机了许多，但整体仍然是两个半月牙形状。下面我们就在SVM的基础上使用多项式来看看对这个样本数据决策边界的计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br></pre></td></tr></table></figure>
<p>首先我们需要引入我们用到的几个类：</p>
<ul>
<li>多项式自然要用到之前讲过的<code>PolynomialFeatures</code>类。</li>
<li>数据归一化需要用到<code>StandardScaler</code>类。</li>
<li>同样需要用到Pipeline将多个过程封装在一个函数里，所以要用到<code>Pipeline</code>类。</li>
</ul>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义多项式SVM函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomialSVC</span><span class="params">(degree, C=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"linearSVC"</span>, LinearSVC(C=C))</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用多项式SVM训练样本数据</span></span><br><span class="line">ploy_svc = PolynomialSVC(degree=<span class="number">3</span>)</span><br><span class="line">ploy_svc.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># meshgrid函数用两个坐标轴上的点在平面上画格，返回坐标矩阵</span></span><br><span class="line">	X0, X1 = np.meshgrid(</span><br><span class="line">		<span class="comment"># 随机两组数，起始值和密度由坐标轴的起始值决定</span></span><br><span class="line">		np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line">	<span class="comment"># ravel()方法将高维数组降为一维数组，c_[]将两个数组以列的形式拼接起来，形成矩阵</span></span><br><span class="line">	X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 通过训练好的逻辑回归模型，预测平面上这些点的分类</span></span><br><span class="line">	y_predict = model.predict(X_grid_matrix)</span><br><span class="line">	y_predict_matrix = y_predict.reshape(X0.shape)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 设置色彩表</span></span><br><span class="line">	<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">	my_colormap = ListedColormap([<span class="string">'#0000CD'</span>, <span class="string">'#40E0D0'</span>, <span class="string">'#FFFF00'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 绘制等高线，并且填充等高区域的颜色</span></span><br><span class="line">	plt.contourf(X0, X1, y_predict_matrix, linewidth=<span class="number">5</span>, cmap=my_colormap)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(ploy_svc, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/2b40b1ec8ba7140788c9b4be1f463260.jpg" alt=""></p>
<p>从上图可以看到决策边界的情况，很明显已经不是线性决策边界了，并且能较好的将两类点区分开。</p>
<h2 id="u6838_u51FD_u6570_uFF08Kernel_Function_uFF09"><a href="#u6838_u51FD_u6570_uFF08Kernel_Function_uFF09" class="headerlink" title="核函数（Kernel Function）"></a>核函数（Kernel Function）</h2><p>核函数是机器学习算法中一个重要的概念。简单来讲，核函数就是样本数据点的转换函数。之所以在SVM这篇笔记中介绍核函数，是因为在SVM处理非线性的问题中除了上一节介绍的方法外，还有使用核函数的方式。而介绍核函数的切入点也是通过多项式这个知识点来讲的。</p>
<h3 id="u4EC0_u4E48_u662F_u6838_u51FD_u6570"><a href="#u4EC0_u4E48_u662F_u6838_u51FD_u6570" class="headerlink" title="什么是核函数"></a>什么是核函数</h3><p>首先我们回顾一下多项式方法。对样本数据进行多项式转换并不是真正的增加了样本数据的特征数量，而且对原有的特征数据进行转换，构造出其他特征，这些新构造出的特征和原始特征都有强关联。</p>
<p>比如如果将原本只有$x_1$、$x_2$两个特征的样本数据通过多项式转换为有10个特征的数据，那么转换后的这10个特征分别为：1，$x_1$，$x_2$，$x_1^2$，$x_2^2$，$x_1x_2$，$x_1^3$，$x_2^3$，$x_1^2x_2$，$x_1x_2^2$。</p>
<p>假设某个机器学习算法的原始公式中含有$x_ix_j$（$x_i $和$x_j$为特征向量），如果是使用多项式的方法，那么首先需要将$x_i$和$x_j$转换为$x’_i$和$x’_j$，然后转换后的，具有10个元素的两个新特征向量再相乘。如果转换后的特征数量比较多的时候，新特征向量的计算复杂度就会变的很高，并且还需要占用大量内存在存储庞大的特征向量。</p>
<p>那么核函数就是要解决上面的问题。所以多项式核函数就是有这样一个函数，将$x_i$和$x_j$作为参数传入，再指定要构造的特征数，然后该函数直接返回$x’_ix’_j$的值。该函数既可以模拟多项式构造特征的原理，又能将新特征向量的点乘计算出来，同时计算复杂度相对比较小，而且也不用存储庞大的新特征向量。</p>
<p>下面就来看看多项式核函数：</p>
<p>$$K(x,y)=(x \cdot y + 1)^2$$</p>
<p>我们将上面的公式展开来看看：</p>
<p>$$K(x,y)=(\sum_{i=1}^nx_iy_i + 1)$$</p>
<p>应用任意次项和的展开式公式，上面的公式继续展开后得：</p>
<p>$$K(x,y)=\sum_{i=1}^n(x_i^2)(y_i^2) + \sum_{i=2}^n\sum_{j=1}^{i-1}(\sqrt 2 x_ix_j)(\sqrt 2 y_iy_j) + \sum_{i=1}^n(\sqrt 2x_i)(\sqrt 2y_i) + 1$$</p>
<p>上面的公式我们就可以看做是若干项相乘再相加，对于$x$来说：</p>
<ul>
<li>从$\sum_{i=1}^n(x_i^2)(y_i^2)$可以分析出共有$(x_n^2,…,x_1^2)$这么多项。</li>
<li>从$\sum_{i=2}^n\sum_{j=1}^{i-1}(\sqrt 2 x_ix_j)(\sqrt 2 y_iy_j)$可以分析出共有$(\sqrt 2x_nx_{n-1},…,\sqrt 2x_2x_1)$这么多项。</li>
<li>从$\sum_{i=1}^n(\sqrt 2x_i)(\sqrt 2y_i)$可以分析出共有$(\sqrt 2x_n,…,\sqrt 2x_1)$</li>
</ul>
<p>将上面的三个向量合起来其实就相当于将原始$x$特征向量根据多项式原理转换后的新的特征向量$x’$：</p>
<p>$$x’= (x_n^2,…,x_1^2,\sqrt 2x_nx_{n-1},…,\sqrt 2x_2x_1,\sqrt 2x_n,…,\sqrt 2x_1,1)$$</p>
<p>根据同样的道理，也可以得出新的$y’$。所以将$K(x,y)$展开后的公式就相当于$x’ \cdot y’$。</p>
<p>综上多项式核函数的公式为：</p>
<p>$$K(x,y) = (x \cdot y + c)^d$$</p>
<p>$d$ 代表多项式中的degree，$c$代表Soft Margin SVM中的$C$，是两个超参数。</p>
<p>以上是通过多项式核函数解释核函数的概念和数学原理，那么不同的机器学习算法里有很多不同的核函数，对应对原始样本数据不同的转换。</p>
<h3 id="u901A_u8FC7_u6838_u51FD_u6570_u65B9_u5F0F_u4F7FSVM_u5904_u7406_u975E_u7EBF_u6027_u95EE_u9898"><a href="#u901A_u8FC7_u6838_u51FD_u6570_u65B9_u5F0F_u4F7FSVM_u5904_u7406_u975E_u7EBF_u6027_u95EE_u9898" class="headerlink" title="通过核函数方式使SVM处理非线性问题"></a>通过核函数方式使SVM处理非线性问题</h3><p>我们再回过头来看看SVM解决非线性的问题，之前我们使用常规的多项式方法，这节我们来看看如何使用核函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomailKernelSVC</span><span class="params">(degree, C=<span class="number">1</span>)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"kernelSVC"</span>, SVC(kernel=<span class="string">"poly"</span>, degree=degree, C=C))</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">poly_kernel_svc = PolynomailKernelSVC(degree=<span class="number">3</span>)</span><br><span class="line">poly_kernel_svc.fit(X, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(poly_kernel_svc, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/98349c680b17d31f564214a7a949010b.jpg" alt=""></p>
<p>Scikit Learn中使用了多项式核函数的SVM的类是<code>SVC</code>，它的<code>kernel</code>参数就表示要使用哪种核函数，上面示例中传入的<code>poly</code>就表示多项式核函数。</p>
<h2 id="u9AD8_u65AF_u6838_u51FD_u6570"><a href="#u9AD8_u65AF_u6838_u51FD_u6570" class="headerlink" title="高斯核函数"></a>高斯核函数</h2><p>这一节我们来看看应用非常广泛的一个核函数，高斯核函数。它的名称比较多，以下名称指的都是高斯核函数：</p>
<ul>
<li>高斯核函数。</li>
<li>RBF（Radial Basis Function Kernel）。</li>
<li>径向基函数。</li>
</ul>
<p>对于多项式核函数而言，它的核心思想是将样本数据进行升维，从而使得原本线性不可分的数据线性可分。那么高斯核函数的核心思想是<strong>将每一个样本点映射到一个无穷维的特征空间</strong>，从而使得原本线性不可分的数据线性可分。</p>
<p>我们先来回顾一下多项式特征，如下图所示，有一组一维数据，两个类别，明显是线性不可分的情况：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/16435fc028edde97071d1aef6bbbbe0c.jpg" alt=""></p>
<p>然后通过多项式将样本数据再增加一个维度，假设就是$x^2$，样本数据就变成这样了：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c79ce6560066eeec40ccd24ad7871cdb.jpg" alt=""></p>
<p>此时原本线性不可分的样本数据，通过增加一个维度后就变成线性可分的状态。这就是多项式升维的意义。</p>
<p>下面我们先来认识一下高斯核函数的公式：</p>
<p>$$K(x, y) = e^{-\gamma||x-y||^2}$$</p>
<p>上面公式中的$\gamma$就是高斯核函数的超参数。然后我们再来看看高斯核函数使线性不可分的数据线性可分的原理。</p>
<p>为了方便可视化，我们将高斯核函数中的$y$取两个定值$l_1$核$l_2$，这类点称为地标（Land Mark）。那么高斯核函数升维过程就是假如有两个地标点，那么就将样本数据转换为二维，也就是将原本的每个$x$值通过高斯核函数和地标，将其转换为2个值，既：</p>
<p>$$x \to (e^{-\gamma||x-l_1||^2}, e^{-\gamma||x-l_2||^2})$$</p>
<p>下面我们用程序来实践一下这个过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据，x值从-4到5，每个数间隔为1</span></span><br><span class="line">x = np.arange(-<span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([-<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># y构建为0，1向量，且是线性不可分的</span></span><br><span class="line">y = np.array((x &gt;= -<span class="number">2</span>) &amp; (x &lt;= <span class="number">2</span>), dtype=<span class="string">'int'</span>)</span><br><span class="line">y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制样本数据</span></span><br><span class="line">plt.scatter(x[y==<span class="number">0</span>], [<span class="number">0</span>]*len(x[y==<span class="number">0</span>]))</span><br><span class="line">plt.scatter(x[y==<span class="number">1</span>], [<span class="number">0</span>]*len(x[y==<span class="number">1</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/bdea4696beca278092273b69240bceb0.jpg" alt=""></p>
<p>可以看到我们构建的样本数据是明显线性不可分的状态。下面我们来定义高斯核函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, l)</span>:</span></span><br><span class="line">	<span class="comment"># 这一节对gamma先不做探讨，先定为1</span></span><br><span class="line">	gamma = <span class="number">1.0</span></span><br><span class="line">	<span class="comment"># 这里x-l是一个数，不是向量，所以不需要取模</span></span><br><span class="line">	<span class="keyword">return</span> np.exp(-gamma * (x - l)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将每一个x值通过高斯核函数和l1，l2地标转换为2个值，构建成新的样本数据</span></span><br><span class="line">l1, l2 = -<span class="number">1</span>, <span class="number">1</span></span><br><span class="line">X_new = np.empty((len(x), <span class="number">2</span>))</span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(x):</span><br><span class="line">	X_new[i, <span class="number">0</span>] = gaussian(data, l1)</span><br><span class="line">	X_new[i, <span class="number">1</span>] = gaussian(data, l2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制新的样本点</span></span><br><span class="line">plt.scatter(X_new[y==<span class="number">0</span>, <span class="number">0</span>], X_new[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X_new[y==<span class="number">1</span>, <span class="number">0</span>], X_new[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/bdb98677297d6e15f9eab2820cb46d19.jpg" alt=""></p>
<p>可以看到通过高斯函数将原本的一维样本数据转换为二维后，新样本数据明显成为线性可分的状态。</p>
<p>上面的示例中，我们将高斯核函数中的$y$取定了两个值$l_1$和$l_2$。在实际运用中，是需要真实的将每个$y$值带进去的，也就是每一个样本数据中的$y$都是一个地标，那么可想而知，原始样本数据的行数就是新样本数据的维数，既原始$m*n$的样本数据通过高斯核函数转换后成为$m*m$的数据。当样本数据行数非常多的话，转换后的新样本数据维度自然会非常高，这也就是为什么在这节开头会说高斯核函数的核心思想是将每一个样本点映射到一个无穷维的特征空间的原因。</p>
<h3 id="u9AD8_u65AF_u6838_u51FD_u6570_u4E2D_u7684Gamma"><a href="#u9AD8_u65AF_u6838_u51FD_u6570_u4E2D_u7684Gamma" class="headerlink" title="高斯核函数中的Gamma"></a>高斯核函数中的Gamma</h3><p>在看高斯核函数中的$\gamma$之前，我们先来探讨一个问题，我们以前有学过正态分布，它是一个非常常见的连续概率分布，最关键的是它又名<strong>高斯分布</strong>，我们再来看看高斯分布的函数：</p>
<p>$$f(x) = \frac 1 {\sigma \sqrt {2 \pi}} e^{-\frac {(x-\mu)^2} {2\sigma^2}}$$</p>
<p>仔细看这个函数就能发现，它和高斯核函数的公式在形态上是一致的：</p>
<ul>
<li>高斯函数$e$前的系数是$\frac 1 {\sigma \sqrt {2 \pi}} $，$e$指数的系数是$-\frac 1 {2\sigma^2}$。</li>
<li>高斯核函数$e$前的系数是1，$e$指数的系数是$-\gamma$。</li>
</ul>
<p>所以高斯核函数的曲线其实也是一个高斯分布图。</p>
<p>下面再来看看高斯分布图以及$\mu$和$\sigma$对分布图的影响：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/26e4e54cdb28d00546112e369065db63.jpg" alt=""></p>
<p>上图是维基百科对高斯分布解释中的分布图，从图中可以看到：</p>
<ul>
<li>高斯分布曲线的形状都是相似的钟形图。</li>
<li>$\mu$决定分布图中心的偏移情况。</li>
<li>$\sigma$决定分布图峰值的高低，或者说钟形的胖瘦程度。</li>
</ul>
<p>因为高斯函数中的$\sigma$和高斯核函数中$\gamma$成倒数关系。所以：</p>
<ul>
<li>高斯函数中$\sigma$越大、高斯分布峰值越小。$\sigma$越小、高斯分布峰值越大。</li>
<li>高斯核函数中$\gamma$越大、高斯分布峰值越大，既钟形越窄。$\gamma$越小、高斯分布峰值越小，既钟形越宽。</li>
</ul>
<h3 id="Scikit_Learn_u4E2D_u7684RBF_SVM"><a href="#Scikit_Learn_u4E2D_u7684RBF_SVM" class="headerlink" title="Scikit Learn中的RBF SVM"></a>Scikit Learn中的RBF SVM</h3><p>这一小节来看看如果使用Scikit Learn中封装的RBF SVM。首先还是先构建样本数据，我们使用和多项式SVM相同的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">X, y = datasets.make_moons(noise=<span class="number">0.15</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/042cdeddff35d8657edbbccd2fe453a7.jpg" alt=""></p>
<p>然后通过RBF SVM训练数据并绘制决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RBFKernelSVC</span><span class="params">(gamma = <span class="number">1.0</span>)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"svc"</span>, SVC(kernel=<span class="string">"rbf"</span>, gamma=gamma))</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">rbf_svc = RBFKernelSVC(gamma=<span class="number">1</span>)</span><br><span class="line">rbf_svc.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>使用RBF SVM和使用多项式SVM其实基本一样，只是将<code>SVC</code>中的<code>kernel</code>参数由之前的<code>poly</code>变更为<code>rbf</code>，然后传入该核函数需要的超参数既可。</p>
<p>接下来绘制决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># meshgrid函数用两个坐标轴上的点在平面上画格，返回坐标矩阵</span></span><br><span class="line">	X0, X1 = np.meshgrid(</span><br><span class="line">		<span class="comment"># 随机两组数，起始值和密度由坐标轴的起始值决定</span></span><br><span class="line">		np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line">	<span class="comment"># ravel()方法将高维数组降为一维数组，c_[]将两个数组以列的形式拼接起来，形成矩阵</span></span><br><span class="line">	X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 通过训练好的逻辑回归模型，预测平面上这些点的分类</span></span><br><span class="line">	y_predict = model.predict(X_grid_matrix)</span><br><span class="line">	y_predict_matrix = y_predict.reshape(X0.shape)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 设置色彩表</span></span><br><span class="line">	<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">	my_colormap = ListedColormap([<span class="string">'#0000CD'</span>, <span class="string">'#40E0D0'</span>, <span class="string">'#FFFF00'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 绘制等高线，并且填充等高区域的颜色</span></span><br><span class="line">	plt.contourf(X0, X1, y_predict_matrix, linewidth=<span class="number">5</span>, cmap=my_colormap)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(rbf_svc, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/91ee9feeb0a6a58eb20695434783ab93.jpg" alt=""></p>
<p>上图就是当$\gamma$为1时，RBF SVM训练样本数据后的决策边界，我们先来解释一下它的高斯分布有什么关系。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/3054e67f350f04826ace4379db777a8a.jpg" alt=""></p>
<p>如上图所示，蓝色虚线表示等高线，橘黄色点表示一个样本点，所以上面的图其实是俯视以橘黄色样本点为峰值点的高斯分布图。</p>
<p>对于每个样本点都有围绕它的一个高斯分布图，所以连起来就形成了一片区域，然后形成了决策区域和决策边界：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/6d96b78336d261269b68a73e06b24350.jpg" alt=""></p>
<p>可以看到当$\gamma$取1时，RBF SVM训练样本数据后的决策边界和多项式SVM的几乎一致。下面我们尝试变化超参数$\gamma$来看看决策边界会有怎样的变化。</p>
<p>先来看看将$\gamma$取较大值后的决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将gamma取100</span></span><br><span class="line">rbf_svc100 = RBFKernelSVC(gamma=<span class="number">100</span>)</span><br><span class="line">rbf_svc100.fit(X, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(rbf_svc100, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/67a1776dbbf2d2167969985d08cd533d.jpg" alt=""></p>
<p>从上图可以看到，决策边界几乎就是围绕着蓝色点的区域，这也印证了高斯核函数中$\gamma$越大、高斯分布峰值越大，既钟形越窄的定义。因为钟形比较窄，所以不足以连成大片区域，就呈现出了上图中的情况。</p>
<p>我们再来看看将$\gamma$取较小值后的决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rbf_svc01 = RBFKernelSVC(gamma=<span class="number">0.1</span>)</span><br><span class="line">rbf_svc01.fit(X, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(rbf_svc01, axis=[-<span class="number">1.5</span>, <span class="number">2.5</span>, -<span class="number">1.0</span>, <span class="number">1.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/e9bfc098b31af59f61370c3eeae326fd.jpg" alt=""></p>
<p>可以看到当$\gamma$取0.1后，决策边界几乎成了线性决策边界，说明每个样本点的高斯分布钟形太宽了。所以我们得出结论，当$\gamma$取值比较大时，数据训练结果趋于过拟合，当$\gamma$取值比较小时，数据训练结果趋于欠拟合。</p>
<h3 id="SVM_u89E3_u51B3_u56DE_u5F52_u95EE_u9898"><a href="#SVM_u89E3_u51B3_u56DE_u5F52_u95EE_u9898" class="headerlink" title="SVM解决回归问题"></a>SVM解决回归问题</h3><p>SVM解决回归问题的思路和解决分类问题的思路正好是相反的。我们回忆一下，在Hard Margin SVM中，我们希望在Margin区域中一个样本点都没有，即便在Soft Margin SVM中也是希望Margin区域中的样本点越少越好。</p>
<p>而在SVM解决回归问题时，是希望Margin区域中的点越多越好：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d18d230fe810996874d6f1b078a8fd8a.jpg" alt=""></p>
<p>也就是找到一条拟合直线，使得这条直线的Margin区域中的样本点越多，说明拟合的越好，反之依然。Margin边界到拟合直线的距离称为$\epsilon$是SVM解决回归问题的一个超参数。</p>
<p>在Scikit Learn中有<code>LinearSVR</code>和<code>SVR</code>两个类，前者就是使用SVM线性方式解决回归问题的类，后者是SVM使用核函数方式解决回归问题的类。用法和<code>LinearSVC</code>及<code>SVC</code>的一致，只不过需要传入$\epsilon$这个超参数既可。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="SVM_u5904_u7406_u975E_u7EBF_u6027_u95EE_u9898"><a href="#SVM_u5904_u7406_u975E_u7EBF_u6027_u95EE_u9898" class="headerlink" title="SVM处理非线性问题"></a>SVM处理非线性问题</h2><p>我们在<a href="http://www.devtalking.com/articles/machine-learning-8/"> 机器学习笔记八之多项式回归、拟合程度、模型泛化 </a>中讲过线性回归通过多项式回归方法处理非线性问题，同样SVM也可以使用多项式方法处理非线性问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用datasets提供的方法构建非线性数据</span></span><br><span class="line">X, y = datasets.make_moons()</span><br></pre></td></tr></table></figure>
<p>我们可以使用<code>datasets</code>提供的<code>make_moons()</code>函数生成非线性数据，默认为100行2列的矩阵，既100个样本数据，每个样本数据2个特征。可以使用<code>n_samples</code>参数指定样本数据数量。我们将其绘制出来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/df044e91c5054a81223e83ab5e339d9d.jpg" alt=""></p>
<p>可以看到<code>make_moons()</code>默认生成的数据绘制出的图像是两个规整的半月牙曲线。但是作为样本数据有点太规整了，所以我们可以使用<code>noise</code>参数给生成的数据增加一点噪音：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = datasets.make_moons(noise=<span class="number">0.15</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/e1409e9a5bd253d605c321d1edf4aa9c.jpg" alt=""></p>
<p>可以看到，增加了噪音后虽然数据点较之前的分布分散随机了许多，但整体仍然是两个半月牙形状。下面我们就在SVM的基础上使用多项式来看看对这个样本数据决策边界的计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br></pre></td></tr></table></figure>
<p>首先我们需要引入我们用到的几个类：</p>
<ul>
<li>多项式自然要用到之前讲过的<code>PolynomialFeatures</code>类。</li>
<li>数据归一化需要用到<code>StandardScaler</code>类。</li>
<li>同样需要用到Pipeline将多个过程封装在一个函数里，所以要用到<code>Pipeline</code>类。</li>
</ul>]]>
    
    </summary>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="核函数" scheme="http://www.devtalking.com/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十三之支撑向量机（SVM）]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-13/"/>
    <id>http://www.devtalking.com//articles/machine-learning-13/</id>
    <published>2018-05-29T16:00:00.000Z</published>
    <updated>2018-08-27T02:36:32.567Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这一篇笔记主要讲机器学习算法中一个重要的分类算法，支撑向量机（Support Vector Machine）。它背后有严格的数学理论和统计学理论支撑的，这里我们只对它的原理和应用做以介绍，更深层次的数学理论有兴趣可以查阅其他资料。</p>
<h2 id="u4EC0_u4E48_u662FSVM"><a href="#u4EC0_u4E48_u662FSVM" class="headerlink" title="什么是SVM"></a>什么是SVM</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/5142c26ba0e86429631a63609c314a97.jpg" alt=""></p>
<p>上面的图展示的是一个二分类问题，在逻辑回归的笔记中，我们知道了决策边界，图中橘黄色的直线就是决策边界，它看似很好的将样本数据的不同分类区分开了。我们在多项式回归的笔记中，讲过模型泛化的问题，一个模型的好坏程度很大程度上是体现了模型泛化能力的好坏，也就是对未知样本的预测能力。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b26d6614606ac304144b99549eeb390e.jpg" alt=""></p>
<p>如果新来一个样本数据A，按照决策边界划分它是被分为了蓝色分类，可事实真的是如此吗，因为点A和红色分类的点非常近，很有可能点A是属于红色分类的。所以上图中的决策边界可能并不是最优的，那最优的决策边界应该是什么样的呢？</p>
<p>上面的决策边界之所以有缺陷，是因为它和红色类别的点太近了，当然肯定也不能和蓝色的点太近了，所以我们希望这条决策边界离红色分类和蓝色分类都尽可能的远，才能很好的避免上面出现的问题。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/990fbea083569d4503be599d202b049d.jpg" alt=""></p>
<p>想要决策边界离红蓝两个分类都尽可能的远，也就是红蓝分类两边离决策边界最近的点的距离要远，并且两边的点到决策边界的距离要相等，这才说明决策边界是不偏不倚的，如上图所示。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d4642c6673e5e75023088be725dcd4f8.jpg" alt=""></p>
<p>过红蓝两边离决策边界最近的点画两条直线，并且平行于决策边界，那么就确定了一个区域，在这个区域内不应该再有任何数据点出现。那么SVM就是尝试寻找这个区域中间那条最优的决策边界。这个区域边界上的点称为支撑向量，换句话说，是支撑向量定义出了这个区域，那么最优的决策边界也是由支撑向量决定的，这也是支撑向量机这个名称的由来。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f5af760ed417acf74d79d065b437807d.jpg" alt=""></p>
<p>如上图所示，由支撑向量确定的区域之间的距离称为Margin。那么SVM要做的事情就是最大化Margin，既将问题转化为了求最优解的问题。</p>
<p>以上对SVM的解释是基于线性可分问题的基础上，也就是能确确实实找到一条直线作为决策边界，这种问题称为Hard Margin SVM，但是很多真实的情况是线性不可分的，这类问题称为Soft Margin SVM，这两类问题都是支撑向量机可以处理的，但基础还是Hard Margin SVM。</p>
<a id="more"></a>
<h2 id="SVM_u80CC_u540E_u7684_u6700_u4F18_u5316_u95EE_u9898"><a href="#SVM_u80CC_u540E_u7684_u6700_u4F18_u5316_u95EE_u9898" class="headerlink" title="SVM背后的最优化问题"></a>SVM背后的最优化问题</h2><p>从上面的示例图上可以看出，Margin其实就是二倍的$d$，$d$是支撑向量到决策边界的距离，那么求Margin的最大值就是求$d$的最大值。</p>
<h3 id="u70B9_u5230_u76F4_u7EBF_u7684_u8DDD_u79BB"><a href="#u70B9_u5230_u76F4_u7EBF_u7684_u8DDD_u79BB" class="headerlink" title="点到直线的距离"></a>点到直线的距离</h3><p>在解析几何中我们学过，假设有条直线定义为$ax + by + c = 0$，有一点的坐标为$(x_0, y_0)$，那么该点到该条直线的距离为：</p>
<p>$$d = \frac {|ax_0 + by_0 + c|} {\sqrt{a^2 + b^2}}$$</p>
<p>那么如果扩展到高维空间中，点到直线的距离为：</p>
<p>$$d = \frac {|ax_0 + by_0 + cz_0 + d|} {\sqrt{a^2 + b^2 + c^2}}$$</p>
<p>上面公式中的$x_0$、$y_0$、$z_0$可以看作样本数据的特征，而$a$、$b$、$c$可以看作是特征系数。那么上面的公式可以写为：</p>
<p>$$d = \frac {|ax_1 + bx_2 + cx_3 + d|} {\sqrt{a^2 + b^2 + c^2}}$$</p>
<p>将$ax_1 + bx_2 + cx_3$向量化后，公式可写为：</p>
<p>$$d = \frac {|w^Tx + d|} {\sqrt{a^2 + b^2 + c^2}}$$</p>
<p>再根据向量的模的公式$||\vec v|| = \sqrt {v_1^2 + v_2^2 + … + v_n^2}$，上面的公式可继续转换为：</p>
<p>$$d = \frac {|w^Tx + d|} {||w||}$$</p>
<p>$w$为特征系数向量，$x$为特征向量。</p>
<h3 id="u9650_u5B9A_u6761_u4EF6_u7684_u6700_u4F18_u5316_u95EE_u9898"><a href="#u9650_u5B9A_u6761_u4EF6_u7684_u6700_u4F18_u5316_u95EE_u9898" class="headerlink" title="限定条件的最优化问题"></a>限定条件的最优化问题</h3><p>有了上面的公式后，我们就可以假定决策边界直线的公式为$w^Tx + d = 0$，支撑向量到它的距离为$d = \frac {|w^Tx + d|} {||w||}$。那么除了支撑向量以外的点到决策边界的距离都应该大于$\frac {|w^Tx + d|} {||w||}$。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9cb17884cae3c002e35a1cf7e3b3ba95.jpg" alt=""></p>
<p>我们将上图中红色点定义为1类别，蓝色点定义为-1类别，那么就有：</p>
<p>$$\left\{<br>\begin{aligned}<br>{\frac {w^Tx^{(i)} + d} {||w||} \ge d \ \ \ \ \ \ \forall y^{(i)} = 1 \\<br>\frac {w^Tx^{(i)} + d} {||w||} \le -d \ \ \ \ \ \ \forall y^{(i)} = -1}<br>\end{aligned}<br>\right.<br>$$</p>
<p>既任意类别为1的红色点距离决策边界的距离要大于等于$d$。任意类别为-1的蓝色点距离决策边界的距离要小于等于$-d$。将上面的两个公式左右分别除以$d$得：</p>
<p>$$\left\{<br>\begin{aligned}<br>{\frac {w^Tx^{(i)} + d} {||w||d} \ge 1 \ \ \ \ \ \ \forall y^{(i)} = 1 \\<br>\frac {w^Tx^{(i)} + d} {||w||d} \le -1 \ \ \ \ \ \ \forall y^{(i)} = -1}<br>\end{aligned}<br>\right.<br>$$</p>
<p>下面来分析上面的公式，$||w||$是一个标量，$d$也是一个标量，所以分母$||w||d $也是一个标量。那么对于分子中的向量$w^T$除以一个标量仍然是一个向量，可以记为$w^T_d$。分子中的标量$d$除以一个标量自然也是一个标量，记为$d_d$。所以上面的公式又可以转换为：</p>
<p>$$\left\{<br>\begin{aligned}<br>{w^T_dx^{(i)} + d_d \ge 1 \ \ \ \ \ \ \forall y^{(i)} = 1 \\<br>w^T_dx^{(i)} + d_d \le -1 \ \ \ \ \ \ \forall y^{(i)} = -1}<br>\end{aligned}<br>\right.<br>$$</p>
<p>这样也就得出了由红色点支撑向量构成的直线公式为：</p>
<p>$$w^Tx + d = 1$$</p>
<p>由蓝色点支撑向量构成的直线公式为：</p>
<p>$$w^Tx + d = -1$$</p>
<p>在逻辑回归的笔记中我们知道通过一个技巧可以将上面两个公式通过一个公式表示出来，既公式左右两边都乘以$y^{(i)} $，得：</p>
<p>$$y^{(i)}(w^T_d x^{(i)} + d_d) \ge 1$$</p>
<p>为了书写方便，我们就将$w^T_d $和$d_d$命名为$w^T$和$d$。所以最终我们希望的是对于所有的样本数据点都满足下面的公式：</p>
<p>$$y^{(i)}(w^T x^{(i)} + d) \ge 1$$</p>
<p>那么上面这个公式就是SVM中最优化目标函数的限定条件，标识为$S.T.$（Such That）。</p>
<h3 id="u76EE_u6807_u51FD_u6570"><a href="#u76EE_u6807_u51FD_u6570" class="headerlink" title="目标函数"></a>目标函数</h3><p>在这一节开始时我们就很明确我们的目标是求$d$的最大值，既：</p>
<p>$$max \frac {|w^Tx + d|} {||w||}$$</p>
<p>根据之前的推导我们知道，无论是红色点的支撑向量，还是蓝色点的支撑向量构成的直线公式取绝对值后都为1，所以我们的目标函数又可以写为：</p>
<p>$$max \frac 1 {||w||}$$</p>
<p>既：</p>
<p>$$min ||w||$$</p>
<p>为了方便求导，我们再将其转换一下，最终SVM的目标函数为：</p>
<p>$$min \frac 1 2 ||w||^2$$</p>
<p>那么最后SVM的最优化问题的两个函数为：</p>
<p>$$\left\{<br>\begin{aligned}<br>{min \frac 1 2 ||w||^2 \\<br> S.T. \ \ \ y^{(i)}(w^T x^{(i)} + d) \ge 1}<br>\end{aligned}<br>\right.<br>$$</p>
<p>也就是在$y^{(i)}(w^T x^{(i)} + d) \ge 1$这个限定条件下 ，求目标函数$min \frac 1 2 ||w||^2 $的最优解。</p>
<p>这就和我们之前学习过的算法不一样了，之前不论是线性回归还是逻辑回归，在求最优化问题时都是求全局最优化问题，也就是没有限定条件的目标函数最优解问题，这类问题对目标函数求导让他等于0，然后相应的极值点就是最大值或最小值的位置。而SVM是有条件的最优化问题，此时求目标函数的极值就复杂了很多，这里就不对求解过程做详细阐述了。</p>
<h2 id="Soft_Margin_SVM"><a href="#Soft_Margin_SVM" class="headerlink" title="Soft Margin SVM"></a>Soft Margin SVM</h2><p>在上一节介绍了Hard Margin SVM和Soft Margin SVM，并且在诠释SVM背后最优化问题的数学原理时也是基于Hard Margin SVM前提的。这一节我们来看看Soft Margin SVM。</p>
<h3 id="Soft_Margin_SVM_u6982_u5FF5"><a href="#Soft_Margin_SVM_u6982_u5FF5" class="headerlink" title="Soft Margin SVM概念"></a>Soft Margin SVM概念</h3><p><img src="http://paxigrdp0.bkt.clouddn.com/9c7ae21a6ff46c774740afa4a2f7cb05.jpg" alt=""></p>
<p>如上图所示，点A是一个蓝色分类的点，但是它离红色分类的点非常近，那么如果按Hard Margin SVM的思路，上图情况的决策边界很有可能是下图所示这样：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d5668bb993d9a3b28bb2d2585a37f56e.jpg" alt=""></p>
<p>这条决策边界直线看似很好的将蓝色和红色点完全区分开了，但是它的泛化能力是值得怀疑的，因为这条决策边界极大的受到了点A的影响，而点A可能是蓝色点中极为特殊的一个点，也有可能它根本就是一个错误的点。所以根据SVM的思想，比较合理的决策边界应该下图绿色的直线所示：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/766a45ecb9df58b5ab41525655347c87.jpg" alt=""></p>
<p>虽然绿色直线的决策边界没有完全将红蓝点分开，但是如果将它放在生产数据中，可能预测准确度更高，也就是泛化能力更强。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/6b71f9d805f89f614fd9bd0a393b7890.jpg" alt=""></p>
<p>再如上图中的情况，已经根本不可能有一条线性决策边界能将红蓝点分开了，所以我们希望决策边界具有一定的包容性或容错性，已降低分类准确度的代价换来更高的泛化能力。那么这种SVM就称为Soft Margin SVM。</p>
<h3 id="Soft_Margin_SVM_u539F_u7406"><a href="#Soft_Margin_SVM_u539F_u7406" class="headerlink" title="Soft Margin SVM原理"></a>Soft Margin SVM原理</h3><p>在Hard Margin SVM最优化问题的两个函数中，限定条件$y^{(i)}(w^T x^{(i)} + d) \ge 1$表示在Margin区域内不会有任何点出现，但是在Soft Margin SVM中为了容错性，是允许在Margin区域内出现点的，也就是将Hard Margin SVM的限定条件加以宽松量，并且这个宽松量必须是正数：</p>
<p>$$S.T. \ \ \ y^{(i)}(w^T x^{(i)} + d) \ge 1 - \zeta_i$$</p>
<p>$$\zeta_i \ge 0$$</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/129d7835a23b0db515ef342d94810ca2.jpg" alt=""></p>
<p>上图中有四条直线，其中橘黄色的三条是在讲Hard Margin SVM中得出的，如果我们将Hard Margin SVM中的限定条件加以宽松量的话，其实Margin的区域就会变小，也就是图中绿色虚线和决策边界构成的区域。而绿色虚线的方程为：</p>
<p>$$w^Tx + d = 1 - \zeta$$</p>
<p>那么Soft Margin SVM的限定条件我们就知道了。</p>
<p>但是现在问题来了，如果当$\zeta$无穷大时会发生什么情况呢？那就意味着容错性无穷大，也就是可以将所有点都认为是同一类了，故而分不出类别。解决这个问题的思路我们之前已经了解过了，那就是模型正则化。</p>
<p>我们知道Hard Margin SVM的优化目标函数为$min \frac 1 2 ||w||^2$，Soft Margin SVM也是基于Hard Margin SVM的思想演变的，所以我们将这个目标函数加一个正则模型，而这个正则模型又恰是Soft Margin SVM的宽松量，这样就达到了在Hard Margin SVM的思路下，增加宽松量从而实现Soft Margin SVM，所以 Soft Margin SVM的目标函数为：</p>
<p>$$min \frac 1 2 ||w||^2 + C \sum_{i=1}^m\zeta_i$$</p>
<p>公式里的$C$是模型正则化中的一个超参数，取值范围在0到1之间。用来权衡Hard Margin SVM目标函数和Soft Margin SVM宽松量两者之间的比例。如此一来也就限制了$\zeta$无穷大的问题，因为Soft Margin SVM的目标函数最优化要同时估计两部分，相互制约。</p>
<p>我们在之前的笔记中学习过了$L_p$范数及$L_p$正则模型。这里的$C \sum_{i=1}^m\zeta_i$就是$L_1$正则模型，而$L_2$正则模型是$C \sum_{i=1}^m\zeta_i^2$。</p>
<blockquote>
<p>模型正则化的内容请参见<a href="http://www.devtalking.com/articles/machine-learning-9/"> 机器学习笔记九之交叉验证、模型正则化 </a>。</p>
<h2 id="Scikit_Learn_u4E2D_u7684SVM"><a href="#Scikit_Learn_u4E2D_u7684SVM" class="headerlink" title="Scikit Learn中的SVM"></a>Scikit Learn中的SVM</h2><p>前面两小节介绍了SVM背后的数学原理，这一节来看看如何使用Scikit Learn中封装的SVM方法。我们还是使用之前使用过很多次的鸢尾花数据：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris();</span><br><span class="line"></span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取鸢尾花的两种类型，每种类型只取两个特征</span></span><br><span class="line"><span class="comment"># 我们先作用于二分类问题，并且为了绘图方便，所以先使用两个特征</span></span><br><span class="line">X = X[y &lt; <span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line">y = y[y &lt; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据绘制出来</span></span><br><span class="line">plt.scatter(X[y == <span class="number">0</span>, <span class="number">0</span>], X[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X[y == <span class="number">1</span>, <span class="number">0</span>], X[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a44b5821f9b48dbcb9fd85051dab83e3.jpg" alt=""></p>
<p>接下来我们使用Scikit Learn封装的SVM方法对样本数据进行分类：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里的参数C就是在讲Soft Margin SVM中的那个超参数，这里将其取一个很大的值</span></span><br><span class="line">svc = LinearSVC(C=<span class="number">1e9</span>)</span><br><span class="line">svc.fit(X_sd, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仍然使用之前绘制决策边界的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># meshgrid函数用两个坐标轴上的点在平面上画格，返回坐标矩阵</span></span><br><span class="line">	X0, X1 = np.meshgrid(</span><br><span class="line">		<span class="comment"># 随机两组数，起始值和密度由坐标轴的起始值决定</span></span><br><span class="line">		np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line">	<span class="comment"># ravel()方法将高维数组降为一维数组，c_[]将两个数组以列的形式拼接起来，形成矩阵</span></span><br><span class="line">	X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 通过训练好的逻辑回归模型，预测平面上这些点的分类</span></span><br><span class="line">	y_predict = model.predict(X_grid_matrix)</span><br><span class="line">	y_predict_matrix = y_predict.reshape(X0.shape)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 设置色彩表</span></span><br><span class="line">	<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">	my_colormap = ListedColormap([<span class="string">'#0000CD'</span>, <span class="string">'#40E0D0'</span>, <span class="string">'#FFFF00'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 绘制等高线，并且填充等高区域的颜色</span></span><br><span class="line">	plt.contourf(X0, X1, y_predict_matrix, linewidth=<span class="number">5</span>, cmap=my_colormap)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(svc, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">0</span>, <span class="number">0</span>], X_sd[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">1</span>, <span class="number">0</span>], X_sd[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/890c26515621cbce34e121af68d6f924.jpg" alt=""></p>
<p>在上面的示例代码中，我将超参数$C$取了一个非常大的数，那么为了平衡$min \frac 1 2 ||w||^2 + C \sum_{i=1}^m\zeta_i$整个函数，$\zeta$就得等于0才能让最大限度的平衡正则模型和整个目标函数。所以宽松量为零，就成了Hard Margin SVM。并且从上图可以看出，离决策边界最近的点，无论是红色点还是蓝色点，也就是红蓝支撑向量到决策边界的距离都差不多。</p>
<p>如果我们将超参数$C$的值取的小一些，那么整个问题就变成了Soft Margin SVM，来对比看看决策边界会有什么不同：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svc2 = LinearSVC(C=<span class="number">0.01</span>)</span><br><span class="line">svc2.fit(X_sd, y)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(svc2, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">0</span>, <span class="number">0</span>], X_sd[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">1</span>, <span class="number">0</span>], X_sd[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/26dc2972ed477705b9ef58d24ab96366.jpg" alt=""></p>
<p>此时从上图可以看到，红蓝支撑向量到决策边界的距离已经不一样了，并且还有一个红色的点被划到蓝色点的范围内，那么我们知道这都是因为Soft Margin SVM中加了宽松量的缘故。</p>
<h3 id="u7ED8_u5236_u652F_u6491_u5411_u91CF_u76F4_u7EBF"><a href="#u7ED8_u5236_u652F_u6491_u5411_u91CF_u76F4_u7EBF" class="headerlink" title="绘制支撑向量直线"></a>绘制支撑向量直线</h3><p>在上一小节，我们知道决策边界以及上下支撑向量构成的直线的公式分别是：</p>
<ul>
<li>决策边界：$w^Tx + d = 0$</li>
<li>上支撑向量直线：$w^Tx + d = 1$</li>
<li>下支撑向量直线：$w^Tx + d = -1$</li>
</ul>
<p>那么在鸢尾花的示例中，如何来求这三条直线呢？以决策边界直线为例，$w$和$d$其实我们已经知道了，就是特征系数和截距：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svc.coef_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">4.03243252</span>, -<span class="number">2.49295032</span>]])</span><br><span class="line"></span><br><span class="line">svc.intercept_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.95364645</span>])</span><br></pre></td></tr></table></figure>
<p>因为我们只用了鸢尾花的两个类型和两个特征，所以决策边界直线的公式可以展开为：</p>
<p>$$w_0 * x_0 + w_1 * x_1 + d = 0$$</p>
<p>将上面的公式转换为$y = ax + b$的形式：</p>
<p>$$w_1 * x_1 = - w_0 * x_0 - d  \\<br>x_1 = - \frac {w_0 * x_0} {w_1} - \frac {d} {w_1}$$</p>
<p>同理我们也可以将支撑向量直线的公式作以转换：</p>
<p>$$x_1 = - \frac {w_0 * x_0} {w_1} - \frac {d} {w_1} + \frac 1 {w_1}$$</p>
<p>$$x_1 = - \frac {w_0 * x_0} {w_1} - \frac {d} {w_1} - \frac 1 {w_1}$$<br>此时我们在给定的坐标系内，构建一组$x_0$，那么就可以求出一组$x_1$，然后将这些点连起来，就绘制出了决策边界的直线：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个绘制决策边界和上下支撑向量直线的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># 因为SVM可以解决多分类问题，所以特征系数和截距都是数组，在二分类问题下取第0个元素</span></span><br><span class="line">	w = model.coef_[<span class="number">0</span>]</span><br><span class="line">	d = model.intercept_[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 构建200个在axis范围内的，有线性关系的点，既构建x0</span></span><br><span class="line">	plot_x = np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据上面转换的公式，求出x1</span></span><br><span class="line">	y = - (w[<span class="number">0</span>] * plot_x)/w[<span class="number">1</span>] - d / w[<span class="number">1</span>]</span><br><span class="line">	up_y = - (w[<span class="number">0</span>] * plot_x)/w[<span class="number">1</span>] - d / w[<span class="number">1</span>] + <span class="number">1</span> / w[<span class="number">1</span>]</span><br><span class="line">	down_y = - (w[<span class="number">0</span>] * plot_x)/w[<span class="number">1</span>] - d / w[<span class="number">1</span>] - <span class="number">1</span> / w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 对y轴的上下边界作以限制</span></span><br><span class="line">	y_index = (y &gt;= axis[<span class="number">2</span>]) &amp; (y &lt;= axis[<span class="number">3</span>])</span><br><span class="line">	up_y_index = (up_y &gt;= axis[<span class="number">2</span>]) &amp; (up_y &lt;= axis[<span class="number">3</span>])</span><br><span class="line">	down_y_index = (down_y &gt;= axis[<span class="number">2</span>]) &amp; (down_y &lt;= axis[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">	plt.plot(plot_x[y_index], y[y_index], color=<span class="string">'black'</span>)</span><br><span class="line">	plt.plot(plot_x[up_y_index], up_y[up_y_index], color=<span class="string">'black'</span>)</span><br><span class="line">	plt.plot(plot_x[down_y_index], down_y[down_y_index], color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_decision_boundary(svc, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plot_svc_decision_boundary(svc, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">0</span>, <span class="number">0</span>], X_sd[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">1</span>, <span class="number">0</span>], X_sd[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ecf8b9cd82f869204f83be40a2a396d3.jpg" alt=""></p>
<p>从上图可以看到，是一个标准的Hard Margin SVM，Margin区域没有任何一个点，支撑向量一共有五个，红色点三个，蓝色点两个。</p>
<p>我们再来看看Soft Margin SVM的情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_decision_boundary(svc2, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plot_svc_decision_boundary(svc2, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">0</span>, <span class="number">0</span>], X_sd[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X_sd[y == <span class="number">1</span>, <span class="number">0</span>], X_sd[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f890f7e814104e7b8581049811da361f.jpg" alt=""></p>
<p>从上图中可以看出，Margin区域内有很多点，说明相比Hard Margin SVM，Soft Margin SVM增加了不少宽松量。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这一篇笔记主要讲机器学习算法中一个重要的分类算法，支撑向量机（Support Vector Machine）。它背后有严格的数学理论和统计学理论支撑的，这里我们只对它的原理和应用做以介绍，更深层次的数学理论有兴趣可以查阅其他资料。</p>
<h2 id="u4EC0_u4E48_u662FSVM"><a href="#u4EC0_u4E48_u662FSVM" class="headerlink" title="什么是SVM"></a>什么是SVM</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/5142c26ba0e86429631a63609c314a97.jpg" alt=""></p>
<p>上面的图展示的是一个二分类问题，在逻辑回归的笔记中，我们知道了决策边界，图中橘黄色的直线就是决策边界，它看似很好的将样本数据的不同分类区分开了。我们在多项式回归的笔记中，讲过模型泛化的问题，一个模型的好坏程度很大程度上是体现了模型泛化能力的好坏，也就是对未知样本的预测能力。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b26d6614606ac304144b99549eeb390e.jpg" alt=""></p>
<p>如果新来一个样本数据A，按照决策边界划分它是被分为了蓝色分类，可事实真的是如此吗，因为点A和红色分类的点非常近，很有可能点A是属于红色分类的。所以上图中的决策边界可能并不是最优的，那最优的决策边界应该是什么样的呢？</p>
<p>上面的决策边界之所以有缺陷，是因为它和红色类别的点太近了，当然肯定也不能和蓝色的点太近了，所以我们希望这条决策边界离红色分类和蓝色分类都尽可能的远，才能很好的避免上面出现的问题。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/990fbea083569d4503be599d202b049d.jpg" alt=""></p>
<p>想要决策边界离红蓝两个分类都尽可能的远，也就是红蓝分类两边离决策边界最近的点的距离要远，并且两边的点到决策边界的距离要相等，这才说明决策边界是不偏不倚的，如上图所示。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d4642c6673e5e75023088be725dcd4f8.jpg" alt=""></p>
<p>过红蓝两边离决策边界最近的点画两条直线，并且平行于决策边界，那么就确定了一个区域，在这个区域内不应该再有任何数据点出现。那么SVM就是尝试寻找这个区域中间那条最优的决策边界。这个区域边界上的点称为支撑向量，换句话说，是支撑向量定义出了这个区域，那么最优的决策边界也是由支撑向量决定的，这也是支撑向量机这个名称的由来。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f5af760ed417acf74d79d065b437807d.jpg" alt=""></p>
<p>如上图所示，由支撑向量确定的区域之间的距离称为Margin。那么SVM要做的事情就是最大化Margin，既将问题转化为了求最优解的问题。</p>
<p>以上对SVM的解释是基于线性可分问题的基础上，也就是能确确实实找到一条直线作为决策边界，这种问题称为Hard Margin SVM，但是很多真实的情况是线性不可分的，这类问题称为Soft Margin SVM，这两类问题都是支撑向量机可以处理的，但基础还是Hard Margin SVM。</p>]]>
    
    </summary>
    
      <category term="SVM" scheme="http://www.devtalking.com/tags/SVM/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十二之算法精准率、召回率、混淆矩阵]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-12/"/>
    <id>http://www.devtalking.com//articles/machine-learning-12/</id>
    <published>2018-05-14T16:00:00.000Z</published>
    <updated>2018-08-27T02:35:56.456Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>机器学习算法中有一个重要环节就是评判算法的好坏，我们在之间的笔记中讲过多种评价回归算法的评测标准，比如均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、$R^2$（R Squared）。但是在分类问题中我们一直使用分类准确度这一个指标，也就是预测对分类的样本数量除以总预测样本数量。但是这个方法存在很大的一个缺陷，所以这篇笔记主要介绍评价分类问题的方式方法。</p>
<h2 id="u6781_u5EA6_u504F_u659C_u6570_u636E_uFF08Skewed_Data_uFF09"><a href="#u6781_u5EA6_u504F_u659C_u6570_u636E_uFF08Skewed_Data_uFF09" class="headerlink" title="极度偏斜数据（Skewed Data）"></a>极度偏斜数据（Skewed Data）</h2><p>为什么说分类准确度这个指标存在很大的一个缺陷呢。举个例子，假设有一个癌症预测系统，输入体检信息，判断是否患有癌症。我们知道世界上相对于其他病症，患癌症的比例还是很小，如果癌症产生的概率只有0.1%，那么有99.9%的人都不会患有癌症。这就意味着，就算癌症预测系统什么都不做，但凡有体检信息输入，就给出没有患癌症的结果，那准确率也是达到了99.9%。那此时这个分类准确度是真实的吗？所以当样本数据或领域的实际情况存在数据极度偏斜的时候，只使用分类准确度这个指标是远远不够的。</p>
<h2 id="u6DF7_u6DC6_u77E9_u9635_uFF08Confusion_Matrix_uFF09"><a href="#u6DF7_u6DC6_u77E9_u9635_uFF08Confusion_Matrix_uFF09" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>这一节介绍一个能进一步分析分类结果的工具，混淆矩阵。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c997c342900bf1764c1982113ad3e571.jpg" alt=""></p>
<p>上面这个表针对二分类问题，所有将类别就分为两类0和1，0表示Negative，类似医院上的阴性，1表示Positive，类似医学上的阳性。行代表真实值，列代表预测值。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/11d0bcf47d48fee84429d1f387b21847.jpg" alt=""></p>
<p>如上图所示：</p>
<ul>
<li>(0, 0)格子真实值和预测值都为0，称为预测Negative正确，记作True Negative，简写为TN。</li>
<li>(0, 1)格子真实值为0，但预测值为1，称为预测Positive错误，记作False Positive，简写为FP。</li>
<li>(1, 0)格子真实值为1， 但预测值为0，称为预测Negative错误，记作False Negative，简写为FN。</li>
<li>(1, 1)格子真实值和预测值都为1，称为预测Positive正确，记作True Positive，简写为TP。</li>
</ul>
<p>以上这个表格就叫做混淆矩阵。举个例子，如果对10000个人预测他们是否患癌症，通过混淆矩阵表示出的真实情况就是：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c3d5ddc7046eae2076c334850889e4e8.jpg" alt=""></p>
<p>解读一下：</p>
<ul>
<li>没有患癌症，系统也预测出没有患癌症的人为9978人（TN）。</li>
<li>没有患癌症，但系统预测出患癌症的人为12人（FP）。</li>
<li>患有癌症，但系统预测出没有患癌症的人为2人（FN）。</li>
<li>患有癌症，系统也预测出患有癌症的人为8人（TP）。</li>
</ul>
<a id="more"></a>
<h2 id="u7CBE_u51C6_u7387_uFF08Presicion_uFF09"><a href="#u7CBE_u51C6_u7387_uFF08Presicion_uFF09" class="headerlink" title="精准率（Presicion）"></a>精准率（Presicion）</h2><p>分类问题的精准率是建立在混淆矩阵的基础上的，那么精准率的公式为：</p>
<p>$$precision = \frac {TP} {TP + FP}$$</p>
<p>还是以预测10000人是否患癌症的例子来说明：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9728167b52fe04a016ed59fdb15add2b.jpg" alt=""></p>
<p>这个例子中，预测患癌的精准率是8 / (8+12) = 40%，既真实患癌并预测出患癌的人数在所有预测出患癌人数中的占比。</p>
<h2 id="u53EC_u56DE_u7387_uFF08Recall_uFF09"><a href="#u53EC_u56DE_u7387_uFF08Recall_uFF09" class="headerlink" title="召回率（Recall）"></a>召回率（Recall）</h2><p>分类问题的召回率同样也是建立在混淆矩阵的基础上的，召回率的公式为：</p>
<p>$$recall = \frac {TP} {TP + FN}$$</p>
<p>以预测10000人是否患癌症的例子来说明：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c7b2bfae1b3c119a87ae471c346b271e.jpg" alt=""></p>
<p>这个例子中，预测患癌的召回率是8 / (8+2) = 80%，既真实患癌并预测出患癌的人数在真实患癌总人数中的占比。</p>
<p>我们之前解释过分类准确度存在的缺陷，那么在这个例子中，我们通过混淆矩阵来直观的看一下这个缺陷。我们的前提是10000个人中患癌的人占比为0.1%，那么健康的人占比为99%，在这种情况下，有极大的可能出现分类准确度为99.9%，但是实际上一个患癌的人都没有预测出来。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/3bd489890b33ecaa9670e6e651686969.jpg" alt=""></p>
<p>上面这个混淆矩阵就满足我们假定的这个情况，现在来看看分类准确度、精准率和召回率分别是多少：</p>
<ul>
<li>分类准确度：9990 / 10000 = 99.9%</li>
<li>精准率：0 /  (0+0)，无意义，既为0</li>
<li>召回率：0 / (10 +0) = 0</li>
</ul>
<p>现在可以很清晰的看出分类准确度存在的缺陷和混淆矩阵对分类评价的重要性了。</p>
<h2 id="u5B9E_u73B0_u7CBE_u51C6_u7387_u548C_u53EC_u56DE_u7387"><a href="#u5B9E_u73B0_u7CBE_u51C6_u7387_u548C_u53EC_u56DE_u7387" class="headerlink" title="实现精准率和召回率"></a>实现精准率和召回率</h2><p>有了上面的定义，下面我们来实现一下混淆矩阵、精准率和召回率。首先我们使用手写数据作为样本数据，因为手写数据是多分类问题，所以还要对其做一下处理，转换为二分类问题，同时让其数据产生极度偏差：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用手写数据作为样本数据</span></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多分类问题转换为二分类问题，同时让样本数据产生极度偏斜，</span></span><br><span class="line"><span class="comment"># 也就是我们关注的数据占总数据的1/9</span></span><br><span class="line">y[digits.target == <span class="number">9</span>] = <span class="number">1</span></span><br><span class="line">y[digits.target != <span class="number">9</span>] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>样本数据构建好后，我们先使用逻辑回归训练出模型，先看看分类准确度指标：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line">log_reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.97555555555555551</span></span><br></pre></td></tr></table></figure>
<p>下面我们来逐个实现混淆矩阵中的TN、FP、FN、TP：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用逻辑回归模型计算测试特征数据的预测目标值</span></span><br><span class="line">y_predict = log_reg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TN</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict)</span><br><span class="line">	<span class="keyword">return</span> np.sum((y_true == <span class="number">0</span>) &amp; (y_predict == <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">TN(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">403</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FP</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict)</span><br><span class="line">	<span class="keyword">return</span> np.sum((y_true == <span class="number">0</span>) &amp; (y_predict == <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">FP(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FN</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict)</span><br><span class="line">	<span class="keyword">return</span> np.sum((y_true == <span class="number">1</span>) &amp; (y_predict == <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">FN(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TP</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict)</span><br><span class="line">	<span class="keyword">return</span> np.sum((y_true == <span class="number">1</span>) &amp; (y_predict == <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">TP(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">36</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">confusion_matrix</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> np.array([</span><br><span class="line">		[TN(y_test, y_predict), FP(y_test, y_predict)],</span><br><span class="line">		[FN(y_test, y_predict), TP(y_test, y_predict)]</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">confusion_matrix(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">403</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">9</span>,  <span class="number">36</span>]])</span><br></pre></td></tr></table></figure>
<p>我们通过上面的混淆矩阵来分析一下：</p>
<ul>
<li><code>y</code>被拆分后的测试数据量<code>y_test</code>为450，混淆矩阵中的四个熟总和为450。</li>
<li>真值为0，但预测值为1有2个。</li>
<li>真值为1，但预测值为0有9个。</li>
<li>真值为1，预测值也为1有36个。</li>
</ul>
<p>也就是全部预测对的有439个，分类准确度为439 / 450 = 97.56%。下面来看看如何实现精准率和召回率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 精准率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precision_score</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	tp = TP(y_test, y_predict)</span><br><span class="line">	fp = FP(y_test, y_predict)</span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		<span class="keyword">return</span> tp / (tp + fp) <span class="comment"># 避免分母为0报错 </span></span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">	</span><br><span class="line">precision_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.94736842105263153</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 召回率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recall_score</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	tp = TP(y_test, y_predict)</span><br><span class="line">	fn = FN(y_test, y_predict)</span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		<span class="keyword">return</span> tp / (tp + fn) <span class="comment"># 避免分母为0报错 </span></span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">	</span><br><span class="line">recall_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80000000000000004</span></span><br></pre></td></tr></table></figure>
<p>以上是我们根据定义自己实现的混淆矩阵和精准率、召回率。下面来看看Scikit Learn中封装的它们：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Scikit Learn 中的混淆矩阵</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">confusion_matrix(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">403</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">9</span>,  <span class="number">36</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scikit Learn中的精准率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line">precision_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.94736842105263153</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scikit Learn中的召回率</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line">recall_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80000000000000004</span></span><br></pre></td></tr></table></figure>
<h2 id="F1_Score"><a href="#F1_Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>上一节介绍了精准率和召回率，那么如果在一个指标好，一个指标不好的情况下，如何确定一个模型的好坏呢？这就要分情况而视了。像预测股市的系统中，一般主要关注精准率，也就是关注在预测出要涨的股票中的准确程度。那么像医疗相关的系统中就会主要关注召回率，也就是关注在真正患病的人群中预测出的准确程度。如果两个指标都要关注的话，就要引入第三个指标了，那就是F1 Score。</p>
<h3 id="F1_Score_u5B9A_u4E49"><a href="#F1_Score_u5B9A_u4E49" class="headerlink" title="F1 Score定义"></a>F1 Score定义</h3><p>我们看多个数的综合情况时，一般情况都会求这些数的平均值，称为算数平均值。但是在机器学习中，算数平均值是有缺陷的，因为它们是求和然后取平均，如果大多数数很大，个别几个数很小的话，平均值并不会被拉下来，但作为机器学习模型的评测标准，可能只要有一个指标不好，那么整个模型就不是一个好的模型。所以我们得使用调和平均值：</p>
<p>$$\frac 1 {F1} = \frac 1 2 (\frac 1 {precision} + \frac 1 {recall})$$</p>
<p>最后换算下来的最终F1 Score公式为：</p>
<p>$$F1 = \frac {2 \cdot precision \cdot recall} {precision + recall}$$</p>
<p>调和平均值的最大特点就是，精准率和召回率只要有一个比较小的话，整个F1 Score也会被拉下来，既避免了算数平均值在评估机器学习算法分类模型时的缺陷。</p>
<h3 id="u5B9E_u73B0F1_Score"><a href="#u5B9E_u73B0F1_Score" class="headerlink" title="实现F1 Score"></a>实现F1 Score</h3><p>我们知道了F1 Score的定义后，实现它就很容易了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f1_score</span><span class="params">(precision, recall)</span>:</span></span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">precision = <span class="number">0.5</span></span><br><span class="line">recall = <span class="number">0.5</span></span><br><span class="line">f1_score(precision, recall)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">precision = <span class="number">0.1</span></span><br><span class="line">recall = <span class="number">0.9</span></span><br><span class="line">f1_score(precision, recall)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.18000000000000002</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看到，当两个指标相等时，F1的值也和它们相等。当其中一个指标比较小时，F1的值也会被拉的比较小。</p>
<h3 id="Scikit_Learn__u4E2D_u7684_F1_Score"><a href="#Scikit_Learn__u4E2D_u7684_F1_Score" class="headerlink" title="Scikit Learn 中的 F1 Score"></a>Scikit Learn 中的 F1 Score</h3><p>Scikit Learn中也封装了F1 Score，但是它封装时传的参数和我们实现的不太一样，它只需要传入真值和预测值既可，精准率和召回率是在函数中计算的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">f1_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.86746987951807231</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看到上一小节中的手写数据的例子，虽然分类准确度达到了97.56%，但是F1 Score只有86.75%，而这个86.75%才是更能真正反应模型好坏程度的指标。</p>
<h2 id="u7CBE_u51C6_u7387_u548C_u53EC_u56DE_u7387_u76F4_u63A5_u7684_u5E73_u8861"><a href="#u7CBE_u51C6_u7387_u548C_u53EC_u56DE_u7387_u76F4_u63A5_u7684_u5E73_u8861" class="headerlink" title="精准率和召回率直接的平衡"></a>精准率和召回率直接的平衡</h2><p>在上一篇笔记中，我们了解了逻辑回归的决策边界，比如在二分类问题中，决策边界公式为：</p>
<p>$$\theta^T \cdot X_b = 0$$</p>
<p>当$\theta^T \cdot X_b$大于0时，我们认为分类是1，当小于0时，我们认为分类为0。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/dacb284241679250a45def7cdb71b20d.jpg" alt=""></p>
<p>如上图所示，黑色直线表示$\theta^T \cdot X_b$，橘黄色直线所在位置表示区分类别为1还是0的分界点，既大于0是蓝色点类型，小于0是红色点类型。那如果我们让$\theta^T \cdot X_b$不等于0，而等于一个阀值$threshold$呢？</p>
<p>$$\theta^T \cdot X_b = threshold$$</p>
<p>那上面的图就会是下面这样：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/4c58ce92799ae9ae2ea93d82a60f3b2a.jpg" alt=""></p>
<p>从上面的图看，$threshold$是大于0的，这样就相当于调整了区别分类的分界点位置。那么会影响到什么呢？</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d709771df57fa17b98c3b2876ae09cf0.jpg" alt=""></p>
<p>从上图可以看到，当$threshold$为0时，示例中的精准率是0.86，召回率是0.75。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ba261865bdd68f808b0e8bc0b0485341.jpg" alt=""></p>
<p>当调整$threshold$大于0后，示例中的精准率是1，召回率是0.38。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f6d1880cbbc985789de681fb45e86e7a.jpg" alt=""></p>
<p>当调整$threshold$小于0后，示例中的精准率是0.7，召回率是0.88。</p>
<p>从这三种情况可以看出，精准率和召回率是互相牵制的，精准率高了，召回率就低。召回率高，精准率就低。所以$threshold$就又是一个超参数，用来调节使精准率和召回率达到平衡。</p>
<h3 id="u901A_u8FC7_u7A0B_u5E8F_u9A8C_u8BC1_u7CBE_u51C6_u7387_u548C_u53EC_u56DE_u7387_u7684_u5E73_u8861_u5173_u7CFB"><a href="#u901A_u8FC7_u7A0B_u5E8F_u9A8C_u8BC1_u7CBE_u51C6_u7387_u548C_u53EC_u56DE_u7387_u7684_u5E73_u8861_u5173_u7CFB" class="headerlink" title="通过程序验证精准率和召回率的平衡关系"></a>通过程序验证精准率和召回率的平衡关系</h3><p>我们还是使用手写数据的样本数据来验证：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="comment"># 使用手写数据作为样本数据</span></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将多分类问题转换为二分类问题，同时让样本数据产生极度偏斜，</span></span><br><span class="line"><span class="comment"># 也就是我们关注的数据占总数据的1/9</span></span><br><span class="line">y[digits.target == <span class="number">9</span>] = <span class="number">1</span></span><br><span class="line">y[digits.target != <span class="number">9</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line">y_predict = log_reg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">f1_score(y_test, y_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.86746987951807231</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">confusion_matrix(y_test, y_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">403</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">9</span>,  <span class="number">36</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line">precision_score(y_test, y_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.94736842105263153</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line">recall_score(y_test, y_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80000000000000004</span></span><br></pre></td></tr></table></figure>
<p>我们如何设置$threshold$呢，其实Scikit Learn中的逻辑回归提供了一个获取评判分数的函数，也就是上图中黑色直线的Score值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">decision_score = log_reg.decision_function(X_test)</span><br><span class="line">decision_score.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">450</span>,)</span><br></pre></td></tr></table></figure>
<p>Scikit Learn中的<code>confusion_matrix</code>、<code>precision_score</code>、<code>recall_score</code>函数都是基于$threshold$为0计算的，也就是判断<code>decision_score</code>中的所有值，如果大于0就分类为1，如果小于0就分类为0。那我们现在将$threshold$调大一点，比如将5作为区分1和0的分界点，那么我们的预测值就可以这样求：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict2 = np.array(decision_score &gt;= <span class="number">5</span>, dtype=<span class="string">'int'</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们再来看看精准率和召回率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">precision_score(y_test, y_predict2)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.95999999999999996</span></span><br><span class="line"></span><br><span class="line">recall_score(y_test, y_predict2)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.53333333333333333</span></span><br></pre></td></tr></table></figure>
<p>再将$threshold$调小看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict3 = np.array(decision_score &gt;= -<span class="number">5</span>, dtype=<span class="string">'int'</span>)</span><br><span class="line">precision_score(y_test, y_predict3)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.72727272727272729</span></span><br><span class="line"></span><br><span class="line">recall_score(y_test, y_predict3)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.88888888888888884</span></span><br></pre></td></tr></table></figure>
<p>通过代码我们可以很明显的看到调节$threshold$后，精准率和召回率的变化。</p>
<h2 id="PR_u66F2_u7EBF"><a href="#PR_u66F2_u7EBF" class="headerlink" title="PR曲线"></a>PR曲线</h2><p>通过上一小节我们知道精准率和召回率是相互牵制的，我也认识了一个新的超参数$threshold$，通过它能调节精准率和召回率。那么我们如何找到一个平衡点，使得精准率和召回率都在一个比较好的水平，换句话说也就是如何找到好的超参数$threshold$。</p>
<p>这一小节就介绍一个工具，帮助我们更好的找到这个超参数，这就是PR曲线（Precision-Recall曲线）。我们直接来看看Scikit Learn中提供的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_test, decision_score)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(thresholds, precisions[:-<span class="number">1</span>])</span><br><span class="line">plt.plot(thresholds, recalls[:-<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/60696f6a0f96e52e334313400d73102d.jpg" alt=""></p>
<p>上图的横轴是$threshold$值，蓝色曲线是精准率，黄色曲线是召回率，他们相交点的$threshold$值，就是PR达到平衡的点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(precisions, recalls)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/74ba40d0f2b2d54a85890834ecaeb616.jpg" alt=""></p>
<p>上图中，横轴是精准率，纵轴是召回率。这个图反应了PR的总体趋势。通过这个PR曲线我们除了可以判断选择最优的$threshold$值，还可以判断不同模型的好坏程度。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/128711011f4a4e1249040612c72b62d4.jpg" alt=""></p>
<p>比如上图中的模型A和模型B可以是通过不同的算法训练的出的模型，也可以是同一个算法，通过不同超参数组合训练出的模型。显然模型B要比模型A好，因为模型B无论是精准率还是召回率都要比模型A的高。</p>
<h2 id="ROC_u66F2_u7EBF"><a href="#ROC_u66F2_u7EBF" class="headerlink" title="ROC曲线"></a>ROC曲线</h2><p>这一小节我们来看一个新的指标，ROC曲线，既接收者操作特征曲线，是Receiver Operation Characteristic Curve缩写，最早出现在信号检测理论中，后来被广泛应用在不同领域。在机器学习中，ROC用来描述分类模型的TPR和FPR之间的关心，从而确定分类模型的好坏。</p>
<h3 id="FPR_u548CTPR"><a href="#FPR_u548CTPR" class="headerlink" title="FPR和TPR"></a>FPR和TPR</h3><p>FPR和TPR同样是基于混淆矩阵而来的，FPR的公式为：</p>
<p>$$FPR=\frac {FP} {TN + FP}$$</p>
<p>TPR的公式为：</p>
<p>$$TPR = \frac {TP} {TP + FN}$$</p>
<p>可以看到TPR其实就是Recall指标，而FPR是和TPR相反的指标。下面我们使用Scikit Learn中封装的方法来看看手写数据的TPR、FPR和ROC曲线：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fprs, tprs, thresholds = roc_curve(y_test, decision_score)</span><br><span class="line"></span><br><span class="line">plt.plot(fprs, tprs)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/947c72645b6d0aa3a5d2f2a4d39e1719.jpg" alt=""></p>
<p>从ROC曲线图可以看出，随着FPR的增大，TPR也是随之增大的。我们通过观察这根曲线下的面积大小来判断分类模型的好坏程度，面积越大，说明分类模型越好。Scikit Learn中也提供了计算这个面积的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">roc_auc_score(y_test, decision_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.98304526748971188</span></span><br></pre></td></tr></table></figure>
<p>ROC曲线和PR曲线有一个不同之处是，ROC曲线对极度有偏的数据是不敏感的。所以如果样本数据有极度有偏的情况时，通常还是主要使用PR曲线来判断模型的好坏，ROC曲线辅助判断。</p>
<h2 id="u591A_u5206_u7C7B_u95EE_u9898_u4E2D_u7684_u6DF7_u6DC6_u77E9_u9635"><a href="#u591A_u5206_u7C7B_u95EE_u9898_u4E2D_u7684_u6DF7_u6DC6_u77E9_u9635" class="headerlink" title="多分类问题中的混淆矩阵"></a>多分类问题中的混淆矩阵</h2><p>我们之前讲的混淆矩阵和精准、召回率都是在二分类问题的前提下。这篇笔记的最后来看看多分类问题中的混淆矩阵。我们同样使用手写数字数据，但这次不再对数据做极度有偏处理了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line">log_reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.93949930458970787</span></span><br></pre></td></tr></table></figure>
<p>Scikit Learn 的<code>precision_score</code>方法有一个<code>average</code>参数，默认值为<code>binary</code>，既默认计算二分类问题。如果要计算多分类问题，需要将<code>average</code>参数设置为<code>micro</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"></span><br><span class="line">precision_score(y_test, y_predict, average=<span class="string">'micro'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.93949930458970787</span></span><br></pre></td></tr></table></figure>
<p>下面来看看这个手写数字十分类问题的混淆矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">confusion_matrix(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">141</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>],</span><br><span class="line">	   [  <span class="number">0</span>, <span class="number">132</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">2</span>,   <span class="number">0</span>,   <span class="number">4</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">2</span>, <span class="number">141</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">1</span>, <span class="number">131</span>,   <span class="number">0</span>,   <span class="number">5</span>,   <span class="number">1</span>,   <span class="number">0</span>,  <span class="number">10</span>,   <span class="number">0</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>,   <span class="number">0</span>, <span class="number">136</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">1</span>,   <span class="number">4</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>,   <span class="number">0</span>, <span class="number">141</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">2</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>, <span class="number">146</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>, <span class="number">137</span>,   <span class="number">2</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">9</span>,   <span class="number">3</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">4</span>,   <span class="number">4</span>,   <span class="number">1</span>, <span class="number">120</span>,   <span class="number">3</span>],</span><br><span class="line">	   [  <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>,   <span class="number">6</span>,   <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">9</span>, <span class="number">126</span>]])</span><br></pre></td></tr></table></figure>
<p>看多分类问题的混淆矩阵和二分类问题的混淆矩阵方法一样，同样行表示真值，列表示预测值。从上面的结果可看到，混淆矩阵的对角线数值最大，这个对角线就是真值和预测值相同的TP值。我们将这个多分类混淆矩阵通过Matplotlib的<code>matshow</code>方法绘制出来，直观的看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cfm = confusion_matrix(y_test, y_predict)</span><br><span class="line"><span class="comment"># cmap是colormap，既将绘制的矩阵的每个点映射成什么颜色，这里映射成灰度值</span></span><br><span class="line">plt.matshow(cfm, cmap=plt.cm.gray)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ee9c81360959e29c827b5d9f26f7e6dd.jpg" alt=""></p>
<p>上面这个图可以很清晰的看到TP值，但是我们希望能从图上直观的分析问题，既这个模型预测错误的数据。下面我们将混淆矩阵做一下转换，求出错误矩阵，既FP值矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先求出一个向量，这个向量的每个元素表示每个手写数字有多少个样本，也就是将混淆矩阵在列方向，将每行的数加起来。</span></span><br><span class="line">row_sums = np.sum(cfm, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 然后让混淆矩阵中的每个元素和它所在那一行的求和相除，既得到了每个数字的预测召回率</span></span><br><span class="line">err_matrix = cfm / row_sums</span><br><span class="line"><span class="comment"># 通过Numpy的fill_diagonal方法，将错误矩阵的对角线的值都替换成0，因为我们主要看FP，所以要消除掉最高的精准率</span></span><br><span class="line">np.fill_diagonal(err_matrix, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 最后同样用灰度值将错误矩阵绘制出来</span></span><br><span class="line">plt.matshow(err_matrix, cmap=plt.cm.gray)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/1e6cad586f860101d4a0a1fef57e28ce.jpg" alt=""></p>
<p>上图中，颜色约亮的格子表示预测错误的数量越多，比如左上角那个白色的格子就表示真值为3，但是有不少样本数据被预测成了8。左下角的白色格子表示真值为8，但是有不少样本数据被预测成了1。所以从这个错误矩阵上可以很好的分析出具体的预测错误点，从而根据这些信息调整分类模型或者样本数据。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>机器学习算法中有一个重要环节就是评判算法的好坏，我们在之间的笔记中讲过多种评价回归算法的评测标准，比如均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）、$R^2$（R Squared）。但是在分类问题中我们一直使用分类准确度这一个指标，也就是预测对分类的样本数量除以总预测样本数量。但是这个方法存在很大的一个缺陷，所以这篇笔记主要介绍评价分类问题的方式方法。</p>
<h2 id="u6781_u5EA6_u504F_u659C_u6570_u636E_uFF08Skewed_Data_uFF09"><a href="#u6781_u5EA6_u504F_u659C_u6570_u636E_uFF08Skewed_Data_uFF09" class="headerlink" title="极度偏斜数据（Skewed Data）"></a>极度偏斜数据（Skewed Data）</h2><p>为什么说分类准确度这个指标存在很大的一个缺陷呢。举个例子，假设有一个癌症预测系统，输入体检信息，判断是否患有癌症。我们知道世界上相对于其他病症，患癌症的比例还是很小，如果癌症产生的概率只有0.1%，那么有99.9%的人都不会患有癌症。这就意味着，就算癌症预测系统什么都不做，但凡有体检信息输入，就给出没有患癌症的结果，那准确率也是达到了99.9%。那此时这个分类准确度是真实的吗？所以当样本数据或领域的实际情况存在数据极度偏斜的时候，只使用分类准确度这个指标是远远不够的。</p>
<h2 id="u6DF7_u6DC6_u77E9_u9635_uFF08Confusion_Matrix_uFF09"><a href="#u6DF7_u6DC6_u77E9_u9635_uFF08Confusion_Matrix_uFF09" class="headerlink" title="混淆矩阵（Confusion Matrix）"></a>混淆矩阵（Confusion Matrix）</h2><p>这一节介绍一个能进一步分析分类结果的工具，混淆矩阵。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c997c342900bf1764c1982113ad3e571.jpg" alt=""></p>
<p>上面这个表针对二分类问题，所有将类别就分为两类0和1，0表示Negative，类似医院上的阴性，1表示Positive，类似医学上的阳性。行代表真实值，列代表预测值。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/11d0bcf47d48fee84429d1f387b21847.jpg" alt=""></p>
<p>如上图所示：</p>
<ul>
<li>(0, 0)格子真实值和预测值都为0，称为预测Negative正确，记作True Negative，简写为TN。</li>
<li>(0, 1)格子真实值为0，但预测值为1，称为预测Positive错误，记作False Positive，简写为FP。</li>
<li>(1, 0)格子真实值为1， 但预测值为0，称为预测Negative错误，记作False Negative，简写为FN。</li>
<li>(1, 1)格子真实值和预测值都为1，称为预测Positive正确，记作True Positive，简写为TP。</li>
</ul>
<p>以上这个表格就叫做混淆矩阵。举个例子，如果对10000个人预测他们是否患癌症，通过混淆矩阵表示出的真实情况就是：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c3d5ddc7046eae2076c334850889e4e8.jpg" alt=""></p>
<p>解读一下：</p>
<ul>
<li>没有患癌症，系统也预测出没有患癌症的人为9978人（TN）。</li>
<li>没有患癌症，但系统预测出患癌症的人为12人（FP）。</li>
<li>患有癌症，但系统预测出没有患癌症的人为2人（FN）。</li>
<li>患有癌症，系统也预测出患有癌症的人为8人（TP）。</li>
</ul>]]>
    
    </summary>
    
      <category term="F1 Score" scheme="http://www.devtalking.com/tags/F1-Score/"/>
    
      <category term="PR曲线" scheme="http://www.devtalking.com/tags/PR%E6%9B%B2%E7%BA%BF/"/>
    
      <category term="ROC曲线" scheme="http://www.devtalking.com/tags/ROC%E6%9B%B2%E7%BA%BF/"/>
    
      <category term="召回率" scheme="http://www.devtalking.com/tags/%E5%8F%AC%E5%9B%9E%E7%8E%87/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="混淆矩阵" scheme="http://www.devtalking.com/tags/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/"/>
    
      <category term="精准率" scheme="http://www.devtalking.com/tags/%E7%B2%BE%E5%87%86%E7%8E%87/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于OmniFocus的任务系统]]></title>
    <link href="http://www.devtalking.com//articles/task-manage-omnifocus/"/>
    <id>http://www.devtalking.com//articles/task-manage-omnifocus/</id>
    <published>2018-04-30T16:00:00.000Z</published>
    <updated>2018-06-14T16:43:16.000Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="OmniFocus_u7684_u51E0_u4E2A_u4E3B_u8981_u6982_u5FF5"><a href="#OmniFocus_u7684_u51E0_u4E2A_u4E3B_u8981_u6982_u5FF5" class="headerlink" title="OmniFocus的几个主要概念"></a>OmniFocus的几个主要概念</h2><p>文章中所有的内容都是以OmniFocus 3为例。</p>
<ul>
<li>文件夹：OmniFocus中任务的组织结构最顶级一层，但只是用于分类，本身没有什么属性。</li>
<li>项目：顾名思义，用于定义我们生活和工作中大大小小的项目，项目本身也有属性设置，可设置标签、其内任务的关系（并行、串行、单个），截止日期等。项目下可包含多个任务。</li>
<li>任务：OmniFocus中承载事件的最小颗粒。因为OmniFocus支持任务无限嵌套，所以当多个任务在一个任务下时该任务就成为一个任务组。</li>
<li>任务组：任务组本身也是一个任务，但是它又是其他任务的父任务，所以任务组有子任务关系（并行、串行、单个）的设置。</li>
<li>标签：任务、项目身上都可以设置一到多个标签。标签可以嵌套，并且标签支持设置地理位置，并且有根据进入，走出两个行为和距离范围设置通知。</li>
<li>透视：根据各种条件筛选任务的视图。比如查看有标签“办公室”，即将截至的任务。条件可以按照包含任意、全部包含和不包含进行组合。然后还可以设置筛选出来任务的排列、分组方式。</li>
</ul>
<h2 id="u4EFB_u52A1_u7CFB_u7EDF"><a href="#u4EFB_u52A1_u7CFB_u7EDF" class="headerlink" title="任务系统"></a>任务系统</h2><p>我参照John Z. Sonmez的看板+番茄钟的系统，将其调整为OmniFocus+番茄钟的系统。</p>
<h3 id="u7CFB_u7EDF_u8BBE_u7F6E"><a href="#u7CFB_u7EDF_u8BBE_u7F6E" class="headerlink" title="系统设置"></a>系统设置</h3><ul>
<li>在截止日期这一项，将即将截止日期的表示设置为今天。</li>
<li>今天和Watch这一项，设置为自定义透视的<strong>今日</strong>（该透视下面会讲到）。</li>
</ul>
<h3 id="u6784_u5EFA_u7CFB_u7EDF_u7684_u4E3B_u8981_u6807_u7B7E"><a href="#u6784_u5EFA_u7CFB_u7EDF_u7684_u4E3B_u8981_u6807_u7B7E" class="headerlink" title="构建系统的主要标签"></a>构建系统的主要标签</h3><ul>
<li>仪式：周期性重复的任务会设置该标签。</li>
<li>地点：标明某个任务能在何种场所进行。<ul>
<li>办公室</li>
<li>家</li>
<li>通勤中</li>
</ul>
</li>
<li>状态：进行任务时需要投入的专注度。<ul>
<li>集中精力</li>
<li>放松</li>
</ul>
</li>
<li>跟踪：需要跟踪的任务会设置该标签，比如安排下去的任务需要在某个时间点知道完成的结果。</li>
<li>采购：需要购买东西的任务会设置该标签。</li>
<li>星期：规划一周任务时设置的标签。<ul>
<li>周一</li>
<li>周二</li>
<li>周三</li>
<li>周四</li>
<li>周五</li>
<li>周六</li>
<li>周日</li>
</ul>
</li>
<li>二分钟：能在很短时间内完成的任务会设置该标签。</li>
</ul>
<a id="more"></a>
<h3 id="u4EFB_u52A1_u6C60_u900F_u89C6"><a href="#u4EFB_u52A1_u6C60_u900F_u89C6" class="headerlink" title="任务池透视"></a>任务池透视</h3><h4 id="u529E_u516C_u5BA4-_u805A_u7126"><a href="#u529E_u516C_u5BA4-_u805A_u7126" class="headerlink" title="办公室-聚焦"></a>办公室-聚焦</h4><p>该透视能筛选出所有我需要在办公室内并且投入100%专注度要做的事情。是我在规划一周内工作的主要透视视图。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：办公室、集中精力</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周六、周日</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：到期 </li>
</ul>
<h4 id="u529E_u516C_u5BA4-_u653E_u677E"><a href="#u529E_u516C_u5BA4-_u653E_u677E" class="headerlink" title="办公室-放松"></a>办公室-放松</h4><p>该透视能筛选出所有我需要在办公室内处理，但是不需要投入很多专注度的事情。是我在规划一周内工作时补充任务的透视，还有当每天清空任务后，会从该透视中进行补充。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：办公室、放松</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周六、周日</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：到期 </li>
</ul>
<h4 id="u5BB6-_u805A_u7126"><a href="#u5BB6-_u805A_u7126" class="headerlink" title="家-聚焦"></a>家-聚焦</h4><p>该透视能筛选出所有我需要在家并且投入100%专注度要做的事情。是我在规划一周内业余时间工作的主要透视视图。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：家、集中精力</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周六、周日</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：到期 </li>
</ul>
<h4 id="u5BB6-_u653E_u677E"><a href="#u5BB6-_u653E_u677E" class="headerlink" title="家-放松"></a>家-放松</h4><p>该透视能筛选出所有我需要在家处理，但是不需要投入很多专注度的事情。是我在规划一周内业余时间工作时补充任务的透视。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：家、放松</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周六、周日</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：到期 </li>
</ul>
<h4 id="u901A_u52E4"><a href="#u901A_u52E4" class="headerlink" title="通勤"></a>通勤</h4><p>该透视能筛选出所有我需要在通勤路上处理的事情，比如回邮件、看书等。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：通勤中</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周六、周日</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：已标注</li>
</ul>
<h4 id="u8DDF_u8E2A"><a href="#u8DDF_u8E2A" class="headerlink" title="跟踪"></a>跟踪</h4><p>该透视能筛选出所有我需要跟踪处理的事情，比如我需要在两天后收到报告等。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：跟踪</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：已标注</li>
</ul>
<h3 id="u4EFB_u52A1_u67E5_u770B_u900F_u89C6"><a href="#u4EFB_u52A1_u67E5_u770B_u900F_u89C6" class="headerlink" title="任务查看透视"></a>任务查看透视</h3><p>上面的透视其实都是我的任务池，只不过区分了不同的地点和精力状态，根据具体情况从这些任务池中挑选出任务，也就是规划工作的过程。那么任务挑选的问题就来了，如何挑选？如何处理任务算是已挑选？挑选出的任务去哪看？</p>
<h4 id="u5982_u4F55_u5904_u7406_u4EFB_u52A1_u7B97_u662F_u5DF2_u6311_u9009_uFF1F"><a href="#u5982_u4F55_u5904_u7406_u4EFB_u52A1_u7B97_u662F_u5DF2_u6311_u9009_uFF1F" class="headerlink" title="如何处理任务算是已挑选？"></a>如何处理任务算是已挑选？</h4><p>我先从挑选任务说起，我挑选任务有三步：</p>
<ul>
<li>给任务设置截至日期。</li>
<li>给任务设置对应周的标签。</li>
<li>编辑任务标题，注明需要几个番茄钟。</li>
</ul>
<p>给任务设置截至日期的范围一般是本周内，比如周一规划任务，那么给任务设置的最晚截至日期是周日，也就是说，如果某个任务的到期日超过了本周，那么在本周规划时，在<strong>设置截至日期这个维度</strong>我是不会关注的。</p>
<p>给任务设置对应周标签的方式是挑选我认为应该在本周内处理的任务，不考虑该任务的截至日期，不过有本周范围内截至日期的任务已经在上一步筛选完了，所以设置对应周标签的任务在要么没有截至日期，要么是截止日期超过本周的任务范围内。</p>
<p>OmniFocus中，任务有一个预估时长的属性，其实变相的代表了番茄钟的数量，但是查看很不直观。所以我就会在挑选完任务后，给这些任务的标题上注明番茄钟的数量，我一般会用“🍅”表示，有一个🍅就表示该任务需要一个番茄钟，以此类推。这种方式能让我在查看今日任务时能一目了然任务所需的番茄钟数量。</p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h4 id="u6311_u9009_u51FA_u7684_u4EFB_u52A1_u53BB_u54EA_u770B_uFF1F"><a href="#u6311_u9009_u51FA_u7684_u4EFB_u52A1_u53BB_u54EA_u770B_uFF1F" class="headerlink" title="挑选出的任务去哪看？"></a>挑选出的任务去哪看？</h4><p>一周的任务通过上面的方式规划好后，应该怎么查看呢？这时候还是用到透视。我创建了周一至周日七个透视，每天只关注当天的透视，里面筛选出的任务必定就是我规划好应该在这天干的任务。</p>
<h5 id="u5468_u4E00"><a href="#u5468_u4E00" class="headerlink" title="周一"></a>周一</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周二、周三、周四、周五、周六、周日</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周一</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h5 id="u5468_u4E8C"><a href="#u5468_u4E8C" class="headerlink" title="周二"></a>周二</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周一、周三、周四、周五、周六、周日</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周二</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h5 id="u5468_u4E09"><a href="#u5468_u4E09" class="headerlink" title="周三"></a>周三</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周四、周五、周六、周日</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周三</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h5 id="u5468_u56DB"><a href="#u5468_u56DB" class="headerlink" title="周四"></a>周四</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周五、周六、周日</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周四</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h5 id="u5468_u4E94"><a href="#u5468_u4E94" class="headerlink" title="周五"></a>周五</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周六、周日</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周五</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h5 id="u5468_u516D"><a href="#u5468_u516D" class="headerlink" title="周六"></a>周六</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周日</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周六</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h5 id="u5468_u65E5"><a href="#u5468_u65E5" class="headerlink" title="周日"></a>周日</h5><p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>无以下内容：标签为任何以下内容：周一、周二、周三、周四、周五、周六</li>
<li>任何以下内容：<ul>
<li>标签为全部以下内容：周日</li>
<li>状态：即将到期</li>
</ul>
</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：单独操作</li>
<li>项目分组方式：到期</li>
<li>项目排列方式：已标注</li>
</ul>
<h4 id="u661F_u671F_u900F_u89C6_u89E3_u8BFB"><a href="#u661F_u671F_u900F_u89C6_u89E3_u8BFB" class="headerlink" title="星期透视解读"></a>星期透视解读</h4><p>以上这七个透视筛选的任务表示了在一周里，每天需要做的事情。它会根据条件筛选以下两种任务：</p>
<ul>
<li>可用的，并且没有其他星期的标签，并且设置了当天星期的标签的任务。</li>
<li>可用的，并且没有其他星期的标签，并且在今天到期的任务。</li>
</ul>
<p>其实这两个筛选条件就是在前面小节挑选任务时，对任务做的操作。</p>
<p>另外在排序方面，不按项目分组，只安到期日分类，并且排序顺序安是否已标注排序。也就是说，我在处理今天的任务时，不是特别关心这个任务属于哪个项目，而是关注任务在今天的到期时间，并且如果是打了标注的，表名优先级最高，排在最前面。</p>
<h3 id="u5176_u4ED6_u900F_u89C6"><a href="#u5176_u4ED6_u900F_u89C6" class="headerlink" title="其他透视"></a>其他透视</h3><p>上面的透视是组成我任务系统的核心透视，还有几个其他的透视可以辅助我查看一些有共性的任务。</p>
<h4 id="u4ECA_u65E5"><a href="#u4ECA_u65E5" class="headerlink" title="今日"></a>今日</h4><p>这个透视主要是用于OmniFocus的通知小组件和Apple Watch的小组件使用的。该视图主要作用的是提示我今天有多少个即将到期的任务。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>任何以下内容：即将到期</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：项目顺序</li>
</ul>
<h4 id="u4EEA_u5F0F"><a href="#u4EEA_u5F0F" class="headerlink" title="仪式"></a>仪式</h4><p>该视图主要显示哪些周期性重复执行的任务。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为全部以下内容：仪式</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：到期</li>
</ul>
<h4 id="u8D2D_u4E70_u6E05_u5355"><a href="#u8D2D_u4E70_u6E05_u5355" class="headerlink" title="购买清单"></a>购买清单</h4><p>该视图主要显示我的待购物品清单，不考虑什么时候买，但只要有想买的念头就会记下来，在这里显示。</p>
<p>筛选规则：</p>
<ul>
<li>可用性：可用</li>
<li>标签为任何以下内容：采购</li>
</ul>
<p>排列规则：</p>
<ul>
<li>群组和分类：整个项目</li>
<li>项目分组方式：未分组</li>
<li>项目排列方式：到期</li>
</ul>
<h2 id="u756A_u8304_u949F_u7CFB_u7EDF"><a href="#u756A_u8304_u949F_u7CFB_u7EDF" class="headerlink" title="番茄钟系统"></a>番茄钟系统</h2><p>番茄钟这个时间管理工具在这里不做解释。我选择番茄钟应用的原则是有较好的统计功能，能明确的看到一定范围内我的工作时间和番茄数，如果能跨多个平台当然更好，所以我最终选择了Flat Tomato这个番茄钟应用。该应用有iOS版、WatchOS版、MacOS版，在任何环境下我都可以快速的开始一个25分钟的番茄时间，并且它有比较好的统计系统。另外该应用还有一个亮点功能是自带一个轻量级的任务管理系统，也借鉴了一点GTD的概念，并且可以对每一个任务设置番茄钟数量预估，可以快速启动对应的项目的番茄钟，并且番茄时间、休息时间、再番茄时间是可以自动进行的，可以有效提供我们连续专注的效率。</p>
<p>因为OmniFocus没有集成任何番茄系统，而Flat Tomato也没有集成OF，所以我的番茄系统目前属于半外挂形式，也就是当要执行OF中的任务时，标题上标注的🍅数量只是一个参考作用，然后再单独启动Flat Tomato的番茄时间，每个番茄钟完成后还需要手动标注这个番茄钟做了什么事。</p>
<p>那么为什么说是半外挂呢？因为Flat Tomato集成了Todoist，它可以同步Todoist中的项目和任务，从而给每个任务设置番茄钟，所以我将Todoist作为一个任务中转站。将OmniFocus中的项目和任务同步到Todoist中，然后在Flat Tomato里就可以规划番茄数量了。这样每当我要执行OF中的任务时，我就可以直接在Flat Tomato中找到开始番茄钟。目前这种方式我只在做系统的任务分解时候才会用到。</p>
<h3 id="u4EFB_u52A1_u5206_u89E3"><a href="#u4EFB_u52A1_u5206_u89E3" class="headerlink" title="任务分解"></a>任务分解</h3><p>首先我做任务分解时并不在OmniFocus里做，而是在Day One中，其实或者任意iOS中的文本App就可以。其次将任务同步进Todoist中使用了它提供的REST API和<a href="https://support.omnigroup.com/omnifocus-taskpaper-reference/" target="_blank" rel="external">OmniFocus支持TaskPaper规范导入</a>的功能。因为目前OmniFocus 3的Mac版还没有发布，只发布了iOS版。所以我制作了一个<a href="https://workflow.is/workflows/13a29ed7370b4b09923e8bd6ccaa672f" target="_blank" rel="external">Workflow</a>，将我在Day One中规划的任务创建进OmniFocus，同时通过REST API同步进Todoist。</p>
<h2 id="u7EFC_u8FF0"><a href="#u7EFC_u8FF0" class="headerlink" title="综述"></a>综述</h2><p>以上就是我的任务系统的核心点。当然我还依然遵循GTD的方法，比如收集的概念，检查回顾等，这里就不再做解释了。希望大家能和我一起交流任务管理、时间管理的心得。</p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="OmniFocus_u7684_u51E0_u4E2A_u4E3B_u8981_u6982_u5FF5"><a href="#OmniFocus_u7684_u51E0_u4E2A_u4E3B_u8981_u6982_u5FF5" class="headerlink" title="OmniFocus的几个主要概念"></a>OmniFocus的几个主要概念</h2><p>文章中所有的内容都是以OmniFocus 3为例。</p>
<ul>
<li>文件夹：OmniFocus中任务的组织结构最顶级一层，但只是用于分类，本身没有什么属性。</li>
<li>项目：顾名思义，用于定义我们生活和工作中大大小小的项目，项目本身也有属性设置，可设置标签、其内任务的关系（并行、串行、单个），截止日期等。项目下可包含多个任务。</li>
<li>任务：OmniFocus中承载事件的最小颗粒。因为OmniFocus支持任务无限嵌套，所以当多个任务在一个任务下时该任务就成为一个任务组。</li>
<li>任务组：任务组本身也是一个任务，但是它又是其他任务的父任务，所以任务组有子任务关系（并行、串行、单个）的设置。</li>
<li>标签：任务、项目身上都可以设置一到多个标签。标签可以嵌套，并且标签支持设置地理位置，并且有根据进入，走出两个行为和距离范围设置通知。</li>
<li>透视：根据各种条件筛选任务的视图。比如查看有标签“办公室”，即将截至的任务。条件可以按照包含任意、全部包含和不包含进行组合。然后还可以设置筛选出来任务的排列、分组方式。</li>
</ul>
<h2 id="u4EFB_u52A1_u7CFB_u7EDF"><a href="#u4EFB_u52A1_u7CFB_u7EDF" class="headerlink" title="任务系统"></a>任务系统</h2><p>我参照John Z. Sonmez的看板+番茄钟的系统，将其调整为OmniFocus+番茄钟的系统。</p>
<h3 id="u7CFB_u7EDF_u8BBE_u7F6E"><a href="#u7CFB_u7EDF_u8BBE_u7F6E" class="headerlink" title="系统设置"></a>系统设置</h3><ul>
<li>在截止日期这一项，将即将截止日期的表示设置为今天。</li>
<li>今天和Watch这一项，设置为自定义透视的<strong>今日</strong>（该透视下面会讲到）。</li>
</ul>
<h3 id="u6784_u5EFA_u7CFB_u7EDF_u7684_u4E3B_u8981_u6807_u7B7E"><a href="#u6784_u5EFA_u7CFB_u7EDF_u7684_u4E3B_u8981_u6807_u7B7E" class="headerlink" title="构建系统的主要标签"></a>构建系统的主要标签</h3><ul>
<li>仪式：周期性重复的任务会设置该标签。</li>
<li>地点：标明某个任务能在何种场所进行。<ul>
<li>办公室</li>
<li>家</li>
<li>通勤中</li>
</ul>
</li>
<li>状态：进行任务时需要投入的专注度。<ul>
<li>集中精力</li>
<li>放松</li>
</ul>
</li>
<li>跟踪：需要跟踪的任务会设置该标签，比如安排下去的任务需要在某个时间点知道完成的结果。</li>
<li>采购：需要购买东西的任务会设置该标签。</li>
<li>星期：规划一周任务时设置的标签。<ul>
<li>周一</li>
<li>周二</li>
<li>周三</li>
<li>周四</li>
<li>周五</li>
<li>周六</li>
<li>周日</li>
</ul>
</li>
<li>二分钟：能在很短时间内完成的任务会设置该标签。</li>
</ul>]]>
    
    </summary>
    
      <category term="任务管理" scheme="http://www.devtalking.com/tags/%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86/"/>
    
      <category term="生产力" scheme="http://www.devtalking.com/tags/%E7%94%9F%E4%BA%A7%E5%8A%9B/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十一之决策边界]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-11/"/>
    <id>http://www.devtalking.com//articles/machine-learning-11/</id>
    <published>2018-04-29T16:00:00.000Z</published>
    <updated>2018-08-27T02:33:59.081Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u51B3_u7B56_u8FB9_u754C"><a href="#u51B3_u7B56_u8FB9_u754C" class="headerlink" title="决策边界"></a>决策边界</h2><p>决策边界顾名思义就是需要分类的数据中，区分不同类别的边界，举个不恰当的例子，就像省的地界一样，你处在北京还是处在河北，全看你站在区分北京和河北的那条线的哪边。这节我们来看看使用逻辑回归算法如何绘制鸢尾花前两个分类的决策边界。</p>
<h3 id="u7EBF_u6027_u51B3_u7B56_u8FB9_u754C"><a href="#u7EBF_u6027_u51B3_u7B56_u8FB9_u754C" class="headerlink" title="线性决策边界"></a>线性决策边界</h3><p>再来回顾一下逻辑回归，我们需要找到一组$\theta$值，让这组$\theta$和训练数据相乘，然后代入Sigmoid函数，求出某个类别的概率，并且假设，当概率大于等于0.5时，分类为1，当概率小于0.5时，分类为0：</p>
<p>$$\hat p = \sigma(\theta^T X_b)=\frac 1 {1+e^{-\theta^{T}X_b}}$$</p>
<p>$$\hat y =\left\{<br>\begin{aligned}<br>1, \ \ \ \hat p \ge 0.5 \\<br>0, \ \ \ \hat p &lt; 0.5 \\<br>\end{aligned}<br>\right.<br>$$</p>
<p>在Sigmoid函数那节解释过，当$t&gt;0$时，$\hat p&gt;0.5$。当$t&lt;0$时，$\hat p&lt;0.5$，因为$t= \theta^T X_b $，所以：</p>
<p>$$\hat y =\left\{<br>\begin{aligned}<br>1, \ \ \ \ \hat p \ge 0.5, \ \ \ \ \theta^T X_b \ge 0 \\<br>0, \ \ \ \ \hat p &lt; 0.5, \ \ \ \ \theta^T X_b &lt; 0 \\<br>\end{aligned}<br>\right.<br>$$</p>
<p>那么当$\theta^T X_b =0$时，理论上$\hat p$就是0.5，分类既可以为0，也可以为1。只不过我们在这里将$\hat p=0.5$是，分类假设为1。由此可见$\theta^T X_b =0$就是逻辑回归中的决策边界，并且是线性决策边界。</p>
<a id="more"></a>
<p>下面来解释一下为何说是线性决策边界。我们以前两个分类的鸢尾花为例，将$\theta^T X_b =0$展开得：</p>
<p>$$\theta_0 + \theta_1 X_1 + \theta_2 X_2=0$$</p>
<p>$\theta_0$就是截距，$\theta_1$和$\theta_2$是系数，这个公式绘制出来的是一条直线，这条直线就是能将鸢尾花数据的前两个分类区分开的直线，既线性决策边界。为了能方便的将这条直线绘制出来，我们对上面的公式做一下转换：</p>
<p>$$X_2 = \frac {-\theta_0 - \theta_1 X_1} {\theta_2}$$</p>
<p>下面我们在Jupyter Notebook中绘制出来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 还是使用鸢尾花的前两个类型的前两个特征</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X = X[y&lt;<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line">y = y[y&lt;<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> myML.LogisticRegression <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line">log_reg.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 系数，既theta1和theta2</span></span><br><span class="line">log_reg.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">3.01749692</span>, -<span class="number">5.03046934</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 截距</span></span><br><span class="line">log_reg.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">-<span class="number">0.68273836989931069</span></span><br></pre></td></tr></table></figure>
<p>上面的代码中可以看到，$\theta_0$，$\theta_1$和$\theta_2$都已经知道了。接下来要做的就是给定一组$X_1$然后通过上面的公式求出$X_2$，最后绘制出线性决策边界直线：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义求X2的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">X2</span><span class="params">(X1)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> (-log_reg.intercept_ - log_reg.coef_[<span class="number">0</span>] * X1) / log_reg.coef_[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建X1</span></span><br><span class="line">X1 = np.linspace(<span class="number">4</span>, <span class="number">8</span>, <span class="number">1000</span>)</span><br><span class="line">X2 = X2(X1)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'b'</span>)</span><br><span class="line">plt.plot(X1, X2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c99b013a642d5a94de6a3ddf1ef15f2b.jpg" alt=""></p>
<h3 id="u4E0D_u89C4_u5219_u51B3_u7B56_u8FB9_u754C"><a href="#u4E0D_u89C4_u5219_u51B3_u7B56_u8FB9_u754C" class="headerlink" title="不规则决策边界"></a>不规则决策边界</h3><p>目前我们实现的逻辑回归是使用线性回归来实现的，同样可以通过添加多项式项使决策边界不再是直线。同样，还有像KNN算法在多分类问题中决策边界必然都不是直线，而是不规则的决策边界，所以自然也无法通过一个线性方程来绘制。那么这一小节来看看如何绘制不规则决策边界。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d21db36f419130b7568c5dfb78af4713.jpg" alt=""></p>
<p>从上面的图中可以看出，红蓝点的区分界限并不是一条直线，而是一个不规则的形状，这就是不规则决策边界。那么绘制不规则决策边界的方法其实也很简单，就是将特征平面上的每一个点都用我们训练出的模型判断它属于哪一类，然后将判断出的分类颜色绘制出来，就得到了上图所示的效果，那么不规则决策边界自然也就出来了，这个原理类似绘制地形图的等高线，在同一等高范围内的点就是同一类。</p>
<blockquote>
<p>等高线指的是地形图上高程相等的各点所连成的闭合曲线。</p>
</blockquote>
<p>既然运用了等高线的原理，那么我们的绘制方法思路就很明了了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">	<span class="comment"># meshgrid函数用两个坐标轴上的点在平面上画格，返回坐标矩阵</span></span><br><span class="line">	X0, X1 = np.meshgrid(</span><br><span class="line">		<span class="comment"># 随机两组数，起始值和密度由坐标轴的起始值决定</span></span><br><span class="line">		np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line">	<span class="comment"># ravel()方法将高维数组降为一维数组，c_[]将两个数组以列的形式拼接起来，形成矩阵</span></span><br><span class="line">	X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 通过训练好的逻辑回归模型，预测平面上这些点的分类</span></span><br><span class="line">	y_predict = model.predict(X_grid_matrix)</span><br><span class="line">	y_predict_matrix = y_predict.reshape(X0.shape)</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 设置色彩表</span></span><br><span class="line">	<span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">	my_colormap = ListedColormap([<span class="string">'#0000CD'</span>, <span class="string">'#40E0D0'</span>, <span class="string">'#FFFF00'</span>])</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 绘制等高线，并且填充等高区域的颜色</span></span><br><span class="line">	plt.contourf(X0, X1, y_predict_matrix, linewidth=<span class="number">5</span>, cmap=my_colormap)</span><br></pre></td></tr></table></figure>
<p>我对这个方法中的几个函数做一下解释：</p>
<ul>
<li><code>np.meshgrid()</code>这个函数的作用是用给定坐标轴上的点在平面上画格，返回组成网格点的坐标矩阵。</li>
<li><code>ravel()</code>方法将高维数组降为一维数组。</li>
<li><code>c_[]</code>将两个数组以列的形式拼接起来，形成矩阵。</li>
</ul>
<p>我用一幅图对上面的方法做以形象的说明：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/bf12018ff3808ef78bcfa8b64b7fa520.jpg" alt=""></p>
<p>假设传给<code>np.meshgrid()</code>方法的两个坐标轴上共计六个点，然后返回由这六个点组成的网格的坐标矩阵，既网格相交点的坐标矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">		np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">		np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">x0</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.</span> ,  <span class="number">1.5</span>,  <span class="number">3.</span> ],</span><br><span class="line">	   [ <span class="number">0.</span> ,  <span class="number">1.5</span>,  <span class="number">3.</span> ],</span><br><span class="line">	   [ <span class="number">0.</span> ,  <span class="number">1.5</span>,  <span class="number">3.</span> ]])</span><br><span class="line"></span><br><span class="line">x1</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.</span> ,  <span class="number">0.</span> ,  <span class="number">0.</span> ],</span><br><span class="line">	   [ <span class="number">1.5</span>,  <span class="number">1.5</span>,  <span class="number">1.5</span>],</span><br><span class="line">	   [ <span class="number">3.</span> ,  <span class="number">3.</span> ,  <span class="number">3.</span> ]])</span><br></pre></td></tr></table></figure>
<p>因为返回的结果将这九个点的坐标分开了，所以通过<code>np.c_[X0.ravel(), X1.ravel()]</code>将这九个点的坐标合起来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.</span> ,  <span class="number">0.</span> ],</span><br><span class="line">	   [ <span class="number">1.5</span>,  <span class="number">0.</span> ],</span><br><span class="line">	   [ <span class="number">3.</span> ,  <span class="number">0.</span> ],</span><br><span class="line">	   [ <span class="number">0.</span> ,  <span class="number">1.5</span>],</span><br><span class="line">	   [ <span class="number">1.5</span>,  <span class="number">1.5</span>],</span><br><span class="line">	   [ <span class="number">3.</span> ,  <span class="number">1.5</span>],</span><br><span class="line">	   [ <span class="number">0.</span> ,  <span class="number">3.</span> ],</span><br><span class="line">	   [ <span class="number">1.5</span>,  <span class="number">3.</span> ],</span><br><span class="line">	   [ <span class="number">3.</span> ,  <span class="number">3.</span> ]])</span><br></pre></td></tr></table></figure>
<p>然后通过训练好的逻辑回归模型对这九个点预测它们的分类，将预测出的分类作为等高区间。最后通过<code>ListedColormap</code>定义我们自己的色彩表，再使用Matplotlib的<code>contourf</code>函数将等高区域绘制出来，也就是将分类用颜色区分出来。<code>contourf</code>函数的前两个参数是确定点的坐标矩阵，第三个参数是高度，第四个参数是等高线的粗细度，第五个参数是色彩表。</p>
<p>下面我们来使用一下<code>plot_decision_boundary</code>方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_decision_boundary(log_reg, axis=[<span class="number">4</span>, <span class="number">7.5</span>, <span class="number">1.5</span>, <span class="number">4.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'b'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/5624ad5284cad5b733941979c035cb62.jpg" alt=""></p>
<h3 id="kNN_u7684_u51B3_u7B56_u8FB9_u754C"><a href="#kNN_u7684_u51B3_u7B56_u8FB9_u754C" class="headerlink" title="kNN的决策边界"></a>kNN的决策边界</h3><p>因为kNN算法在解决二分类问题时是无法像逻辑回归算法那样推导出线性决策边界的公式的，所以我们使用绘制不规则决策边界的方式来看一下kNN算法的决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(knn_clf, axis=[<span class="number">4</span>, <span class="number">7.5</span>, <span class="number">1.5</span>, <span class="number">4.5</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'b'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/0bc50c52d714eac55d0c3df8f7f334f4.jpg" alt=""></p>
<p>下面再来看看当kNN在解决多分类问题时的决策边界是怎样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf_all = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 鸢尾花还是取前两个特征，但是使用全部的三个分类</span></span><br><span class="line">knn_clf_all.fit(iris.data[:, :<span class="number">2</span>], iris.target)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(knn_clf_all, axis=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">1.5</span>, <span class="number">4.5</span>])</span><br><span class="line">plt.scatter(iris.data[iris.target==<span class="number">0</span>,<span class="number">0</span>], iris.data[iris.target==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(iris.data[iris.target==<span class="number">1</span>,<span class="number">0</span>], iris.data[iris.target==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(iris.data[iris.target==<span class="number">2</span>,<span class="number">0</span>], iris.data[iris.target==<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/1753cc6e40fdb3734a7c8a41ff585a47.jpg" alt=""></p>
<p>从上面的三分类不规则决策边界图中可以看到，在绿色区域里还有些黄色区域，这表示我们的kNN模型有过拟合的现象，也就是k值过小导致的。在第三篇笔记中讲kNN算法时讲过，k值越小，kNN的模型就越复杂。所以我们手动将k值调整为50，再看一下决策边界的情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf_all = KNeighborsClassifier(n_neighbors=<span class="number">50</span>)</span><br><span class="line">knn_clf_all.fit(iris.data[:,:<span class="number">2</span>], iris.target)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(knn_clf_all, axis=[<span class="number">4</span>, <span class="number">8</span>, <span class="number">1.5</span>, <span class="number">4.5</span>])</span><br><span class="line">plt.scatter(iris.data[iris.target==<span class="number">0</span>,<span class="number">0</span>], iris.data[iris.target==<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(iris.data[iris.target==<span class="number">1</span>,<span class="number">0</span>], iris.data[iris.target==<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">plt.scatter(iris.data[iris.target==<span class="number">2</span>,<span class="number">0</span>], iris.data[iris.target==<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b639aa52f1f5e00348e6a00020c87082.jpg" alt=""></p>
<p>现在可以看到分类区域界限是相对比较规整清晰了。</p>
<h2 id="u903B_u8F91_u56DE_u5F52_u4E2D_u4F7F_u7528_u591A_u9879_u5F0F_u7279_u5F81"><a href="#u903B_u8F91_u56DE_u5F52_u4E2D_u4F7F_u7528_u591A_u9879_u5F0F_u7279_u5F81" class="headerlink" title="逻辑回归中使用多项式特征"></a>逻辑回归中使用多项式特征</h2><p>在讲逻辑回归中使用多项式特征前，先来举个例子看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建随机的均值为0，标准差为1的矩阵X</span></span><br><span class="line">X = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">200</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 构造一个生成y的函数，让其值判断是大于1.5还是小于1.5，既将y值分类</span></span><br><span class="line">y = np.array(X[:, <span class="number">0</span>]**<span class="number">2</span> + X[:, <span class="number">1</span>]**<span class="number">2</span> &lt; <span class="number">1.5</span>, dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘样本数据</span></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/5420707445a99c5cb7520cb19361ce36.jpg" alt=""></p>
<p>从上图可以看到，我们构建的样本数据，明显无法用一条直线将两个不同颜色的点区分开，我们使用上一节的方法来验证一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入我们实现的逻辑回归方法训练模型</span></span><br><span class="line"><span class="keyword">from</span> myML.LogisticRegression <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line">log_reg.score(X, y)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.42499999999999999</span></span><br></pre></td></tr></table></figure>
<p>可以看到训练出的模型预测分数非常低。再来看看决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_decision_boundary(log_reg, axis=[-<span class="number">4</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b38865f865d8d7d391732d6537ec1d47.jpg" alt=""></p>
<p>从图中可以看到，绘制出的线性决策边界是完全没办法区分样本数据中的两种类型的。</p>
<p>并且我们也清楚的知道，样本数据的决策边界应该是下图所示：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/14c901391bfcd5c647d83a90de0450df.jpg" alt=""></p>
<p>那么我们如何能得到一个圆形的决策边界呢？大家回忆一下，在几何中我们学过圆的标准方程应该是：</p>
<p>$$(x-a)^2 + (y-b)^2 = r^2$$</p>
<p>$a$和$b$是圆心坐标，$r$是半径。那如果我们将上图的圆看作是一个圆心在$(0, 0)$的圆，那么这个圆形的决策边界公式应该就是：</p>
<p>$$x^2 + y^2 -r^2 = 0$$</p>
<p>和逻辑回归的线性决策边界公式做一下对比：</p>
<p>$$\theta_1 X_1 + \theta_2 X_2 + \theta_0 = 0$$</p>
<p>是不是发现相当于给线性决策边界的特征增加幂次，再回想之前笔记中讲过的多项式回归，此时大家应该心中就了然了。那就是如果要让逻辑回归处理不规则决策边界分类问题，那么就运用多项式回归的原理，下面我们实现来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用到了前面笔记中讲过的Pipeline</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomialLogisticRegression</span><span class="params">(degree)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">"std_scalar"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"log_reg"</span>, LogisticRegression())</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">ploy_log_reg = PolynomialLogisticRegression(degree=<span class="number">2</span>)</span><br><span class="line">ploy_log_reg.fit(X, y)</span><br><span class="line">ploy_log_reg.score(X, y)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.97999999999999998</span></span><br></pre></td></tr></table></figure>
<p>可以看到使用多项式回归原理后，我们训练出的新的模型对样本数据的预测评分达到了98%。再来绘制一下决策边界看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_decision_boundary(ploy_log_reg, axis=[-<span class="number">4</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/80d9d6cdeb044c5d245b57d7d8c73957.jpg" alt=""></p>
<p>现在圆形的决策边界就被绘制出来了，并且将样本数据的类型区分的很准确。</p>
<h2 id="u903B_u8F91_u56DE_u5F52_u4E2D_u4F7F_u7528_u6A21_u578B_u6B63_u5219_u5316"><a href="#u903B_u8F91_u56DE_u5F52_u4E2D_u4F7F_u7528_u6A21_u578B_u6B63_u5219_u5316" class="headerlink" title="逻辑回归中使用模型正则化"></a>逻辑回归中使用模型正则化</h2><p>上一节中，我们讲了使用多项式的方式使得逻辑回归可以解决非线性分类的问题，那么既然使用了多项式方法，那势必模型就会变的很复杂，继而产生过拟合的问题。所以和多项式解决回归问题一样，在逻辑回归中使用多项式也要使用模型正则化来避免过拟合的问题。</p>
<p>这一节我们使用Scikit Learn中提供的逻辑回归来看一下如何使用模型正则化。在这之前先来复习一下模型正则化。所谓模型正则化，就是在损失函数中加一个带有系数的正则模型，那么此时如果想让损失函数尽可能的小，就要兼顾原始损失函数和正则模型中的$\theta$值，从而做以权衡，起到约束多项式系数大小的作用。正则模型前的系数$\alpha$ 决定了新的损失函数中每一个$\theta$都尽可能的小，这个小的程度占整个优化损失函数的多少。</p>
<p>$$L(\theta)_{new} = L(\theta) + \alpha L_p$$</p>
<blockquote>
<p>$L_p$范数请参见<a href="http://www.devtalking.com/articles/machine-learning-9/">机器学习笔记九之交叉验证、模型正则化 </a>。</p>
</blockquote>
<p>但是这种方式有一个问题，那就是可以刻意回避模型正则化，也就是将$\alpha$取值为0的时候。所以还有另一种模型正则化的方式是将这个系数加在原始损失函数前面，这种情况的话相当于正则模型前的系数永远是1，无论如何都要进行模型正则化。</p>
<p>$$L(\theta)_{new} = CL(\theta) + L_p$$</p>
<p>Scikit Learn中的逻辑回归就自带这种形式的模型正则化，下面我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据，构建200行2列的矩阵，均值为0，标准差为1，既200个样本，每个样本2个特征</span></span><br><span class="line">X = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">200</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 构建y的方程，曲线为抛物线</span></span><br><span class="line">y = np.array(X[:, <span class="number">0</span>]**<span class="number">2</span> + X[:, <span class="number">1</span>] &lt;<span class="number">1.5</span>, dtype=<span class="string">'int'</span>)</span><br><span class="line"><span class="comment"># 在样本数据中加一些噪音</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">	y[np.random.randint(<span class="number">200</span>)] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘样本数据</span></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ea97f4a72fc0eb2e31cac5e105166f9c.jpg" alt=""></p>
<p>样本数据构建好了，我们先用逻辑回归对其进行分类预测看看准确度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">LogisticRegression(C=<span class="number">1.0</span>, class_weight=<span class="keyword">None</span>, dual=<span class="keyword">False</span>, fit_intercept=<span class="keyword">True</span>,</span><br><span class="line">		  intercept_scaling=<span class="number">1</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">'ovr'</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">		  penalty=<span class="string">'l2'</span>, random_state=<span class="keyword">None</span>, solver=<span class="string">'liblinear'</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">		  verbose=<span class="number">0</span>, warm_start=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>当模型训练完后，我们在返回内容中可以看到两个超参数<code>C</code>和<code>penalty</code>，前者就是前面讲到的原始损失函数前的系数，后者就是正则模型，逻辑回归中这两个超参数的默认值是1和$L_2$范式正则模型，也就是LASSO正则模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80000000000000004</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制决策边界</span></span><br><span class="line">plot_decision_boundary(log_reg, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/2802fa23be191a0afa093f0a86cbddd7.jpg" alt=""></p>
<p>可以看到用线性逻辑回归训练出的模型准确度只有80%，并且线性决策边界无法很好的区分两种分类。下面我们再用多项式逻辑回归训练模型看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomialLogisiticRegression</span><span class="params">(degree, C)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">'poly'</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">'std_scaler'</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">'log_reg'</span>, LogisticRegression(C=C))</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">ploy_log_reg = PolynomialLogisiticRegression(degree=<span class="number">2</span>, C=<span class="number">1</span>)</span><br><span class="line">ploy_log_reg.fit(X_train, y_train)</span><br><span class="line">ploy_log_reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.93999999999999995</span></span><br><span class="line"></span><br><span class="line">plot_decision_boundary(ploy_log_reg, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/37ef4687f99025680ef6f919045dbf07.jpg" alt=""></p>
<p>可以看到当使用多项式逻辑回归后，模型准确度达到了94%，不规则决策边界也很好的区分了两种类型。下面我们增加多项式的复杂度，再来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ploy_log_reg2 = PolynomialLogisiticRegression(degree=<span class="number">20</span>, C=<span class="number">1</span>)</span><br><span class="line">ploy_log_reg2.fit(X_train, y_train)</span><br><span class="line">ploy_log_reg2.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
<p>可以看到当<code>degree</code>增大到20时，模型准确率有所下降，因为我们的样本数据量比较小，所以过拟合的现象不是很明显，我们绘制出决策边界看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_decision_boundary(ploy_log_reg2, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/520afe6eaba281739ca38058e4d86772.jpg" alt=""></p>
<p>从决策边界上能很明显的看到过拟合的状态。下面我们来调整<code>C</code>这个系数，让正则模型来干预整个损失函数中$\theta$的大小，然后再看看模型准确率和决策边界：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ploy_log_reg3 = PolynomialLogisiticRegression(degree=<span class="number">20</span>, C=<span class="number">0.1</span>)</span><br><span class="line">ploy_log_reg3.fit(X_train, y_train)</span><br><span class="line">ploy_log_reg3.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.92000000000000004</span></span><br><span class="line"></span><br><span class="line">plot_decision_boundary(ploy_log_reg3, axis=[-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8b403e17ab26fe11a1dbcfd561007632.jpg" alt=""></p>
<p>可以看到当正则模型进行干预后，模型的准确率有所提升，决策边界也比之前好了许多。这就是逻辑回归中的模型正则化。</p>
<h2 id="u903B_u8F91_u56DE_u5F52_u89E3_u51B3_u591A_u5206_u7C7B_u95EE_u9898"><a href="#u903B_u8F91_u56DE_u5F52_u89E3_u51B3_u591A_u5206_u7C7B_u95EE_u9898" class="headerlink" title="逻辑回归解决多分类问题"></a>逻辑回归解决多分类问题</h2><p>在前面的章节中，对逻辑回归的应用一直是在二分类问题中进行的。这一节来讲讲能够让逻辑回归解决多分类问题的方法。</p>
<h3 id="OvR"><a href="#OvR" class="headerlink" title="OvR"></a>OvR</h3><p>所谓OvR就是One vs Rest的缩写，从字面上来讲是一对剩余的所有的意思。那么我们通过一系列示图来解释一下OvR：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d2805fa3251e0b23ced05d648514576d.jpg" alt=""></p>
<p>假设有四个分类，如上图所示，如果One指的是蓝色的点，那么剩余的红色、绿色、黄色三个点就是Rest，也就是我们选取一个类别，把其他剩余的类别称之为剩余类别：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9878951b26aa662274f6dcee71126cea.jpg" alt=""></p>
<p>这样就把一个四分类问题转换成了二分类问题，现在我们就可以使用逻辑回归算法预测再来一个点时它属于蓝色点的概率和属于剩余点的概率。</p>
<p>同理，这个过程也可以在其他颜色的点上进行，如果是上图的四分类问题，那么就可以拆分为四个二分类问题：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b3e4dacbf0262a1b55308bcbaf1a944e.jpg" alt=""></p>
<p>然后对新来的点分别在这四个二分类问题中计算概率，也就是N个类别就进行N次分类，最后选择分类概率最高的那个二分类，这样就可以判断这个新点的类别了。</p>
<h3 id="OvO"><a href="#OvO" class="headerlink" title="OvO"></a>OvO</h3><p>所谓OvO就是One vs One的缩写，从字面上来讲是一对一的意思。那么我们同样通过一系列示图来解释一下OvO：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/4651eb1bf5a06c4fa28800c674be0cf6.jpg" alt=""></p>
<p>还是假设有四个分类，将其中的每两个分类单独拿出来处理，这样就一共有六组二分类问题，然后对新来的点分别在这六个二分类问题中计算概率，最后选择分类概率最高的那个二分类，这样就可以判断这个新点的类别了，这就是OvO方式。很明显OvO方式的时间复杂度要比OvR高很多，但是准确率也高很多，因为每次都是在绝对的二分类中对新来的样本数据进行概率计算</p>
<h3 id="Scikit_Learn_u4E2D_u7684_u903B_u8F91_u56DE_u5F52"><a href="#Scikit_Learn_u4E2D_u7684_u903B_u8F91_u56DE_u5F52" class="headerlink" title="Scikit Learn中的逻辑回归"></a>Scikit Learn中的逻辑回归</h3><p>这一节我们来看看Scikit Learn中封装的逻辑回归。我们使用鸢尾花的样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">LogisticRegression(C=<span class="number">1.0</span>, class_weight=<span class="keyword">None</span>, dual=<span class="keyword">False</span>, fit_intercept=<span class="keyword">True</span>,</span><br><span class="line">		  intercept_scaling=<span class="number">1</span>, max_iter=<span class="number">100</span>, multi_class=<span class="string">'ovr'</span>, n_jobs=<span class="number">1</span>,</span><br><span class="line">		  penalty=<span class="string">'l2'</span>, random_state=<span class="keyword">None</span>, solver=<span class="string">'liblinear'</span>, tol=<span class="number">0.0001</span>,</span><br><span class="line">		  verbose=<span class="number">0</span>, warm_start=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">log_reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.94736842105263153</span></span><br></pre></td></tr></table></figure>
<p>从打印结果中我们能看到有一个属性叫<code>multi_class</code>它的值为<code>ovr</code>，其实Scikit Learn中的逻辑回归是自带OvR和OvO能力的，默认使用OvR。另外还需要注意<code>solver</code>属性，这个属性指定了逻辑回归使用的算法，如果使用了<code>ovr</code>，则对应的算法是<code>liblinear</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_reg1 = LogisticRegression(multi_class=<span class="string">'multinomial'</span>, solver=<span class="string">'newton-cg'</span>)</span><br><span class="line">log_reg1.fit(X_train, y_train)</span><br><span class="line">log_reg1.score(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.9732142857142857</span></span><br></pre></td></tr></table></figure>
<p>如果要使用OvO方式，需要显示的传入<code>multi_class</code>和<code>solver</code>这两个参数，对应OvO的算法是<code>newton_cg</code>，这里就不对这两个算法做详细解释了，可以看看Scikit Learn的官网说明。</p>
<h3 id="OvO_u548COvR_u7C7B"><a href="#OvO_u548COvR_u7C7B" class="headerlink" title="OvO和OvR类"></a>OvO和OvR类</h3><p>除了在逻辑回归中自带了OvR和OvO方式以外，Scikit Learn还专门提供了单独的OvR和OvO的类，只要传入一个二分类器，就可以运用OvR和OvO的原理来解决多分类问题了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line">ovr = OneVsRestClassifier(log_reg)</span><br><span class="line">ovr.fit(X_train, y_train)</span><br><span class="line">ovr.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.94736842105263153</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line"></span><br><span class="line">ovo = OneVsOneClassifier(log_reg)</span><br><span class="line">ovo.fit(X_train, y_train)</span><br><span class="line">ovo.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>这两篇笔记主要讲了逻辑回归，这是解决分类问题应用很广泛的一个算法，它拓展了线性回归算法，将估算概率的方式将回归问题转换为了分类问题。下一篇笔记将讨论如何更好的评价分类问题的准确度。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u51B3_u7B56_u8FB9_u754C"><a href="#u51B3_u7B56_u8FB9_u754C" class="headerlink" title="决策边界"></a>决策边界</h2><p>决策边界顾名思义就是需要分类的数据中，区分不同类别的边界，举个不恰当的例子，就像省的地界一样，你处在北京还是处在河北，全看你站在区分北京和河北的那条线的哪边。这节我们来看看使用逻辑回归算法如何绘制鸢尾花前两个分类的决策边界。</p>
<h3 id="u7EBF_u6027_u51B3_u7B56_u8FB9_u754C"><a href="#u7EBF_u6027_u51B3_u7B56_u8FB9_u754C" class="headerlink" title="线性决策边界"></a>线性决策边界</h3><p>再来回顾一下逻辑回归，我们需要找到一组$\theta$值，让这组$\theta$和训练数据相乘，然后代入Sigmoid函数，求出某个类别的概率，并且假设，当概率大于等于0.5时，分类为1，当概率小于0.5时，分类为0：</p>
<p>$$\hat p = \sigma(\theta^T X_b)=\frac 1 {1+e^{-\theta^{T}X_b}}$$</p>
<p>$$\hat y =\left\{<br>\begin{aligned}<br>1, \ \ \ \hat p \ge 0.5 \\<br>0, \ \ \ \hat p &lt; 0.5 \\<br>\end{aligned}<br>\right.<br>$$</p>
<p>在Sigmoid函数那节解释过，当$t&gt;0$时，$\hat p&gt;0.5$。当$t&lt;0$时，$\hat p&lt;0.5$，因为$t= \theta^T X_b $，所以：</p>
<p>$$\hat y =\left\{<br>\begin{aligned}<br>1, \ \ \ \ \hat p \ge 0.5, \ \ \ \ \theta^T X_b \ge 0 \\<br>0, \ \ \ \ \hat p &lt; 0.5, \ \ \ \ \theta^T X_b &lt; 0 \\<br>\end{aligned}<br>\right.<br>$$</p>
<p>那么当$\theta^T X_b =0$时，理论上$\hat p$就是0.5，分类既可以为0，也可以为1。只不过我们在这里将$\hat p=0.5$是，分类假设为1。由此可见$\theta^T X_b =0$就是逻辑回归中的决策边界，并且是线性决策边界。</p>]]>
    
    </summary>
    
      <category term="OvO" scheme="http://www.devtalking.com/tags/OvO/"/>
    
      <category term="OvR" scheme="http://www.devtalking.com/tags/OvR/"/>
    
      <category term="决策边界" scheme="http://www.devtalking.com/tags/%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记十之逻辑回归]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-10/"/>
    <id>http://www.devtalking.com//articles/machine-learning-10/</id>
    <published>2018-04-14T16:00:00.000Z</published>
    <updated>2018-08-27T02:33:25.810Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在这一篇笔记中我们来学习目前应用比较广泛的一个分类算法逻辑回归（Logistic Regression）。</p>
<h2 id="u4EC0_u4E48_u662F_u903B_u8F91_u56DE_u5F52"><a href="#u4EC0_u4E48_u662F_u903B_u8F91_u56DE_u5F52" class="headerlink" title="什么是逻辑回归"></a>什么是逻辑回归</h2><p>首先大家应该有会有一个疑问，为什么既然叫逻辑回归，但是解决的却是分类问题呢。因为逻辑回归是将样本的特征和样本发生的概率联系起来，在拟合样本数据发生概率的时候，其实是在解决一个回归问题，当概率计算出来后，再根据概率进行分类处理。所以逻辑回归在解决分类问题时，其实中间还是进行了回归问题的处理，但是逻辑回归只能解决二分类问题，这一点要注意。</p>
<p>逻辑回归在解决概率这个回归问题时，和线性回归、多项式回归有一个不同的地方，那就是后者训练出的模型，预测其他样本数据的目标值时值域理论上是在负无穷到正无穷的，换句话说也就是对值域是没有什么限制的。而对于表示概率的数而言，一般值域都是在0到1之间，真实世界中，概率不都是按从0%到100%表示嘛。所以这就引出了逻辑回归和线性回归的相似和不同处，继而体现在公式模型上。在第四篇笔记中，我们知道多元线性回归的公式模型是：<br>$$ \hat y = \theta ^T X_b  $$</p>
<p>因为$\hat y$的值域为$(-\infty,+\infty)$，所以如果要在逻辑回顾中运用在求解概率的公式模型上就需要另外一个函数将其值域限定在$(0, 1)$，我们将这个函数称为<strong>Sigmoid</strong>函数：</p>
<p>$$\hat p = \sigma(\theta ^T X_b)$$</p>
<p>$$\sigma(t) = \frac 1 {1+e^{-t}}$$</p>
<p>最终逻辑回归求解概率的模型为：</p>
<p>$$\hat p = \frac 1 {1+e^{-\theta ^T X_b}}$$</p>
<a id="more"></a>
<h3 id="Sigmoid_u51FD_u6570"><a href="#Sigmoid_u51FD_u6570" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p>我们先来看看这个Sigmoid函数为什么将值域限定在$(0, 1)$，将这个函数绘制出来看一下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(t)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-t))</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 构建向量x，随机生成500个数，范围从-10到10</span></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">500</span>)</span><br><span class="line"><span class="comment"># y值通过Sigmoid计算出</span></span><br><span class="line">y = sigmoid(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/7667662ed99feb578abdec79a31c667c.jpg" alt=""></p>
<p>从曲线图可以看出，$y$的值最小不会小于0，最大不会大于1，表明了它的值域是$(0, 1)$，我将Sigmoid函数再写出来看一下：</p>
<p>$$\sigma(t) = \frac 1 {1+e^{-t}}$$</p>
<p>当$t$无穷大时，$e^{-t}$无穷小，那么当1除以无穷小时，它的值就无限接近于1。那么当$t$无穷小时，$e^{-t}$无穷大，那么当1除以无穷大时，它的值就无限接近于0。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/4292f2003870497bcae2c87e9f6e4485.jpg" alt=""></p>
<p>我将坐标轴在Sigmoid曲线图上绘制出来，可以看到当$t &gt; 0$时，越接近0，概率$p$越接近0.5，$t$越远离0时，概率$p$越大于0.5，并且当$t$大到一定程度时，概率$p$基本稳定在1附近。当$t &lt; 0$时，越接近0，概率$p$越接近0.5。$t$越远离0时，概率$p$越小于0.5，并且当$t$小到一定程度时，概率$p$基本稳定在0附近。这就是Sigmoid函数在逻辑回归中的数学意义。</p>
<h2 id="u903B_u8F91_u56DE_u5F52_u7684_u635F_u5931_u51FD_u6570"><a href="#u903B_u8F91_u56DE_u5F52_u7684_u635F_u5931_u51FD_u6570" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h2><p>我们再将逻辑回归中计算概率的公式写出来：</p>
<p>$$\hat p = \frac 1 {1+e^{-\theta ^T X_b}}$$</p>
<p>现在的问题是我们如何求出上面的$\theta$值，也就是在逻辑回归问题中，损失函数如何定义。</p>
<p>前面说过，逻辑回归只能解决二分类问题，既当概率估计值$\hat p$大于等于0.5时，我们记$\hat y$为1，当$\hat p$小于等于0.5时，记$\hat y$为0，既：</p>
<p>$$\hat y =\left\{<br>\begin{aligned}<br>1, \ \ \ \hat p \ge 0.5 \\<br>0, \  \ \ \hat p \leq 0.5 \\<br>\end{aligned}<br>\right.<br>$$</p>
<p>那么我们先将损失函数也分为两种情况来看：</p>
<p>$$cost = \left\{<br>\begin{aligned}<br>如果y=1，p越小，cost越大（损失越大） \\<br>如果y=0，p越大，cost越大（损失越大）\\<br>\end{aligned}<br>\right.<br>$$</p>
<p>也就是说，在真实的分类为1的情况下，如果概率越小，那么说明损失越大，因为概率越小，分类应该越趋向0。在真实的分类为0的情况下，如果概率越大，那么也说明损失越大，因为概率越大，分类应该越趋向1。</p>
<p>上面我们只是根据逻辑回归分类的方式，定义出了逻辑回归损失函数的损失趋势和语言解释，那么我们需要在数学的海洋中找到符合这种趋势的数学工具，或者说函数。再次之前，我们先来回顾一下高中学过的数学知识<strong>对数</strong>。</p>
<h3 id="u5BF9_u6570"><a href="#u5BF9_u6570" class="headerlink" title="对数"></a>对数</h3><p>在数学中，如果有$x=\alpha ^y$，$y$是$\alpha$的指数，那么可以说以$\alpha$为底$x$的对数是$y$，记为：</p>
<p>$$y=log_{\alpha}x$$</p>
<p>对数有一些简便的写法：</p>
<ul>
<li>如果底数为2，可以省略底数记为$log \ x$</li>
<li>如果底数为10，可以省略底数记为$lg \ x$</li>
<li>如果底数为$e$，可以省略底数记为$ln \ x$</li>
</ul>
<p>对数的主要作用是解幂比如：</p>
<ul>
<li>$log_{\theta} \ xy = log_{\theta} \ x + log_{\theta} \ y$</li>
<li>$log_{\theta} \ \frac x y = log_{\theta} \ x - log_{\theta} \ y$</li>
<li>$log_{\theta} \ x^y = y log_{\theta} \ x$</li>
<li>$log_{\theta} \ \sqrt[y]{x} = \frac {log_{\theta} \ x} y$</li>
</ul>
<p>对数的存在使得求这种复杂幂的问题更快更方便。</p>
<p>我们再来看看对数在几何意义上的曲线：<br><img src="http://paxigrdp0.bkt.clouddn.com/f427c68e356b03469b66f870e0e9f00d.jpg" alt="" title="对数曲线"><br>可以很容易的算出，对数的曲线始终是过$(1, 0)$点的。</p>
<h3 id="u5B9A_u4E49_u635F_u5931_u51FD_u6570"><a href="#u5B9A_u4E49_u635F_u5931_u51FD_u6570" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>回顾完对数后，我们再来看看逻辑回归的损失函数，没错，我们就是要使用对数的函数来表示：</p>
<p>$$cost = \left\{<br>\begin{aligned}<br>{如果y=1,那么损失函数为：-log(\hat p) \\<br>如果y=0,那么损失函数为：-log(1-\hat p)}<br>\end{aligned}<br>\right.<br>$$</p>
<p>我们来看看这两个函数为什么符合之前我们定义的损失函数的描述。首先$-log x$的曲线为：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/cffe527aa375d697fbbcc500da80f28a.jpg" alt=""></p>
<p>之前说了，概率的值域在$(0,1)$之间，所以上图曲线的x轴以下的曲线是没有意义的，所以对于$-log (\hat p)$，它的曲线是：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a8e4252ec69d0c49709eaf37e6b6fd00.jpg" alt=""></p>
<p>从上面的曲线图可以很容易的发现，当$\hat p$趋近于0的时候，$-log (\hat p)$趋近于正无穷，这个正无穷其实就是一个很大的损失惩罚，因为当$\hat p$趋近于0时，$y$应该也是趋近于0，但是这里$y$定义的是1。当$\hat p$在不断趋近1的过程中，$-log (\hat p)$的值逐渐减小，既损失逐渐减小，当$\hat p$趋近于1时，$y$应该也是趋近于0，和这里定义的是一致的，所以$-log (\hat p)$的值是0，说明没有损失。</p>
<p>下面再来看看$-log(1-x)$的曲线：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c9c81130b42d0ec7a1d9d13771791494.jpg" alt=""></p>
<p>同理因为概率的值域在$(0, 1)$之间，所以$-log(1-\hat p)$的曲线为：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/661c9948cbf818ba028f89db922b50eb.jpg" alt=""></p>
<p>这条曲线同样可以解释我们之前定义的损失趋势。</p>
<p>此时我们找到的损失函数还是根据不同的分类分成了两个，其实将其合成一个也很简单：</p>
<p>$$cost=-ylog(\hat p)-(1-y)log(1-\hat p)$$</p>
<p>如此一来，当$y=0$时，损失函数为$-log(1-\hat p)$，当$y=1$时，损失函数为$-log(\hat p)$。</p>
<p>上面的公式，是针对一个样本数据的，那么如果有多个样本数据，其实就是将这些样本数据的损失值加起来然后在求一下平均值：</p>
<p>$$\hat p = \frac 1 {1+e^{-\theta ^T X_b}}$$</p>
<p>$$L(\theta)=-\frac 1 m \sum_{i=1}^my^{(i)}log(\hat p^{(i)})+(1-y^{(i)})log(1-\hat p^{(i)})$$</p>
<p>下面我们要做的就是找到一组$\theta$值，使得上面的$L(\theta)$达到最小值。</p>
<h2 id="u635F_u5931_u51FD_u6570_u7684_u68AF_u5EA6"><a href="#u635F_u5931_u51FD_u6570_u7684_u68AF_u5EA6" class="headerlink" title="损失函数的梯度"></a>损失函数的梯度</h2><p>上面的公式是没法像线性回归那样求出一个正规方程解的，所以我们需要使用梯度下降法来求得使$L(\theta)$最小的一组$\theta$。下面我们先把公式都列出来：</p>
<ul>
<li>多元线性回归公式：$ \hat y = X_b \theta $，注意这里的$X_b$是加上了值全部为1的一列的矩阵，而为了方便推导，这里的$\theta$是一个列向量，就不写成$\theta^T$了。</li>
<li>Sigmoid函数：$\sigma(t) = \frac 1 {1+e^{-t}} $。</li>
<li>逻辑回归概率公式：$\hat p = \sigma(X_b \theta) =\frac 1 {1+e^{-X_b \theta}} $</li>
<li>逻辑回归损失函数：$L(\theta)=-\frac 1 m \sum_{i=1}^m(y^{(i)}log(\sigma(X_b^{(i)} \theta))+(1-y^{(i)})log(1-\sigma(X_b^{(i)} \theta))) $</li>
</ul>
<p>在第五篇笔记中我们知道，求损失函数的梯度就是对$\theta$这个列向量逐个元素求导：</p>
<p>$$\nabla L(\theta)=\begin{bmatrix}<br>\frac {\partial L(\theta)} {\partial \theta_0} \\<br>\frac {\partial L(\theta)} {\partial \theta_1} \\<br>\frac {\partial L(\theta)} {\partial \theta_2} \\<br>… \\<br>\frac {\partial L(\theta)} {\partial \theta_n} \\<br>\end{bmatrix}$$</p>
<p>我们从里往外来看，先从Sigmoid函数求导入手。</p>
<h3 id="Sigmoid_u51FD_u6570_u6C42_u5BFC"><a href="#Sigmoid_u51FD_u6570_u6C42_u5BFC" class="headerlink" title="Sigmoid函数求导"></a>Sigmoid函数求导</h3><p>先变换一下Sigmoid函数：</p>
<p>$$\sigma(t)=\frac 1 {1+e^{-t}} = (1 + e^{-t})^{-1}$$</p>
<p>然后对Sigmoid函数求导，这里遵循求导链式法则以及求导基本法则：</p>
<ul>
<li>复合函数 $(f\circ g)(x)$的导数$(f\circ g)’(x)$为：$(f\circ g)’(x)=f’(g(x))g’(x)$</li>
<li>代数函数导数：$\frac {dx^n} {dx} = nx^{n-1} \ \ \ (x \neq 0)$</li>
<li>数学常数的指数求导还是它自己：$\frac {de^x} {dx}=e^x$</li>
</ul>
<p>所以可得Sigmoid函数的导数为：<br>$$\sigma(t)^{‘} = (-1)(1+e^{-t})^{-2} \cdot e^{-t} \cdot (-1) =<br>(1+e^{-t})^{-2} \cdot e^{-t}<br>$$</p>
<h3 id="Sigmoid_u51FD_u6570_u7684_u5BF9_u6570_u6C42_u5BFC"><a href="#Sigmoid_u51FD_u6570_u7684_u5BF9_u6570_u6C42_u5BFC" class="headerlink" title="Sigmoid函数的对数求导"></a>Sigmoid函数的对数求导</h3><p>下面再往外扩展，来看一下$log(\sigma(t))$的导数。这里遵循的导数法则为：</p>
<ul>
<li>对以2为底的对数求导：$\frac {dlogx} {dx} = \frac 1 x$</li>
<li>复合函数 $(f\circ g)(x)$的导数$(f\circ g)’(x)$为：$(f\circ g)’(x)=f’(g(x))g’(x)$</li>
</ul>
<p>所以$log(\sigma(t))$的导数为：<br>$$log(\sigma(t))^{‘} = \frac 1 {\sigma(t)} \cdot \sigma(t)^{‘}=\\<br>\frac 1 {(1+e^t)^{-1}} \cdot (1+e^{-t})^{-2} \cdot e^{-t} = \\<br>(1+e^{-t})^{-1} \cdot e^{-t}=\frac {e^{-t}} {1+e^{-t}}=\\<br>\frac {1+e^{-t}-1} {1+e^{-t}} = 1-\frac 1 {1+e^{-t}}=\\<br>1-\sigma(t)<br>$$</p>
<p>$log(1-\sigma(t))$的导数为：</p>
<p>$$log(1-\sigma(t))^{‘}=\frac 1 {1-\sigma(t)} \cdot (-\sigma(t))^{‘}=\\<br>\frac 1 {1- \frac 1 {1+e^{-t}}} \cdot (1+e^{-t})^{-2} \cdot e^{-t} \cdot -1=\\<br>\frac {(1+e^{-t})^{-1}} {e^{-t}} \cdot e^{-t} \cdot -1 = \\<br>-(1+e^{-t})^{-1} = -\sigma(t)<br>$$</p>
<h3 id="u903B_u8F91_u56DE_u5F52_u635F_u5931_u51FD_u6570_u6C42_u5BFC"><a href="#u903B_u8F91_u56DE_u5F52_u635F_u5931_u51FD_u6570_u6C42_u5BFC" class="headerlink" title="逻辑回归损失函数求导"></a>逻辑回归损失函数求导</h3><p>当我们知道了Sigmoid函数和Sigmoid函数的对数的求导结果后，我们对逻辑回归损失函数求导就很容易了（这里对第$j$个$\theta$求导），先来看前半部分：<br>$$\frac {d(y^{(i)}log(\sigma(X_b^{(i)} \theta)))} {d\theta_j}=\\<br>y^{(i)} \cdot (1-\sigma(X_b^{(i)} \theta)) \cdot X_j^{(i)}<br>$$</p>
<p>最后的那个$X_j^{(i)}$是第$j$个$\theta$前面的系数，再来看后半部分：<br>$$\frac {d((1-y^{i})log(1-\sigma(X_b^{(i)} \theta)))} {d\theta_j}=\\<br>(1-y^{(i)}) \cdot (-\sigma(X_b^{(i)} \theta)) \cdot X_j^{(i)}<br>$$</p>
<p>此时对整个损失函数求导就是上面两部分相加：<br>$$\frac {d(L(\theta))} {d(\theta_j)}=-\frac 1 m \sum_{i=1}^m (y^{(i)} \cdot (1-\sigma(X_b^{(i)} \theta)) \cdot X_j^{(i)} + (1-y^{(i)}) \cdot (-\sigma(X_b^{(i)} \theta)) \cdot X_j^{(i)})= \\<br>-\frac 1 m \sum_{i=1}^m(y^{(i)}X_j^{(i)}-y^{(i)}\sigma(X_b^{(i)} \theta) \cdot X_j^{(i)}-\sigma(X_b^{(i)} \theta) \cdot X_j^{(i)} + y^{(i)}\sigma(X_b^{(i)} \theta) \cdot X_j^{(i)})= \\<br>-\frac 1 m \sum_{i=1}^m(y^{(i)}X_j^{(i)}-\sigma(X_b^{(i)} \theta) \cdot X_j^{(i)})= \\<br>-\frac 1 m \sum_{i=1}^m(y^{(i)}-\sigma(X_b^{(i)} \theta)) \cdot X_j^{(i)})= \\<br>\frac 1 m \sum_{i=1}^m(\sigma(X_b^{(i)} \theta)-y^{(i)})X_j^{(i)}<br>$$</p>
<p>所以逻辑回归损失函数的梯度为：<br>$$\nabla L(\theta)=\frac 1 m \begin{bmatrix}<br>\sum_{i=1}^m(\sigma(X_b^{(i)} \theta)-y^{(i)}) \\<br>\sum_{i=1}^m(\sigma(X_b^{(i)} \theta)-y^{(i)})X_1^{(i)} \\<br>\sum_{i=1}^m(\sigma(X_b^{(i)} \theta)-y^{(i)})X_2^{(i)} \\<br>… \\<br>\sum_{i=1}^m(\sigma(X_b^{(i)} \theta)-y^{(i)})X_n^{(i)} \\<br>\end{bmatrix}$$</p>
<p>大家再来回顾一下第五篇笔记中线性回归的梯度：<br>$$\nabla L(\theta) = \frac 2 m\begin{bmatrix}<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})\\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_1^{(i)} \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_2^{(i)} \\<br>…\\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<p>可以发现这两个梯度在形态上是非常相似的，因为线性回归的梯度通过向量化可以优化为：<br>$$\nabla L(\theta) =\frac 2 m X_b^\top(X_b \theta - y)$$</p>
<p>所以逻辑回归的梯度最终可以写为：<br>$$\nabla L(\theta) =\frac 1 m X_b^\top(\sigma(X_b \theta) - y)$$</p>
<h2 id="u5B9E_u73B0_u903B_u8F91_u56DE_u5F52_u7B97_u6CD5"><a href="#u5B9E_u73B0_u903B_u8F91_u56DE_u5F52_u7B97_u6CD5" class="headerlink" title="实现逻辑回归算法"></a>实现逻辑回归算法</h2><p>因为逻辑回归拟合损失函数使用的是梯度下降法，所以我们封装逻辑回归算法时大部分都可以套用我们之前封装的线性回归梯度下降方法，需要修改的只是损失函数、预测和评分里的一些代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> .metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="comment"># 截距theta0</span></span><br><span class="line">		self.intercept_ = <span class="keyword">None</span></span><br><span class="line">		<span class="comment"># 系数，theta1 ... thetaN</span></span><br><span class="line">		self.coef_ = <span class="keyword">None</span></span><br><span class="line">		<span class="comment"># theta列向量</span></span><br><span class="line">		self._theta = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 定义Sigmoid私有函数</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(self, t)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">1.</span> / (<span class="number">1.</span> + np.exp(-t))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 使用批量梯度下降法，根据训练数据集X_train，y_train训练LogisticRegression模型</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, y_train, is_debug=False, eta=<span class="number">0.01</span>, n_iters=<span class="number">1e4</span>)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \</span><br><span class="line">			<span class="string">"特征数据矩阵的行数要等于样本结果数据的行数"</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 定义逻辑回归损失函数</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">			<span class="comment"># 定义逻辑回归概率公式</span></span><br><span class="line">			y_hat = self._sigmoid(X_train.dot(theta))</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span>:</span><br><span class="line">				<span class="keyword">return</span> -np.sum(y*np.log(y_hat)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-y_hat)) / len(X_b)</span><br><span class="line">			<span class="keyword">except</span>:</span><br><span class="line">				<span class="keyword">return</span> float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 定义逻辑回归梯度</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">dL</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">			<span class="keyword">return</span> X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(X_b)</span><br><span class="line"></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">dL_debug</span><span class="params">(theta, X_b, y, epsilon=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">			<span class="comment"># 开辟大小与theta向量一致的向量空间</span></span><br><span class="line">			result = np.empty(len(theta))</span><br><span class="line">			<span class="comment"># 便利theta向量中的每一个theta</span></span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta)):</span><br><span class="line">				<span class="comment"># 复制一份theta向量</span></span><br><span class="line">				theta_1 = theta.copy()</span><br><span class="line">				<span class="comment"># 将第i个theta加上一个距离，既求该theta正方向的theta</span></span><br><span class="line">				theta_1[i] += epsilon</span><br><span class="line">				<span class="comment"># 在复制一份theta向量</span></span><br><span class="line">				theta_2 = theta.copy()</span><br><span class="line">				<span class="comment"># 将第i个theta减去同样的距离，既求该theta负方向的theta</span></span><br><span class="line">				theta_2[i] -= epsilon</span><br><span class="line">				<span class="comment"># 求出这两个点连线的斜率，既模拟该theta的导数</span></span><br><span class="line">				result[i] = (L(theta_1, X_b, y) - L(theta_2, X_b, y)) / (<span class="number">2</span> * epsilon)</span><br><span class="line">			<span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 实现批量梯度下降法</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X_b, y, initial_theta, eta, difference=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">			theta = initial_theta</span><br><span class="line">			i_iter = <span class="number">0</span></span><br><span class="line">			<span class="keyword">while</span> i_iter &lt; n_iters:</span><br><span class="line">				<span class="comment"># 当is_debug为True时走debug的求梯度的方法，反之走梯度公式的方法</span></span><br><span class="line">				<span class="keyword">if</span> is_debug:</span><br><span class="line">					gradient = dL_debug(theta, X_b, y)</span><br><span class="line">				<span class="keyword">else</span>:</span><br><span class="line">					gradient = dL(theta, X_b, y)</span><br><span class="line">				last_theta = theta</span><br><span class="line">				theta = theta - eta * gradient</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (abs(L(theta, X_b, y) - L(last_theta, X_b, y)) &lt; difference):</span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">				i_iter += <span class="number">1</span></span><br><span class="line">			<span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 构建X_b</span></span><br><span class="line">		X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br><span class="line">		<span class="comment"># 初始化theta向量为元素全为0的向量</span></span><br><span class="line">		initial_theta = np.zeros(X_b.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">		self._theta = gradient_descent(X_b, y_train, initial_theta, eta)</span><br><span class="line">		self.intercept_ = self._theta[<span class="number">0</span>]</span><br><span class="line">		self.coef_ = self._theta[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 计算概率，给定待预测数据集X_predict，返回表示X_predict的结果概率向量</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict_probability</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> self.intercept_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.coef_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">		<span class="string">"截距和系数都不为空，表示已经经过了fit方法"</span></span><br><span class="line">		<span class="keyword">assert</span> X_predict.shape[<span class="number">1</span>] == len(self.coef_), \</span><br><span class="line">		<span class="string">"要预测的特征数据集列数要与theta的系数数量相等"</span></span><br><span class="line"></span><br><span class="line">		X_b = np.hstack([np.ones((len(X_predict), <span class="number">1</span>)), X_predict])</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 返回0，1之间的浮点数</span></span><br><span class="line">		<span class="keyword">return</span> self._sigmoid(X_b.dot(self._theta))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 给定待预测数据集X_predict，返回表示X_predict的结果向量</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> self.intercept_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.coef_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">		<span class="string">"截距和系数都不为空，表示已经经过了fit方法"</span></span><br><span class="line">		<span class="keyword">assert</span> X_predict.shape[<span class="number">1</span>] == len(self.coef_), \</span><br><span class="line">		<span class="string">"要预测的特征数据集列数要与theta的系数数量相等"</span></span><br><span class="line"></span><br><span class="line">		probability = self.predict_probability(X_predict)</span><br><span class="line">		<span class="comment"># 将概率转换为0和1的向量，True对应1，False对应0</span></span><br><span class="line">		<span class="keyword">return</span> np.array(probability &gt;= <span class="number">0.5</span>, dtype=<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据测试数据集X_test和y_test确定当前模型的准确度</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X_test, y_test)</span>:</span></span><br><span class="line">		y_predict = self.predict(X_test)</span><br><span class="line">		<span class="keyword">return</span> accuracy_score(y_test, y_predict)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">"LinearRegression()"</span></span><br></pre></td></tr></table></figure>
<p>下面我们在Jupyter Notebook中使用Scikit Learn提供的鸢尾花数据验证我们封装的逻辑回归的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure>
<p>因为鸢尾花数据中有三类鸢尾花，而逻辑回归在一开始就讲过是一个解决二分类问题的算法，所以我们只取前两类的鸢尾花数据来验证，并且只用每类鸢尾花的前两个特征，方便绘图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 只取前两类的鸢尾花数据</span></span><br><span class="line">X = X[y&lt;<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line">y = y[y&lt;<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ee5c380438a6b086cbebbf6ade764dc5.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.LogisticRegression <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y, seed=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X_train, y_train)</span><br><span class="line">log_reg.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">log_reg.predict_probability(X_test)</span><br><span class="line"><span class="comment"># 概率结果</span></span><br><span class="line">array([ <span class="number">0.92972035</span>,  <span class="number">0.98664939</span>,  <span class="number">0.14852024</span>,  <span class="number">0.17601199</span>,  <span class="number">0.0369836</span> ,</span><br><span class="line">		<span class="number">0.0186637</span> ,  <span class="number">0.04936918</span>,  <span class="number">0.99669244</span>,  <span class="number">0.97993941</span>,  <span class="number">0.74524655</span>,</span><br><span class="line">		<span class="number">0.04473194</span>,  <span class="number">0.00339285</span>,  <span class="number">0.26131273</span>,  <span class="number">0.0369836</span> ,  <span class="number">0.84192923</span>,</span><br><span class="line">		<span class="number">0.79892262</span>,  <span class="number">0.82890209</span>,  <span class="number">0.32358166</span>,  <span class="number">0.06535323</span>,  <span class="number">0.20735334</span>])</span><br><span class="line"></span><br><span class="line">log_reg.predict(X_test)</span><br><span class="line"><span class="comment"># 分类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">y_test</span><br><span class="line"><span class="comment"># 测试数据结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到我们封装的逻辑回归算法对鸢尾花的分类是100%准确的，当然也是因为当前的数据比较简单。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在这一篇笔记中我们来学习目前应用比较广泛的一个分类算法逻辑回归（Logistic Regression）。</p>
<h2 id="u4EC0_u4E48_u662F_u903B_u8F91_u56DE_u5F52"><a href="#u4EC0_u4E48_u662F_u903B_u8F91_u56DE_u5F52" class="headerlink" title="什么是逻辑回归"></a>什么是逻辑回归</h2><p>首先大家应该有会有一个疑问，为什么既然叫逻辑回归，但是解决的却是分类问题呢。因为逻辑回归是将样本的特征和样本发生的概率联系起来，在拟合样本数据发生概率的时候，其实是在解决一个回归问题，当概率计算出来后，再根据概率进行分类处理。所以逻辑回归在解决分类问题时，其实中间还是进行了回归问题的处理，但是逻辑回归只能解决二分类问题，这一点要注意。</p>
<p>逻辑回归在解决概率这个回归问题时，和线性回归、多项式回归有一个不同的地方，那就是后者训练出的模型，预测其他样本数据的目标值时值域理论上是在负无穷到正无穷的，换句话说也就是对值域是没有什么限制的。而对于表示概率的数而言，一般值域都是在0到1之间，真实世界中，概率不都是按从0%到100%表示嘛。所以这就引出了逻辑回归和线性回归的相似和不同处，继而体现在公式模型上。在第四篇笔记中，我们知道多元线性回归的公式模型是：<br>$$ \hat y = \theta ^T X_b  $$</p>
<p>因为$\hat y$的值域为$(-\infty,+\infty)$，所以如果要在逻辑回顾中运用在求解概率的公式模型上就需要另外一个函数将其值域限定在$(0, 1)$，我们将这个函数称为<strong>Sigmoid</strong>函数：</p>
<p>$$\hat p = \sigma(\theta ^T X_b)$$</p>
<p>$$\sigma(t) = \frac 1 {1+e^{-t}}$$</p>
<p>最终逻辑回归求解概率的模型为：</p>
<p>$$\hat p = \frac 1 {1+e^{-\theta ^T X_b}}$$</p>]]>
    
    </summary>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://www.devtalking.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记九之交叉验证、模型正则化]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-9/"/>
    <id>http://www.devtalking.com//articles/machine-learning-9/</id>
    <published>2018-03-29T16:00:00.000Z</published>
    <updated>2018-08-27T02:32:47.728Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u9A8C_u8BC1_u6570_u636E_u96C6_u4E0E_u4EA4_u53C9_u9A8C_u8BC1"><a href="#u9A8C_u8BC1_u6570_u636E_u96C6_u4E0E_u4EA4_u53C9_u9A8C_u8BC1" class="headerlink" title="验证数据集与交叉验证"></a>验证数据集与交叉验证</h2><p>在上一篇笔记中我们又提到了训练数据集和测试数据集，拆分样本数据的这种做法目的就是通过测试数据集判断模型的好坏，如果我们发现训练出的模型产生了过拟合的现象，既在训练数据集上预测评分很好，但是在测试数据集上预测评分不好的情况，那可能就需要重新调整超参数训练模型，以此类推，最终找到一个或一组参数使得模型在测试数据集上的预测评分也很好，也就是训练出的模型泛化能力比较好。那么这种方式会产生一个问题，就是有可能会针对测试数据过拟合，因为每次都是找到参数训练模型，然后看看在测试数据集上的表现如何，这就让我们的模型又被测试数据集左右了，既可以理解为训练出的模型对特定的训练数据集和特定的测试数据集表现都不错，但是再来一种类似的样本数据，表现可能又不尽如人意了。</p>
<p>那么要彻底解决这个问题，就要引入验证数据集的概念，既将样本数据分为三份，训练数据集、验证数据集、测试数据集。</p>
<ul>
<li>训练数据集和之前的用途一样，是用来训练模型的。</li>
<li>验证数据集的作用和之前的测试数据集一样，是用来验证由训练数据集训练出的模型的好坏程度的，或者说是调整超参数使用的数据集。</li>
<li>此时的测试数据集和之前的作用就不一样了，这里的测试数据集是当训练出的模型在训练数据集和验证数据集上都表现不错的前提下，最终衡量该模型性能的数据集。测试数据集在整个训练模型的过程中是不参与的。</li>
</ul>
<h3 id="u4EA4_u53C9_u9A8C_u8BC1_uFF08Cross_Validation_uFF09"><a href="#u4EA4_u53C9_u9A8C_u8BC1_uFF08Cross_Validation_uFF09" class="headerlink" title="交叉验证（Cross Validation）"></a>交叉验证（Cross Validation）</h3><p>我们在验证数据集概念的基础上，再来看看交叉验证。交叉验证其实解决的是随机选取验证数据集的问题，因为如果验证数据集是固定的，那么万一验证数据集过拟合了，那就没有可用的验证数据集了，所以交叉验证提供了随机的、可持续的、客观的模型验证方式。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9b3e80db3abbe355b5251935cb97e28b.jpg" alt=""></p>
<p>交叉验证的思路是将训练数据分成若干份，假设分为A、B、C三份，分别将这三份各作为一次验证数据集，其他两份作为训练数据集训练模型，然后将训练出的三个模型评分取均值，将这个均值作为衡量算法训练模型的结果来调整参数，如果平均值不够好，那么再调整参数，再训练出三个模型，以此类推。</p>
<a id="more"></a>
<h3 id="u5B9E_u73B0_u4EA4_u53C9_u9A8C_u8BC1"><a href="#u5B9E_u73B0_u4EA4_u53C9_u9A8C_u8BC1" class="headerlink" title="实现交叉验证"></a>实现交叉验证</h3><p>我们使用KNN算法，用训练数据集和测试数据集方式进行超参数<code>k</code>和<code>p</code>的调整查找（KNN的<code>k</code>，<code>p</code>两个超参数查阅第二篇学习笔记）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KNN算法中使用训练数据集和测试数据集进行超参数k和p的调整</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化最佳评分，最佳k值和最佳p值</span></span><br><span class="line">best_score, best_k, best_p = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="comment"># k值从2到10之间搜寻</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>):</span><br><span class="line">	<span class="comment"># p值从1到5之间搜寻</span></span><br><span class="line">	<span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">		<span class="comment"># 对每个k值，p值的组合实例化KNN分类器，通过训练数据训练出模型，然后通过测试数据计算评分，每次将最好的评分和对应的k，p值记录下来，最终找到评分最好的k和p值</span></span><br><span class="line">		knn_clf = KNeighborsClassifier(weights=<span class="string">'distance'</span>, n_neighbors=k, p=p)</span><br><span class="line">		knn_clf.fit(X_train, y_train)</span><br><span class="line">		score = knn_clf.score(X_test, y_test)</span><br><span class="line">		<span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">			best_score, best_k, best_p = score, k, p</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best Score ="</span>, best_score)</span><br><span class="line">print(<span class="string">"Best k = "</span>, best_k)</span><br><span class="line">print(<span class="string">"Best p ="</span>, best_p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Best Score = <span class="number">0.986091794159</span></span><br><span class="line">Best k =  <span class="number">3</span></span><br><span class="line">Best p = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>从结果看，通过上面的算法，我们找到了最好评分98.6%和对应的<code>k</code>值3和<code>p</code>值4。但是需要注意的是，这个结果有可能是对测试数据集过拟合的结果。</p>
<p>下面我们再来看看如何使用交叉验证方法进行超参数的调参：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入Scikit Learn中交叉验证的函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">cross_val_score(knn_clf, X_train, y_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.98895028</span>,  <span class="number">0.97777778</span>,  <span class="number">0.96629213</span>])</span><br></pre></td></tr></table></figure>
<p>我们直接使用Scikit Learn中提供的交叉验证函数，默认会将训练数据分成三份，所以会有三个模型的评分。然后再修改一下上面调参的算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KNN算法中使用训练数据集和验证数据集进行超参数k和p的调整</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">best_score, best_k, best_p = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>):</span><br><span class="line">	<span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">		knn_clf = KNeighborsClassifier(weights=<span class="string">'distance'</span>, n_neighbors=k, p=p)</span><br><span class="line">		<span class="comment"># 对每个k值，p值的组合实例化KNN分类器，通过交叉验证发训练出若干个模型，然后对这若干个模型的评分求平均值，该平均值即为每次的模型评分，每次将最好的评分和对应的k，p值记录下来，最终找到评分最好的k和p值</span></span><br><span class="line">		scores = cross_val_score(knn_clf, X_train, y_train)</span><br><span class="line">		<span class="comment"># 对若干个模型的评分求平均值</span></span><br><span class="line">		score = np.mean(scores)</span><br><span class="line">		<span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">			best_score, best_k, best_p = score, k, p</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best Score ="</span>, best_score)</span><br><span class="line">print(<span class="string">"Best k = "</span>, best_k)</span><br><span class="line">print(<span class="string">"Best p ="</span>, best_p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Best Score = <span class="number">0.982359987401</span></span><br><span class="line">Best k =  <span class="number">2</span></span><br><span class="line">Best p = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>可以看到，使用交叉验证法最后搜寻到的最佳<code>k</code>值是2，最佳<code>p</code>值是2，然后我们再用搜寻出的这两个超参数来训练模型，然后使用测试数据集来计算评分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(weights=<span class="string">'distance'</span>, n_neighbors=<span class="number">2</span>, p=<span class="number">2</span>)</span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br><span class="line">knn_clf.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.98052851182197498</span></span><br></pre></td></tr></table></figure>
<p>最终我们搜寻到的最佳超参数训练出的模型，通过测试数据验证后评分为98.05%，这个评分虽然比之前用训练数据和测试数据搜寻到的最佳评分低一些，但是这个分数不会对验证数据集过拟合，是泛化能力更好的模型。</p>
<p>第三篇笔记中，讲过KNN通过网格搜索搜寻最佳超参数的方法，其实当时<code>GridSearchCV</code>中的CV就是Cross Validation的意思，也就是网格搜索本身就使用的交叉验证的方式搜寻超参数，我们再来回顾一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="comment"># 定义出超参数的组合，就相当于之前我们算法中的两层循环</span></span><br><span class="line">param_grid = [</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="string">"weights"</span>: [<span class="string">'distance'</span>],</span><br><span class="line">		<span class="string">"n_neighbors"</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">11</span>)],</span><br><span class="line">		<span class="string">"p"</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">	&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid, verbose=<span class="number">1</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Fitting <span class="number">3</span> folds <span class="keyword">for</span> each of <span class="number">45</span> candidates, totalling <span class="number">135</span> fits</span><br></pre></td></tr></table></figure>
<p>可以看到执行<code>fit</code>函数后，会打印出一句话来，意思就是超参数组合一共有45个，每个组合会将训练数据分为三份，一共会训练出135个模型，最后求出一个泛化能力最好的模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid_search.best_score_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.98237476808905377</span></span><br><span class="line"></span><br><span class="line">grid_search.best_params_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">&#123;<span class="string">'n_neighbors'</span>: <span class="number">2</span>, <span class="string">'p'</span>: <span class="number">2</span>, <span class="string">'weights'</span>: <span class="string">'distance'</span>&#125;</span><br><span class="line"></span><br><span class="line">best_knn_clf = grid_search.best_estimator_</span><br><span class="line">best_knn_clf.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.98052851182197498</span></span><br></pre></td></tr></table></figure>
<p>通过上面的结果可以看到，和我们之前的结果是一致的。</p>
<h2 id="u504F_u5DEE_uFF08Bias_uFF09_u4E0E_u65B9_u5DEE_uFF08Variance_uFF09"><a href="#u504F_u5DEE_uFF08Bias_uFF09_u4E0E_u65B9_u5DEE_uFF08Variance_uFF09" class="headerlink" title="偏差（Bias）与方差（Variance）"></a>偏差（Bias）与方差（Variance）</h2><p>在机器学习算法中，模型的好坏有一个统称就是预测结果的误差大小。那么这个误差具体可分为偏差和方差。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a608405bddca5a8c05d8727ff4ebb58d.jpg" alt=""><br>上面这幅图有四个靶子，可以很好的诠释方差和偏差的概念。红色靶心就相当于我们目标，灰色弹孔就相当于模型预测的值。我们来解读一下这四幅图：</p>
<ul>
<li>左上：模型预测的值基本都在目标值上，并且每次预测的都很集中，说明偏差和方差都很小。</li>
<li>左下：模型预测的值虽然每次都很集中，但是整体和目标值差的很远，说明偏差很大，方差比较小。</li>
<li>右上：模型预测的值基本都围绕着目标值，但是每次预测的值之间差距较大，说明偏差较小，方差比较大。</li>
<li>右下：模型预测的值离目标值都很远，并且每次预测的值之间差距也比较打，说明偏差和方差都很大。</li>
</ul>
<p>通常情况下，我们训练出的模型误差指的是偏差和方差的总和，再加上一些不可避免的误差，比如训练数据本身噪音比较大等。</p>
<p>通常导致偏差的主要原因是对问题本身的假设不正确，比如本身训练数据并没有线性关系，但我们还是使用线性回归去训练模型，那么模型的偏差肯定会很大，也就是欠拟合的情况。</p>
<p>通常导致方差的主要原因是因为我们的模型太过复杂，学习到太多的噪音，比如多项式回归，当<code>degree</code>参数非常大的时候，也就是过拟合的情况。</p>
<p>非参数学习通常都是高方差算法，比如分类的算法，因为不会对数据进行任何假设。参数学习通常都是高偏差算法，因为会对数据有极强的假设，一旦训练数据有问题，那么就会导致模型整体偏离真实情况。</p>
<p>在使用机器学习解决问题的实践中，通常我们的挑战都是降低模型的方差，一般有以下几种手段：</p>
<ul>
<li>降低模型复杂度。比如降低多项式回归的<code>degree</code>参数。</li>
<li>减少数据维度，降噪。比如使用PCA。</li>
<li>增加样本数量。让训练数据足以支撑复杂的模型，从而能计算出合适的参数。</li>
<li>使用交叉验证。避免过拟合情况。</li>
</ul>
<h2 id="u6A21_u578B_u6B63_u5219_u5316_uFF08Regularization_uFF09"><a href="#u6A21_u578B_u6B63_u5219_u5316_uFF08Regularization_uFF09" class="headerlink" title="模型正则化（Regularization）"></a>模型正则化（Regularization）</h2><p>在说模型正则化之前，我们先来看一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建和之前一样的样本数据</span></span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这些的折线图绘制出来</span></span><br><span class="line">plt.plot(np.sort(x), y[np.argsort(x)], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f88229c7c8998ded048224d19fd9b5a8.jpg" alt=""></p>
<p>可以看到当计算<code>y</code>的方程，多项式前的系数比较低（0.5和1）的时候折线图在横轴从-3到3，纵轴从-1到10的坐标系里还能全部展现，如果将这两个系数扩大10倍，会出现什么情况呢：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y1 = <span class="number">5</span> * x ** <span class="number">2</span> + <span class="number">10</span> * x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line">plt.plot(np.sort(x), y1[np.argsort(x)], color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/cf61ca378119f75352f49f34089284a1.jpg" alt=""></p>
<p>可以看到这个折线图波动已经非常大了，在同样的坐标系中只能展现出一部分了。我们再来回顾一下前面的那张过拟合的图：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/e60fe9ae830a92027e0ffd9f615bba95.jpg" alt=""></p>
<p>上图中，两侧的曲线图波动非常大，其实就说明了这条曲线的线性多项式方程的系数非常大，那么模型正则化做的事情就是限制这些系数的大小。</p>
<p>下面来看看模型正则化的基本思路，在第四篇笔记中将多元线性回归问题的时候，我们知道最终求的是下面这个函数的最优解，既让下面这个损失函数的值尽可能的小：</p>
<p>$$ \sum_{i=1}^m(y^{(i)}- (\theta_0+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+…+\theta_nX_n^{(i)} ))^2$$</p>
<p>下面我们来转变一下这个损失函数：</p>
<p>$$ L(\theta) = \sum_{i=1}^m(y^{(i)}- (\theta_0+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+…+\theta_nX_n^{(i)} ))^2 + \alpha \frac 1 2 \sum_{i=1}^n \theta_i^2$$</p>
<p>我们在损失函数里加了一部分$\alpha \frac 1 2 \sum_{i=1}^n \theta_i^2$，此时要想让损失函数尽可能的小，就不能只考虑前面那一部分了，还要考虑后面新加的这一部分，又因为后面这部分包含多项式系数的平方，所以就整体约束了多项式系数的大小。这就是模型正则化的基本思路。这里的$\alpha$就是一个新的超参数，它的含义代表在模型正则化下，新的损失函数中每一个$\theta$都尽可能的小，这个小的程度占整个优化损失函数的多少。比如如果$\alpha$是0，那么相当于损失函数没有加入模型正则化，如果$\alpha$非常非常大，那么真正的损失函数就可以忽略了，主要考虑使模型正则化中的$\theta$尽可能小。所以$\alpha$的作用就是让真正的损失函数尽可能小和新加入的模型正则化中的$\theta$尽可能小之间找到一个平衡，在实际的运用中，不同的数据，$\alpha$的取值也不同，是需要有不断调整$\alpha$这个超参数的过程。</p>
<h3 id="u5CAD_u56DE_u5F52_uFF08Ridge_Regression_uFF09"><a href="#u5CAD_u56DE_u5F52_uFF08Ridge_Regression_uFF09" class="headerlink" title="岭回归（Ridge Regression）"></a>岭回归（Ridge Regression）</h3><p>在模型正则化中，加入$\alpha \frac 1 2 \sum_{i=1}^n \theta_i^2$的方式，称之为岭回归。下面通过不使用模型正则化和使用岭回归对比来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x + <span class="number">3</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ce6f0729ff4665954be6ab5e2d9a79f8.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入Pipeline和其他需要打包进Pipeline的类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将样本数据集拆分为训练数据集和测试数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.4</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建多项式线性回归Pipeline</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PloynomialRegression</span><span class="params">(degree)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">"std_scalar"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"lr"</span>, LinearRegression())</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入均方误差MSE函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line">ploy_reg = PloynomialRegression(degree=<span class="number">20</span>)</span><br><span class="line">ploy_reg.fit(X_train, y_train)</span><br><span class="line">y_poly_predict = ploy_reg.predict(X_test)</span><br><span class="line">mean_squared_error(y_test, y_poly_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.9558727426614517</span></span><br><span class="line"></span><br><span class="line">X_ploy = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y_ploy = ploy_reg.predict(X_ploy)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(X_ploy[:, <span class="number">0</span>], y_ploy, color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([-<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a8b84f8353e822a872f6a0f34ee5761b.jpg" alt=""></p>
<p>可以看到不试用模型正则化时，MSE为1.95，虽然数值不大，但是拟合曲线的波动非常大，是一个过拟合的特征。下面来看看使用模型正则化岭回归后结果会怎样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入Scikit Learn中岭回归的类Ridge</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建通过岭回归进行模型正则化的Pipeline，有两个超参数degree和alpha</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RidgeRegression</span><span class="params">(degree, alpha)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">"std_scalar"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"rr"</span>, Ridge(alpha=alpha))</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">ridge_reg = RidgeRegression(<span class="number">20</span>, <span class="number">0.0001</span>)</span><br><span class="line">ridge_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_ridge_predict = ridge_reg.predict(X_test)</span><br><span class="line">mean_squared_error(y_test, y_ridge_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.0033258623784587</span></span><br><span class="line"></span><br><span class="line">X_ridge = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y_ridge = ridge_reg.predict(X_ridge)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(X_ridge[:, <span class="number">0</span>], y_ridge, color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([-<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/db8310345d08409779200070e8c67bf0.jpg" alt=""></p>
<p>使用岭回归后MSE降低到1，并且拟合曲线波动平缓了许多，尤其是曲线两头。这就是模型正则化的作用。</p>
<h3 id="LASSO__u56DE_u5F52_uFF08Least_Absolute_Shrinkage_and_Selection_Operator_Regression_uFF09"><a href="#LASSO__u56DE_u5F52_uFF08Least_Absolute_Shrinkage_and_Selection_Operator_Regression_uFF09" class="headerlink" title="LASSO 回归（Least Absolute Shrinkage and Selection Operator Regression）"></a>LASSO 回归（Least Absolute Shrinkage and Selection Operator Regression）</h3><p>其实岭回归和LASSO都是模型正则化的一种具体实现，区别就在于增加的模型正则公式不同。岭回归增加的是$\alpha \frac 1 2 \sum_{i=1}^n \theta_i^2$，而LASSO回归中增加的是$\alpha \sum_{i=1}^n |\theta_i|$。</p>
<p>下面来看看相同的数据使用LASSO回归后的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x + <span class="number">3</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.4</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入Pipeline和其他需要打包进Pipeline的类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LassoRegression</span><span class="params">(degree, alpha)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">"std_scalar"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"lasso"</span>, Lasso(alpha=alpha))</span><br><span class="line">	])</span><br><span class="line"></span><br><span class="line">lasso_reg = LassoRegression(<span class="number">20</span>, <span class="number">0.01</span>)</span><br><span class="line">lasso_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_lasso_predict = lasso_reg.predict(X_test)</span><br><span class="line">mean_squared_error(y_test, y_lasso_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.90488682905372464</span></span><br><span class="line"></span><br><span class="line">X_lasso = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y_lasso = lasso_reg.predict(X_lasso)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(X_lasso[:, <span class="number">0</span>], y_lasso, color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([-<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/00dd5241732ed5f4a1d681ee4091246b.jpg" alt=""></p>
<p>可以看到相同的数据，通过LASSO处理后的模型MSE更小一些，拟合曲线也更平滑。</p>
<h3 id="u5CAD_u56DE_u5F52_u548CLASSO_u56DE_u5F52_u7684_u6570_u5B66_u542B_u4E49_u4E0A_u7684_u533A_u522B"><a href="#u5CAD_u56DE_u5F52_u548CLASSO_u56DE_u5F52_u7684_u6570_u5B66_u542B_u4E49_u4E0A_u7684_u533A_u522B" class="headerlink" title="岭回归和LASSO回归的数学含义上的区别"></a>岭回归和LASSO回归的数学含义上的区别</h3><p>下面来解释一下岭回归和LASSO回归之间深层意义上的区别。先来看看岭回归，在前面讲过，当$\alpha$趋近无穷大时，真正的损失函数就可以忽略了，模型正则化后的损失函数就变成了求解模型正则公式的最小值，既求$\alpha \frac 1 2 \sum_{i=1}^n \theta_i^2$的最小值，那么该公式的梯度就是对$\theta$求导可得：</p>
<p>$$ \nabla = \begin{bmatrix}<br>\theta_1 \\<br>\theta_2 \\<br>\theta_3 \\<br>… \\<br>\theta_n<br>\end{bmatrix} $$</p>
<p>数据点按照梯度一步一步寻求最优解，如下图：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f349a0f686afc20da5d749f1b9683b5f.jpg" alt=""><br>所以经过岭回归处理后训练出的模型，拟合线基本都是曲线。</p>
<p>再来看看LASSO回归，当$\alpha$趋近无穷大时，我们只考虑$\alpha \sum_{i=1}^n |\theta_i|$，但是$|\theta_i|$是不可导的，我们使用数学符号函数来表示其梯度：</p>
<blockquote>
<p>符号函数（$sign(x)$）是很有用的一类函数，能够帮助我们实现一些直接实现有困难的情况。在数学和计算机运算中，其功能是取某个数的符号（正或负），既当$x &gt; 0$时，$sign(x) = 1$，当$x = 0$时，$sign(x) = 0$，当$x &lt; 0$时，$sign(x) = -1$。</p>
</blockquote>
<p>$$ \nabla = \alpha \begin{bmatrix}<br>sign(\theta_1) \\<br>sign(\theta_2) \\<br>sign(\theta_3) \\<br>… \\<br>sign(\theta_n)<br>\end{bmatrix} $$</p>
<p>对于$\alpha \sum_{i=1}^n |\theta_i|$而言，当$x &gt; 0$时，就相当于$y = x$这条直线，当$x &lt; 0$时，就相当于$y = -x$这条直线。当数据点按照LASSO的梯度寻找最优解的路线是下图情况：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/1944c9ae33c895730eef92205517cac0.jpg" alt=""></p>
<p>可以看到真正的寻址路线是橘黄色的虚线，应该中间有很多点直接打到了$y$轴，所以绘制出的拟合蓝色线很多时候就是一条直线，而不是曲线。那些$sign(x)$为0的梯度，既损失函数中的一部分$\theta$为0，这也就说明了LASSO中最后的SO（Selection Operator）选择操作符的含义，既将一些噪音比较大的特征过滤掉，选择出主要的、有用的特征。但是这是有风险的，因为很有可能LASSO将实际有用的特征给过滤掉了，所以就模型正则化的准确率来说，岭回归还是更好一些。但是在处理有巨大量特征的样本数据时，使用LASSO可以作为降低特征数量的一种方法。</p>
<h2 id="LP_u8303_u6570"><a href="#LP_u8303_u6570" class="headerlink" title="LP范数"></a>LP范数</h2><p>在数学定义上，范数包括向量范数和矩阵范数，向量范数表示向量空间中向量的大小，矩阵范数表征矩阵引起向量变化的大小。比如对于向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样。对于矩阵范数，我们知道，通过运算$Ax=B$，可以将向量$x$变化为矩阵$B$，矩阵范数就是来度量这个变化大小的。</p>
<p>下面我们再来看一张图：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/5b021819b37fe49fb71ec11829ffa93f.jpg" alt=""></p>
<p>上图展示了模型正则化岭回归、LASSO回归，线性回归评测标准均方误差（MSE）、平均绝对误差（MAE），距离公式欧拉距离、曼哈顿距离之间的对比。我们可以发现一个很有意思的现象，第一行的三个公式都是平方求和的模式，第二行的三个公式都是绝对值求和的模式。其实虽然机器学习中的公式、名词很多，但是究其背后的数学原理都是有规律可循的。</p>
<p>我们在讲KNN时知道它除了$k$这个超参数外还有一个超参数$p$，继而介绍了明可夫斯基距离：</p>
<p>$$ （\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|^p）^\frac 1 p $$</p>
<p>我们对明可夫斯基距离公式再进行一下泛化，将其提炼成这种形式：</p>
<p>$$||X||_p=(\sum_{i=1}^n|X_i|^p)^{\frac 1 p}$$</p>
<p>在数学上，我们将上面这个公式称为$L_p$范数，当$p$为1时，$L_1$范数就是曼哈顿距离，$p$为2时，$L_2$范数就是欧拉距离。那么在模型正则项中，是$L_2$范数就是岭回归，或者叫$L_2$正则项，$L_1$范数就是LASSO回归，或者叫$L_1$正则化。</p>
<blockquote>
<p>这里要注意的时，如果是$L_2$范数，岭回归的模型正则化公式应该还需要开根号，但是为了计算方便，一般使用时不加这个开根号，但是加不加根号对于模型正则化的效果来说是一样的。</p>
</blockquote>
<h3 id="u5F39_u6027_u7F51_uFF08Elastic_Net_uFF09"><a href="#u5F39_u6027_u7F51_uFF08Elastic_Net_uFF09" class="headerlink" title="弹性网（Elastic Net）"></a>弹性网（Elastic Net）</h3><p>弹性网很简单，就是将$L_1$正则项和$L_2$正则项都加入模型正则化中，既结合了岭回归和LASSO回归：</p>
<p>$$ L(\theta) = \sum_{i=1}^m(y^{(i)}- (\theta_0+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+…+\theta_nX_n^{(i)} ))^2 + (1-r)\alpha \frac 1 2 \sum_{i=1}^n \theta_i^2 + r\alpha \sum_{i=1}^n |\theta_i|$$</p>
<p>从公式中看到我们又引入了一个超参数$r$，这个超参数表示岭回归和LASSO回归在整个模型正则化中各占的比例。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u9A8C_u8BC1_u6570_u636E_u96C6_u4E0E_u4EA4_u53C9_u9A8C_u8BC1"><a href="#u9A8C_u8BC1_u6570_u636E_u96C6_u4E0E_u4EA4_u53C9_u9A8C_u8BC1" class="headerlink" title="验证数据集与交叉验证"></a>验证数据集与交叉验证</h2><p>在上一篇笔记中我们又提到了训练数据集和测试数据集，拆分样本数据的这种做法目的就是通过测试数据集判断模型的好坏，如果我们发现训练出的模型产生了过拟合的现象，既在训练数据集上预测评分很好，但是在测试数据集上预测评分不好的情况，那可能就需要重新调整超参数训练模型，以此类推，最终找到一个或一组参数使得模型在测试数据集上的预测评分也很好，也就是训练出的模型泛化能力比较好。那么这种方式会产生一个问题，就是有可能会针对测试数据过拟合，因为每次都是找到参数训练模型，然后看看在测试数据集上的表现如何，这就让我们的模型又被测试数据集左右了，既可以理解为训练出的模型对特定的训练数据集和特定的测试数据集表现都不错，但是再来一种类似的样本数据，表现可能又不尽如人意了。</p>
<p>那么要彻底解决这个问题，就要引入验证数据集的概念，既将样本数据分为三份，训练数据集、验证数据集、测试数据集。</p>
<ul>
<li>训练数据集和之前的用途一样，是用来训练模型的。</li>
<li>验证数据集的作用和之前的测试数据集一样，是用来验证由训练数据集训练出的模型的好坏程度的，或者说是调整超参数使用的数据集。</li>
<li>此时的测试数据集和之前的作用就不一样了，这里的测试数据集是当训练出的模型在训练数据集和验证数据集上都表现不错的前提下，最终衡量该模型性能的数据集。测试数据集在整个训练模型的过程中是不参与的。</li>
</ul>
<h3 id="u4EA4_u53C9_u9A8C_u8BC1_uFF08Cross_Validation_uFF09"><a href="#u4EA4_u53C9_u9A8C_u8BC1_uFF08Cross_Validation_uFF09" class="headerlink" title="交叉验证（Cross Validation）"></a>交叉验证（Cross Validation）</h3><p>我们在验证数据集概念的基础上，再来看看交叉验证。交叉验证其实解决的是随机选取验证数据集的问题，因为如果验证数据集是固定的，那么万一验证数据集过拟合了，那就没有可用的验证数据集了，所以交叉验证提供了随机的、可持续的、客观的模型验证方式。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9b3e80db3abbe355b5251935cb97e28b.jpg" alt=""></p>
<p>交叉验证的思路是将训练数据分成若干份，假设分为A、B、C三份，分别将这三份各作为一次验证数据集，其他两份作为训练数据集训练模型，然后将训练出的三个模型评分取均值，将这个均值作为衡量算法训练模型的结果来调整参数，如果平均值不够好，那么再调整参数，再训练出三个模型，以此类推。</p>]]>
    
    </summary>
    
      <category term="LP范数" scheme="http://www.devtalking.com/tags/LP%E8%8C%83%E6%95%B0/"/>
    
      <category term="交叉验证" scheme="http://www.devtalking.com/tags/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    
      <category term="偏差" scheme="http://www.devtalking.com/tags/%E5%81%8F%E5%B7%AE/"/>
    
      <category term="方差" scheme="http://www.devtalking.com/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型正则化" scheme="http://www.devtalking.com/tags/%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记八之多项式回归、拟合程度、模型泛化]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-8/"/>
    <id>http://www.devtalking.com//articles/machine-learning-8/</id>
    <published>2018-03-14T16:00:00.000Z</published>
    <updated>2018-08-27T02:31:23.331Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在前面的笔记中，我们使用的样本数据都是具有一定线性关系的，我们使用简单线性回归或者多元线性回归来拟合这些数据。但是这种具有强假设的数据集在实际应用中是比较少的，大多数的样本数据都没有明显的线性关系，那么这一节主要来看看如何对非线性关系的样本数据进行处理并预测。</p>
<h2 id="u591A_u9879_u5F0F_u56DE_u5F52"><a href="#u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>多项式回归就是我们要使用的方法，其本质上还是基于线性回归的数学原理，只不过是对损失函数进行巧妙的改变使得原本我们用线性回归求得一条直线，变为求得一条曲线。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ba54b737f3717cf2fec01814463c4a4c.jpg" alt=""><br>在线性回归的问题中，对于这些数据（蓝色点），我们是要寻找一条直线，让这条直线尽可能的拟合这些数据，如果数据只有一个特征的话，我们的模型就是：</p>
<p>$$ y=ax + b $$</p>
<p>$x$就是数据的已知特征，$a$和$b$是我们的模型需要求出的参数。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f6a83a4e5b65ca84c17ff6d5468c63e2.jpg" alt=""></p>
<p>那如果我们的数据是像上图这种分布形态呢？显然红色那条直线无法拟合这些数据，而黄色那条曲线能更好的拟合这些数据。在样本数据特征数量不变的情况下，这条黄色曲线的模型其实就是一个一元二次方程：</p>
<p>$$y = ax^2+bx+c$$</p>
<p>我们抛开这个方程表示的坐标系形态来看，假设将$x^2$看作$x_1$，将$x$看作$x_2$，那么方式就可以写为：</p>
<p>$$y=ax_1+bx_2+c$$</p>
<p>这就变成了具有两个特征的样本数据的线性回归问题。现在大家就可以理解在本节开头所说的对损失函数进行巧妙改变的含义了吧。</p>
<p>所以对与多项式回归而言，就是需要我们为样本数据增加一个特征，使之可以用线性回归的原理更好的拟合样本数据，但是求出的是对于原始样本而言的非线性的曲线。</p>
<h3 id="u5B9E_u73B0_u591A_u9879_u5F0F_u56DE_u5F52"><a href="#u5B9E_u73B0_u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="实现多项式回归"></a>实现多项式回归</h3><p>这一节我们在Jupyter Notebook中看看如何实现多项式回归：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据，从-3到3的100个随机数</span></span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 转换为100行1列的矩阵，既样本数据只有一个特征</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求y的方程</span></span><br><span class="line">y = <span class="number">0.5</span> * x**<span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将样本数据绘制出来</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ccfd43e57d544b2be620023d01b3cf2a.jpg" alt=""></p>
<a id="more"></a>
<p>可以看到，我们的样本数据很明显是非线性的分布形态。下面我们先用线性回归直接来拟合这些样本数据看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入线性回归类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">lr.fit(X, y)</span><br><span class="line"><span class="comment"># 预测出y值</span></span><br><span class="line">y_predict = lr.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制拟合直线</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, y_predict, color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/dd98486098d41e5fdff5e64bfdea76a2.jpg" alt=""></p>
<p>可以看到这条直线明显无法很好的拟合这些样本数据。下面我们来看看如何使用多项式回归来拟合样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据上文中讲的，我们需要先给样本数据增加一个特征，既X的平方</span></span><br><span class="line">X2 = X**<span class="number">2</span></span><br><span class="line"><span class="comment"># 通过np.hstack方法将原始特征和平方后的特征转换为新的特征矩阵，既100行2列的特征矩阵</span></span><br><span class="line">X_new = np.hstack((X, X2))</span><br><span class="line"></span><br><span class="line">X_new.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后再对新的样本特征用线性回归训练模型</span></span><br><span class="line">lr2 = LinearRegression()</span><br><span class="line">lr2.fit(X_new, y)</span><br><span class="line">y_predict_new = lr2.predict(X_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制出拟合曲线</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(np.sort(x), y_predict_new[np.argsort(x)], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/978a962a971e25def4e4fa508ede6926.jpg" alt=""></p>
<p>这里在绘制拟合曲线时有点技巧，我们使用<code>np.sort(x)</code>对样本数据<code>x</code>数组从小到大排序，也就是从<code>x</code>数组中最小的元素开始绘制，然后使用<code>np.argsort(x)</code>求出数组<code>x</code>从小到大元素的索引，然后通过<code>y_predict_new[np.argsort(x)]</code>就可以求得数组<code>x</code>从小到大元素对应的<code>y</code>值了。否则拟合曲线绘制出来是不连续的，大家可以试一下只传入<code>x</code>和<code>y_predict_new</code>，看看绘制出的曲线是什么样的。</p>
<p>我们再来看看训练出的系数和截距：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr2.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">1.01051072</span>,  <span class="number">0.45036129</span>])</span><br><span class="line"></span><br><span class="line">lr2.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.2249327738612221</span></span><br></pre></td></tr></table></figure>
<p>可以看到，第一个特征的系数是1，第二个特征的系数，既对第一个特征做了平方的系数是0.45，截距是2.2。这和我们在之前构建的求<code>y</code>的方程是非常近似的，因为求<code>y</code>的方程里还加了正态分布的噪音，所以系数和截距是有点误差的，但是误差不大。那这就是多项式回归的原理和实现思路。我们在讲PCA的时候知道它的作用主要是给样本数据降维，而多项式回归是给样本数据升维，所以不同的算法，不同的样本数据是有降维，也有升维的，这点需要大家注意。</p>
<h2 id="Scikit_Learn_u4E2D_u7684_u591A_u9879_u5F0F_u56DE_u5F52"><a href="#Scikit_Learn_u4E2D_u7684_u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="Scikit Learn中的多项式回归"></a>Scikit Learn中的多项式回归</h2><p>在上一节中，我们知道多项式回归其实就是对样本数据进行预处理，然后用线性回归的算法进行模型训练，所以多项式回归的核心在于多样本数据做预处理。那么在Scikit Learn中针对多项式回归的类<code>PolynomialFeatures</code>是在<code>preprocessing</code>包中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入多项式处理的类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="comment"># degree参数表示将样本数据处理为至少包含有几次幂的特征</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line">poly.fit(X)</span><br><span class="line">X2 = poly.transform(X)</span><br></pre></td></tr></table></figure>
<p>在实例化<code>PolynomialFeatures</code>时构造函数需要的<code>degree</code>参数表示将样本数据处理为至少包含有几次幂特征的数据。比如<code>degree=2</code>，那么转换后的样本数据中的特征数量至少是2，其中一个是原始的特征，另一个是将原始特征平方后的特征。那么为什么说是至少有两个特征呢，我们来看看上面代码中转换后的<code>X2</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X2.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>我们看到转换后的样本数据<code>X2</code>是一个100行3列的矩阵，说明通过<code>PolynomialFeatures(degree=2)</code>转换后，我们期望的2个特征变成了3个，我们来看看前5行的样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X2[:<span class="number">5</span>, :]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[  <span class="number">1.00000000e+00</span>,  -<span class="number">2.67091119e+00</span>,   <span class="number">7.13376657e+00</span>],</span><br><span class="line">	   [  <span class="number">1.00000000e+00</span>,   <span class="number">9.26429770e-01</span>,   <span class="number">8.58272118e-01</span>],</span><br><span class="line">	   [  <span class="number">1.00000000e+00</span>,   <span class="number">2.19969933e+00</span>,   <span class="number">4.83867715e+00</span>],</span><br><span class="line">	   [  <span class="number">1.00000000e+00</span>,   <span class="number">3.13320069e-01</span>,   <span class="number">9.81694657e-02</span>],</span><br><span class="line">	   [  <span class="number">1.00000000e+00</span>,  -<span class="number">8.02234471e-02</span>,   <span class="number">6.43580146e-03</span>]])</span><br></pre></td></tr></table></figure>
<p>可以看到转换后的第一列相当于是对特征做了0次方，值都是1，第二列是原始特征，第三列是原始特征平方后的特征。然后我们再用线性回归对转换后的样本数据进行处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X2, y)</span><br><span class="line">y_predict = lr.predict(X2)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(np.sort(x), y_predict[np.argsort(x)], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/403ca0eb85c1d0774b62b84200372070.jpg" alt=""></p>
<p>以上就是在Scikit Learn中使用多项式回归的方式。下面我们再来仔细看一下<code>PolynomialFeatures</code>这个类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造简单的样本数据</span></span><br><span class="line">X_simple = np.arange(<span class="number">1</span>, <span class="number">11</span>).reshape(-<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">X_simple</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">	   [ <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">	   [ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">	   [ <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">	   [ <span class="number">9</span>, <span class="number">10</span>]])</span><br></pre></td></tr></table></figure>
<p>构造一个简单的样本数据，元素从1到10，一共5行2列，既是一个有两个特征的样本数据，然后使用<code>PolynomialFeatures</code>对这个样本数据进行处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line">poly.fit(X_simple)</span><br><span class="line">X2_simple = poly.transform(X_simple)</span><br><span class="line"></span><br><span class="line">X2_simple</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[   <span class="number">1.</span>,    <span class="number">1.</span>,    <span class="number">2.</span>,    <span class="number">1.</span>,    <span class="number">2.</span>,    <span class="number">4.</span>],</span><br><span class="line">	   [   <span class="number">1.</span>,    <span class="number">3.</span>,    <span class="number">4.</span>,    <span class="number">9.</span>,   <span class="number">12.</span>,   <span class="number">16.</span>],</span><br><span class="line">	   [   <span class="number">1.</span>,    <span class="number">5.</span>,    <span class="number">6.</span>,   <span class="number">25.</span>,   <span class="number">30.</span>,   <span class="number">36.</span>],</span><br><span class="line">	   [   <span class="number">1.</span>,    <span class="number">7.</span>,    <span class="number">8.</span>,   <span class="number">49.</span>,   <span class="number">56.</span>,   <span class="number">64.</span>],</span><br><span class="line">	   [   <span class="number">1.</span>,    <span class="number">9.</span>,   <span class="number">10.</span>,   <span class="number">81.</span>,   <span class="number">90.</span>,  <span class="number">100.</span>]])</span><br></pre></td></tr></table></figure>
<p>我们看到当原始样本数据有2个特征，然后希望转换后的样本数据里增加对每个原始特征进行平方的新特征，通常情况下我们的预期可能是转换后有样本数据有4个特征，2个原始特征，2个对应做了平方处理的新特征。但是实际的情况是转换后的样本数据是一个5行6列的矩阵，既有6个特征的样本数据。</p>
<p>我们来分析一下：</p>
<ul>
<li>第一列全部为1。</li>
<li>第二列和第三列是原始特征。</li>
<li>第四列和第六列分别是第二列原始特征和第三列原始特征的平方。</li>
<li>第五列是第二列原始特征和第三列原始特征的乘积。</li>
</ul>
<p>我们再来看一下如果将<code>degree</code>参数设置为3，转换后的样本数据会有几个特征：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">poly.fit(X_simple)</span><br><span class="line">X3_simple = poly.transform(X_simple)</span><br><span class="line"></span><br><span class="line">X3_simple.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">5</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>如果原始特征为$x_1$和$x_2$，那么<code>X3_simple</code>中10个特征分别为：1，$x_1$，$x_2$，$x_1^2$，$x_2^2$，$x_1x_2$，$x_1^3$，$x_2^3$，$x_1^2x_2$，$x_1x_2^2$。</p>
<p>可见当改变<code>PolynomialFeatures</code>的<code>degree</code>参数后，转换后样本数据的特征会成指数级的增长，它会尽可能的列出所有的多项式来丰富样本数据。</p>
<h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>现在我们知道，使用多项回归，首先就是对样本数据进行预处理，但是当<code>degree</code>值比较大的时候，我们的样本数据的特征分布会非常分散，比如1的1次方和100的100次方，这使得样本数据的特征值分布非常不均衡，这会使训练出的模型误差比较大。在第三篇笔记中，我们讲过数据归一化，就是解决这个问题的工具，所以在多项式回归中，数据归一化也是我们经常需要处理的步骤，再然后才是线性回归的处理。</p>
<p>那么整个多项式回归至少就有三个步骤，分别为对样本数据进行多项式的预处理、对样本数据进行归一化、对样本数据进行线性回归处理。对不同的样本数据，每次都要做这三次步骤还是比较繁琐的，所以Scikit Learn给我们提供了一个Pipeline的工具，既可以将若干步骤打包成一个对象，相当于制作一个流程模板，这样面对不同的样本数据，我们只需要处理一次既可，Pipeline内部帮我们处理了打包在里面的其他步骤，极大提高了效率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入Pipeline和其他需要打包进Pipeline的类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装Pipeline</span></span><br><span class="line">poly_pipeline = Pipeline([</span><br><span class="line">	(<span class="string">"poly"</span>, PolynomialFeatures(degree=<span class="number">2</span>)),</span><br><span class="line">	(<span class="string">"std_scalar"</span>, StandardScaler()),</span><br><span class="line">	(<span class="string">"lr"</span>, LinearRegression())</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>构建<code>Pipeline</code>的过程就是封装步骤的过程，<code>Pipeline</code>构造函数的参数需要传入一个数组，数组元素的类型为元组，元组的第一个元素为字符串，既标识这一步是做什么事的，第二个元素是处理对应步骤的类。从上面构造的<code>Pipeline</code>可以看出，是将三个步骤进行了打包，第一个步骤的对样本数据进行多项式的预处理，第二个步骤是对样本数据进行归一化，第三个步骤是对样本数据进行线性回归处理。封装好<code>Pipeline</code>后，使用的方式和机器学习的算法保持一致：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_pipeline.fit(X, y)</span><br><span class="line">y_predict_pipeline = poly_pipeline.predict(X)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(np.sort(x), y_predict_pipeline[np.argsort(x)], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/cb7c0ec8a7e776a768c1faac8be589ce.jpg" alt=""></p>
<p>可以看到最终的结果和我们之前将每一步单独分开得到的结果相同。在实际处理多步骤、复杂的机器学习问题时，<code>Pipeline</code>非常有用，是很好的化繁为简的工具。</p>
<h2 id="u6B20_u62DF_u5408_u548C_u8FC7_u62DF_u5408"><a href="#u6B20_u62DF_u5408_u548C_u8FC7_u62DF_u5408" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h2><p>在讲欠拟合和过拟合前，我们先来回顾一下线性回归的评测标准均方误差（MSE）：</p>
<p>$$ \frac 1 m \sum_{i=1}^m(y_{test}^{(i)} - \hat y_{test}^{(i)})^2$$</p>
<p>就是将原始目标值和通过模型计算出的目标值相减再平方，这就是其中一条样本数据的误差，将所有样本数据的误差相加再除以所有样本数据的数量，就得到了均方误差。均方误差值约小说明训练出的模型越好，也就是拟合的越好，反之亦然。其实欠拟合和过拟合从字面上也很好理解，前者表示拟合程度不够，通过模型计算出的目标值与原始目标值差距太大，后者表示拟合程度过高，以至于无法表示出样本数据的共性和真实情况。下面我们来看看示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先用线性回归进行拟合</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X, y)</span><br><span class="line">y_predict = lr.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mean_squared_error(y, y_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.6823097152963338</span></span><br></pre></td></tr></table></figure>
<p>我们构建和之前一样的样本数据，然后先使用线性回归训练模型，然后通过模型计算出的值的均方误差为2.68。然后再用多项式回归对同样的样本数据训练模型看看结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里用到上节讲到的Pipeline</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">PolynomialRegression</span><span class="params">(degree)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> Pipeline([</span><br><span class="line">		(<span class="string">"poly"</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">		(<span class="string">"std_scalar"</span>, StandardScaler()),</span><br><span class="line">		(<span class="string">"lr"</span>, LinearRegression())</span><br><span class="line">	])</span><br><span class="line">	</span><br><span class="line">poly2_reg = PolynomialRegression(degree=<span class="number">2</span>)</span><br><span class="line">poly2_reg.fit(X, y)</span><br><span class="line">y2_predict = poly2_reg.predict(X)</span><br><span class="line"></span><br><span class="line">mean_squared_error(y, y2_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.93160950356090177</span></span><br></pre></td></tr></table></figure>
<p>可以看到使用多项式回归方式训练出的模型计算出的目标值与原始目标值的均方误差只有0.93，说明拟合程度要远远好与用线性回归训练的模型。那么如果在对样本数据进行预处理时进一步增加多项式特征，MSE会有变化吗？我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly5_reg = PolynomialRegression(degree=<span class="number">5</span>)</span><br><span class="line">poly5_reg.fit(X, y)</span><br><span class="line">y5_predict = poly5_reg.predict(X)</span><br><span class="line"></span><br><span class="line">mean_squared_error(y, y5_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.91189515020672107</span></span><br></pre></td></tr></table></figure>
<p>从结果看到，当增加样本数据的多项式特征后，MSE是逐渐变小的，既多项式特征维度越高，MSE越小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly100_reg = PolynomialRegression(degree=<span class="number">100</span>)</span><br><span class="line">poly100_reg.fit(X, y)</span><br><span class="line">y100_predict = poly100_reg.predict(X)</span><br><span class="line"></span><br><span class="line">mean_squared_error(y, y100_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.4885145394655237</span></span><br></pre></td></tr></table></figure>
<p>当将<code>PolynomialRegression</code>的<code>degree</code>设为100时，MSE已经缩小到0.488了。那么是不是预处理后的样本数据中特征越多就越好呢？我们先将<code>degree=100</code>时的拟合曲线绘制出来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(np.sort(x), y100_predict[np.argsort(x)], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/229b85b2f96659f936afa74b8eb1ab5b.jpg" alt=""></p>
<p>可以看到这根曲线的弯曲程度已经非常复杂了，如果<code>degree</code>取足够高的值的话，是可以求出一条经过所有点的曲线的，既MSE为0，那条线基本就是将所有点连接起来的线，其复杂程度可想而知。那么就上面这根曲线来看，有很多断崖式的下降，对于这些局部下降的曲线来看其实拟合程度是非常不好的，虽然整体曲线呈上升趋势，但是有太多的局部拟合不好的地方，所以虽然MSE比较小，但是这条曲线并不是一条很好的拟合曲线，这就是过拟合。</p>
<p>另外，因为我们的样本数据构建的是一组随机数据，并且在计算$y$的方程中增加了噪音，所以上面曲线有很多地方并没有点，我们再构造一个连续的样本数据来看看经过<code>degree=100</code>后，绘制的曲线是什么样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制从-3到3连续的100个点，然后将其转换为100行1列的矩阵，既我们新的样本数据</span></span><br><span class="line">X_plot = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 使用我们之前degree参数为100时训练出的多项式回归模型来预测新的样本数据的目标值</span></span><br><span class="line">y_plot = poly100_reg.predict(X_plot)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制样本数据点和目标值，并将座标系设置在-3到3的横轴和-1到10的纵轴</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(X_plot[:, <span class="number">0</span>], y_plot, color=<span class="string">'r'</span>)</span><br><span class="line">plt.axis([-<span class="number">3</span>, <span class="number">3</span>, -<span class="number">1</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f0514530f8bef140e45a777b9a89e4d6.jpg" alt=""></p>
<p>我们看到，这时我们曲线波动的更加厉害，尤其曲线两端，所以很明显这条曲线不是一条很好的拟合曲线。</p>
<h2 id="u6A21_u578B_u6CDB_u5316"><a href="#u6A21_u578B_u6CDB_u5316" class="headerlink" title="模型泛化"></a>模型泛化</h2><p>其实在我们实际运用中，通常解决的都是过拟合的问题，这里我们再用数据来阐述一下过拟合问题。在第三篇笔记中，我们讲过对样本数据进行拆分，按一定比例分为训练数据和测试数据，目的在于验证训练出的模型预测训练数据以外的数据的准确性，继而评判模型的好坏。那么对于上面例子中，当<code>degree</code>的值为100时，MSE确实很小，但只是针对训练数据，也就是说这个模型在预测有限的、已知的训练数据而言，准确率很高。但是当这个模型在训练其他样本数据时准确率却很差，这就是典型的过拟合的特征，这也引出了一个概念，那就是泛化模型，这个概念很好理解，一个好的模型，应该具有普适性，对于数据特征类似的不同的样本数据应该都有比较好的预测准确率，说明泛化性比较好，上一节训练出的多项回归模型显然就是一个泛化性不好的模型。下面我们用数据再来验证一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入拆分样本数据为训练数据和测试数据的方法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line">X_train.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">75</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_test.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">25</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用degree为100的PolynomialFeatures通过训练数据训练模型</span></span><br><span class="line">poly100_reg.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 使用训练出的模型对训练特征数据预测训练目标值</span></span><br><span class="line">y100_predict_train = poly100_reg.predict(X_train)</span><br><span class="line"><span class="comment"># 求均方误差</span></span><br><span class="line">mean_squared_error(y_train, y100_predict_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.31894921748706134</span></span><br></pre></td></tr></table></figure>
<p>可以看到，对于范围有限、已知的训练数据<code>X_train</code>而言，训练出的模型预测结果还是比较好的，MSE只有0.32。那我们再来看看用同样的模型来预测测试数据，结果会怎样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y100_predict_test = poly100_reg.predict(X_test)</span><br><span class="line">mean_squared_error(y_test, y100_predict_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">223782770464.89853</span></span><br></pre></td></tr></table></figure>
<p>可以看到MSE都上亿了，足以可见这个模型的泛化性非常差。如果我们使用<code>degree=2</code>来预处理数据，看看训练出的模型的泛化性怎么样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly2_reg.fit(X_train, y_train)</span><br><span class="line">y2_predict_train = poly2_reg.predict(X_train)</span><br><span class="line">mean_squared_error(y_train, y2_predict_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.0237070125963763</span></span><br><span class="line"></span><br><span class="line">y2_predict_test = poly2_reg.predict(X_test)</span><br><span class="line">mean_squared_error(y_test, y2_predict_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.87695907380413585</span></span><br></pre></td></tr></table></figure>
<p>从上面的结果可以看到，<code>poly2_reg</code>的模型泛化性非常好，预测测试数据的MSE甚至比预测训练数据的MSE还要小。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在前面的笔记中，我们使用的样本数据都是具有一定线性关系的，我们使用简单线性回归或者多元线性回归来拟合这些数据。但是这种具有强假设的数据集在实际应用中是比较少的，大多数的样本数据都没有明显的线性关系，那么这一节主要来看看如何对非线性关系的样本数据进行处理并预测。</p>
<h2 id="u591A_u9879_u5F0F_u56DE_u5F52"><a href="#u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>多项式回归就是我们要使用的方法，其本质上还是基于线性回归的数学原理，只不过是对损失函数进行巧妙的改变使得原本我们用线性回归求得一条直线，变为求得一条曲线。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ba54b737f3717cf2fec01814463c4a4c.jpg" alt=""><br>在线性回归的问题中，对于这些数据（蓝色点），我们是要寻找一条直线，让这条直线尽可能的拟合这些数据，如果数据只有一个特征的话，我们的模型就是：</p>
<p>$$ y=ax + b $$</p>
<p>$x$就是数据的已知特征，$a$和$b$是我们的模型需要求出的参数。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f6a83a4e5b65ca84c17ff6d5468c63e2.jpg" alt=""></p>
<p>那如果我们的数据是像上图这种分布形态呢？显然红色那条直线无法拟合这些数据，而黄色那条曲线能更好的拟合这些数据。在样本数据特征数量不变的情况下，这条黄色曲线的模型其实就是一个一元二次方程：</p>
<p>$$y = ax^2+bx+c$$</p>
<p>我们抛开这个方程表示的坐标系形态来看，假设将$x^2$看作$x_1$，将$x$看作$x_2$，那么方式就可以写为：</p>
<p>$$y=ax_1+bx_2+c$$</p>
<p>这就变成了具有两个特征的样本数据的线性回归问题。现在大家就可以理解在本节开头所说的对损失函数进行巧妙改变的含义了吧。</p>
<p>所以对与多项式回归而言，就是需要我们为样本数据增加一个特征，使之可以用线性回归的原理更好的拟合样本数据，但是求出的是对于原始样本而言的非线性的曲线。</p>
<h3 id="u5B9E_u73B0_u591A_u9879_u5F0F_u56DE_u5F52"><a href="#u5B9E_u73B0_u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="实现多项式回归"></a>实现多项式回归</h3><p>这一节我们在Jupyter Notebook中看看如何实现多项式回归：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据，从-3到3的100个随机数</span></span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 转换为100行1列的矩阵，既样本数据只有一个特征</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求y的方程</span></span><br><span class="line">y = <span class="number">0.5</span> * x**<span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将样本数据绘制出来</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ccfd43e57d544b2be620023d01b3cf2a.jpg" alt=""></p>]]>
    
    </summary>
    
      <category term="Pipeline" scheme="http://www.devtalking.com/tags/Pipeline/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型泛化" scheme="http://www.devtalking.com/tags/%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96/"/>
    
      <category term="欠拟合" scheme="http://www.devtalking.com/tags/%E6%AC%A0%E6%8B%9F%E5%90%88/"/>
    
      <category term="过拟合" scheme="http://www.devtalking.com/tags/%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记七之主成分分析法（PCA）、人脸识别应用]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-7/"/>
    <id>http://www.devtalking.com//articles/machine-learning-7/</id>
    <published>2018-02-26T16:00:00.000Z</published>
    <updated>2018-08-27T02:30:55.555Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在机器学习的实际使用中，我们都希望有足够多的样本数据，并且有足够的特征来训练我们的模型，所以高维特征数据是经常会用到的，但是高维特征数据同样会带来一些问题：</p>
<ul>
<li>机器学习算法收敛速度下降。</li>
<li>特征难于分辨，很难第一时间认识某个特征代表的意义。</li>
<li>会产生冗余特征，增加模型训练难度，比如说某一品牌型号汽车的特征数据，有从中国采集的，也有从国外采集的，那么就会产生公里/小时和英里/小时这种特征，但其实这两个特征代表的意义是一样的。</li>
<li>无法通过可视化对训练数据进行综合分析。</li>
</ul>
<p>以上问题都是高维特征数据带来的普遍问题，所以将高维特征数据降为低维特征数据就很重要了。这篇笔记主要讲解机器学习中经常用到的降维算法PCA。</p>
<p>PCA是英文Principle Component Analysis的缩写，既主成分分析法。该算法能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。</p>
<h2 id="u7406_u89E3PCA_u7B97_u6CD5_u964D_u7EF4_u7684_u539F_u7406"><a href="#u7406_u89E3PCA_u7B97_u6CD5_u964D_u7EF4_u7684_u539F_u7406" class="headerlink" title="理解PCA算法降维的原理"></a>理解PCA算法降维的原理</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/96db7d87e7af97e05b630366cc02bcf1.jpg" alt=""></p>
<p>我们从二维降一维的场景来理解PCA降维的原理。上面的图示显示了一个二维的特征坐标，横坐标是特征1，纵座标是特征2。图中的五个点就表示了五条特征数据。我们先来想一下最简单粗暴的降维方式就是丢弃掉其中一个特征。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b51d7ec017736d4f47fb7e836c33f074.jpg" alt=""></p>
<p>如上图中显示，将特征2抛弃，这里大家先注意一下这五个点落在特征1轴上的间距。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/aa709dfbb796ada2b6b6dae9d0d43d64.jpg" alt=""></p>
<p>或者如上图所示抛弃特征1，大家再注意一下这五个点落在特征2轴上的间距。能很明显的发现，抛弃特征2，落在特征1轴上的五个点之间间距比较大，并且分布均匀。而抛弃特征1，落在特征2轴上的五个点之间间距大多都比较小，并且分布不均匀。</p>
<p>就这两种简单粗暴的降维方式而言，哪种更好一些呢？这里我们先来看看方差的概念，方差描述的是随机数据的离散程度，也就是离期望值（不严谨的说，期望值等同于均值）的距离。所以方差越大，数据的离散程度越高，约分散，离均值的距离越大。方差越小，数据的离散程度越小，约聚合，离均值的距离约小。那么大家可以想想作为机器学习算法训练的样本数据，每组特征应该尽可能的全，在该特征的合理范围内尽可能的广，这样才能更高的代表真实性，也就是每组特征数据的方差应该尽可能的大才好。所以就上面两种情况来看，抛弃特征2的降维方式更好一些。</p>
<a id="more"></a>
<p>但是简单粗暴的完全丢弃一个特征自然是不合理的，这极大的影响了训练出模型的正确性。所以，按照方差最大的理论，我们应该再找到一个特征向量，使样本数据落在这个特征向量后的方差最大，那么这个特征向量代表的特征就是我们需要的降维后的特征。这就是支撑PCA算法的理论之一。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/0525efddb7f229a87f9b6443cd26dd8a.jpg" alt=""></p>
<p>如上图所示，降维后的特征方差明显大于抛弃特征1或抛弃特征2后的方差。</p>
<p>我们在使用PCA之前首先需要对样本数据进行特征去均值化，也就是将每个特征的值减去该维特征的平均值。去均值化的目的是去除均值对数据变化的影响，也就是避免第一主成分收到均值的影响。</p>
<p>在第二篇笔记中，我们提到过方差，它的公式为：<br>$$ Var(X) = \frac{\sum_{i=1}^m(X_i-\overline X)^2} m $$</p>
<p>那么当数据去均值化后，公式中的$\overline X$就成了0，所以去均值后的方差为：<br>$$ Var(X) = \frac{\sum_{i=1}^mX_i^2} m $$</p>
<p>此时$X_i$就是降维后的特征，我们记为$X_p^{(i)}$，那么降维后特征值的方差公式为：<br>$$ Var(X_p) = \frac{\sum_{i=1}^m (X_p^{(i)})^2} m $$</p>
<p>因为在高维特征下，$X^{(i)}$和$X_p^{(i)}$都是向量，所以求方差时候应该对他们取模：<br>$$ Var(X_p) = \frac{\sum_{i=1}^m ||X_p^{(i)}||^2} m $$</p>
<p>所以我们就是要求上面这个公式的最大值。那么首先如何求得这个$X_p^{(i)}$<br>呢？我们来具体看一下：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/383a2da3f194b61833471f52e29ed96c.jpg" alt=""></p>
<p>如上图所示，蓝色向量是特征值原始维度的向量，$w$黑色向量就是我们要寻求的新维度的向量，绿色的点就是蓝色点在新维度上的投影点，红色向量的长度就是投影点的特征值。下面我们先来看看初高中数学中学过的知识。</p>
<p>首先向量的点乘有代数定义，也有几何定义。在几何定义下，两个向量的点乘为这两个向量的长度相乘，再乘以这两个向量夹角的余弦：<br>$$ \vec{a} \cdot \vec{b} = |\vec{a}| \, |\vec{b}| \cos \theta $$</p>
<p>所以从上图来看：<br>$$X^{(i)} \cdot w=||X^{(i)}|| \cdot ||w|| \cdot \cos \theta$$</p>
<p>因为我们需要的$w$只是方向，所以它可以是一个方向向量，既长度为1，所以上面的公式就变为：<br>$$X^{(i)} \cdot w=||X^{(i)}||\cdot \cos \theta$$</p>
<p>然后由三角函数可知，夹角的余弦等于邻边除以斜边。上图中$\theta$角的斜边就是$||X^{(i)}||$，邻边就是$||X_p^{(i)}||$，所以：<br>$$||X_p^{(i)}||=||X^{(i)}|| \cdot \cos \theta$$</p>
<p>此时我们就知道了，我们要求得的红色向量的长度，既：<br>$$||X_p^{(i)}||=X^{(i)} \cdot w$$</p>
<p>代入上面的方差公式为：<br>$$ Var(X_p) = \frac{\sum_{i=1}^m ||X^{(i)} \cdot w||^2} m $$</p>
<p>因为两个向量的点乘是一个标量，所以最终公式为：<br>$$ Var(X_p) = \frac{\sum_{i=1}^m (X^{(i)} \cdot w)^2} m $$</p>
<p>那么我们的目标就是求$w$向量，使得上面的这个公式最大。上一篇笔记我们讲了用梯度下降法求函数极小值，那么这里我们就要用到梯度上升法求函数的极大值。</p>
<h2 id="u4F7F_u7528_u68AF_u5EA6_u4E0A_u5347_u6CD5_u89E3_u51B3_u4E3B_u6210_u5206_u5206_u6790_u95EE_u9898"><a href="#u4F7F_u7528_u68AF_u5EA6_u4E0A_u5347_u6CD5_u89E3_u51B3_u4E3B_u6210_u5206_u5206_u6790_u95EE_u9898" class="headerlink" title="使用梯度上升法解决主成分分析问题"></a>使用梯度上升法解决主成分分析问题</h2><p>我们先将上面的公式展开（w是一个列向量）：<br>$$ Var(X_p) = \frac{\sum_{i=1}^m (X^{(i)} \cdot w)^2} m\<br>=\frac 1 m \sum_{i=1}^m(X_1^{(i)}w_1+X_2^{(i)}w_2+…+X_n^{(i)}w_n)^2$$</p>
<p>既然是梯度上升，那么第一步当然是求梯度了，这和梯度下降是一样的，结合第五篇笔记中的梯度推导可得，上面公式的梯度为：<br>$$ \nabla f(w) = \begin{bmatrix}<br> \frac {\partial L}{\partial w_1} \\<br> \frac {\partial L}{\partial w_2} \\<br> … \\<br> \frac {\partial L}{\partial w_n} \\<br>\end{bmatrix} =\frac 2 m\begin{bmatrix}<br>\sum_{i=1}^m 2(X^{(i)}w)X_1^{(i)} \\<br>\sum_{i=1}^m 2(X^{(i)}w)X_2^{(i)} \\<br>… \\<br>\sum_{i=1}^m 2(X^{(i)}w)X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<p>下面对上面的公式再进行向量化，这里我再推导一遍，首先我们将$X^{(i)}w$看成是一个1行m列的行矩阵中的元素：<br>$$\begin{bmatrix}X^{(1)}w&amp; X^{(2)}w&amp; X^{(3)}w&amp; … &amp;X^{(m)}w)\end{bmatrix}$$</p>
<p>然后将它和一个m行n列的矩阵相乘：<br>$$\begin{bmatrix}X^{(1)}w&amp; X^{(2)}w&amp; X^{(3)}w&amp; … &amp;X^{(m)}w)\end{bmatrix} \\<br>\cdot \begin{bmatrix}<br>X_1^{(1)}&amp; X_2^{(1)}&amp; X_3^{(1)}&amp; … &amp;X_n^{(1)} \\<br>X_1^{(2)}&amp; X_2^{(2)}&amp; X_3^{(2)}&amp; … &amp;X_n^{(2)} \\<br>X_1^{(3)}&amp; X_2^{(3)}&amp; X_3^{(3)}&amp; … &amp;X_n^{(3)} \\<br>… \\<br>X_1^{(m)}&amp; X_2^{(m)}&amp; X_3^{(m)}&amp; … &amp;X_n^{(m)} \\<br> \end{bmatrix}$$</p>
<p>因为X是一个m行n列矩阵，w是一个n行1列的矩阵，所以X乘w是一个m行1列的矩阵，上面我们将其转换为了1行m列的矩阵，所以上面的公式简写为$(Xw)^TX$，相乘后的结果是一个1行n列的矩阵：<br>$$(Xw)^TX=\begin{bmatrix}\sum_{i=1}^m(X^{(i)}w)X_1^{(i)}&amp; \sum_{i=1}^m(X^{(i)}w)X_2^{(i)}&amp; …&amp; \sum_{i=1}^m(X^{(i)}w)X_n^{(i)} \end{bmatrix}$$</p>
<p>那我们的梯度是一个n行1列的矩阵，所以将上面的矩阵再做转置：<br>$$((Xw)^TX)^T=X^T(Xw)$$</p>
<p>所以最终主成分分析的梯度向量化后为：<br>$$\nabla f = \frac 2 m X^T(Xw)$$</p>
<h3 id="u4EE3_u7801_u5B9E_u73B0PCA_u68AF_u5EA6_u4E0A_u5347"><a href="#u4EE3_u7801_u5B9E_u73B0PCA_u68AF_u5EA6_u4E0A_u5347" class="headerlink" title="代码实现PCA梯度上升"></a>代码实现PCA梯度上升</h3><p>首先我们构建样本数据，其中有100条样本数据，每条样本数据中有2个特征：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line"><span class="comment"># 构建一个100行2列的矩阵</span></span><br><span class="line">X = np.empty((<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 第一个特征为0到100的随机分布</span></span><br><span class="line">X[:, <span class="number">0</span>] = np.random.uniform(<span class="number">0.</span>, <span class="number">100.</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 第二个特征和第一个特征有一定线性关系，并且增加了0到10的正态分布的噪音</span></span><br><span class="line">X[:, <span class="number">1</span>] = X[:, <span class="number">0</span>] * <span class="number">0.75</span> + <span class="number">3.</span> + np.random.normal(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 将特征绘制出来</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/e01e7bb0d5bbec7f7d1b4207d4378acb.jpg" alt=""></p>
<p>接下来根据上文中讲到呃，下一步要对样本数据的每一个特征进行均值归0操作，也就是每一列的元素减去这一列所有元素的均值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demean</span><span class="params">(X)</span>:</span></span><br><span class="line">	<span class="comment"># 对矩阵X在横轴方向上求均值，既求每一列的均值</span></span><br><span class="line">	<span class="keyword">return</span> X - np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均值归0化后的样本数据    </span></span><br><span class="line">X_demean = demean(X)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_demean[:, <span class="number">0</span>], X_demean[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f32ca27448cba21bd998cc340a256122.jpg" alt=""></p>
<p>可以看到均值归0化后，样本数据的分布形态是没有变化的，但是坐标轴往右上移动了，既0点现在在样本数据的中间。下面来定义目标函数和梯度函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 目标函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(X, w)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> np.sum(X.dot(w)**<span class="number">2</span>) / len(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度函数    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">df</span><span class="params">(X, w)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> X.T.dot(X.dot(x)) / <span class="number">2</span> * len(X)</span><br></pre></td></tr></table></figure>
<p>在上面的公式推导的过程中提到过，我们期望的向量$w$是一个单位向量，所以在代码实现计算的时候需要将传入的初始向量$w$和计算后的新$w$向量都转换为单位向量（向量的单位向量为该向量除以该向量的模）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">direction</span><span class="params">(w)</span>:</span></span><br><span class="line">	<span class="comment"># np.linalg.norm(w)为求向量的模</span></span><br><span class="line">	<span class="keyword">return</span> w / np.linalg.norm(w)</span><br></pre></td></tr></table></figure>
<p>接下来的梯度上升计算和梯度下降计算基本是大同小异的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数传入样本矩阵，初始向量，步长，查找循环次数，两次方差的最小差值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_ascent</span><span class="params">(X, initial_w, eta, n_iters=<span class="number">1e4</span>, different=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">	<span class="comment"># 将向量转换为单位向量</span></span><br><span class="line">	w = direction(initial_w)</span><br><span class="line">	i_iters = <span class="number">0</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">while</span> i_iters &lt; n_iters:</span><br><span class="line">		gradient = df(X, w)</span><br><span class="line">		last_w = w</span><br><span class="line">		w = w + eta * gradient</span><br><span class="line">		w = direction(w)</span><br><span class="line">		<span class="keyword">if</span>(abs(f(X, w) - f(X, last_w)) &lt; different):</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">			</span><br><span class="line">		i_iters += <span class="number">1</span></span><br><span class="line">		</span><br><span class="line">	<span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<p>在使用我们定义的方法前，有两点需要注意的是，一点是在PCA中，初始向量$w$不能为0，因为方差公式里$w$在分子，所以如果为0了，那么方差始终为0，所以每次我们随机初始化一个不为0的向量。另外一点是在PCA中我们不对样本数据做归一化或标准化处理，因为一旦做了归一化处理，样本数据的方差就变成了1，自然求不到最大方差了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化随机向量</span></span><br><span class="line">initial_w = np.random.random(X.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 设置步长</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"><span class="comment"># 梯度上升</span></span><br><span class="line">w = gradient_ascent(X_demean, initial_w, eta)</span><br><span class="line"><span class="comment"># 绘制w向量</span></span><br><span class="line">plt.scatter(X_demean[:, <span class="number">0</span>], X_demean[:, <span class="number">1</span>])</span><br><span class="line">plt.plot([<span class="number">0</span>, w[<span class="number">0</span>]*<span class="number">30</span>], [<span class="number">0</span>, w[<span class="number">1</span>]*<span class="number">30</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/69c624ca541ae98c6bc93edaa8acebe7.jpg" alt=""></p>
<p>这样我们就求出了样本数据的第一个降维到的向量，我们称为样本的第一主成分。</p>
<h3 id="u6C42_u6570_u636E_u7684_u5176_u4ED6_u4E3B_u6210_u5206"><a href="#u6C42_u6570_u636E_u7684_u5176_u4ED6_u4E3B_u6210_u5206" class="headerlink" title="求数据的其他主成分"></a>求数据的其他主成分</h3><p>在上节中我们使用的样本数据是在二维空间的，求出的第一主成分其实可以看作是将坐标轴旋转后横轴或纵轴，我们降维后的数据其实是新的坐标轴上某个轴的分量，那么另外一个分量自然是降维到垂直于第一主成分向量的向量，既新坐标轴的另外一个轴。该向量是第一主成分向量的正交线。那么下面我们来看一下第一主成分的正交线如何求：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/2f981772960cac65e9bd92c0d2730498.jpg" alt=""></p>
<p>从上图可以看到，$X^{‘(i)}$就是第一主成分向量$w$的正交线，由向量的加减法可得：</p>
<p>$$X^{‘(i)}=X^{(i)}-X_p^{(i)}$$</p>
<p>因为上文推导过：</p>
<p>$$X_p^{(i)}=||X_p^{(i)}|| w$$</p>
<p>$$||X_p^{(i)}||=X^{(i)} \cdot w$$</p>
<p>所以得：</p>
<p>$$X^{‘(i)}=X^{(i)}-(X^{(i)} \cdot w)w $$</p>
<p>这就相当于原始样本数据减去投影到第一主成分上的数据，我们对去掉第一主成分数据的样本数据再求第一主成分数据，那么就相当于在求原始样本数据的第二主成分了，以此类推就可以求得样本数据的n个主成分。</p>
<p>下面我们来用代码实现，首先我们算出样本数据在新坐标轴上的另一个分量，根据上面推导出的公式可得：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X2 = X - X.dot(w).reshape(-<span class="number">1</span>, <span class="number">1</span>) * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制X2及第一主成分向量w</span></span><br><span class="line">plt.scatter(X2[:, <span class="number">0</span>], X2[:, <span class="number">1</span>])</span><br><span class="line">plt.plot([<span class="number">0</span>, w[<span class="number">0</span>]*<span class="number">30</span>], [<span class="number">0</span>, w[<span class="number">1</span>]*<span class="number">30</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/cdc274aa080990d956209bd63ce14494.jpg" alt=""></p>
<p>首先可以看到，当样本数据去掉第一主成分数据后，另一个分量的数据其实就是在正交于第一主成分向量的轴上，所以所有点都在一条直线上。</p>
<p>然后将之前的<code>gradient_ascent</code>方法改个名称，因为它就是求第一主成分的方法，所以改名为<code>first_component</code>，然后求出<code>X2</code>的第一主成分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first_component</span><span class="params">(X, initial_w, eta, n_iters=<span class="number">1e4</span>, different=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">	w = direction(initial_w)</span><br><span class="line">	i_iters = <span class="number">0</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">while</span> i_iters &lt; n_iters:</span><br><span class="line">		gradient = df(X, w)</span><br><span class="line">		last_w = w</span><br><span class="line">		w = w + eta * gradient</span><br><span class="line">		w = direction(w)</span><br><span class="line">		<span class="keyword">if</span>(abs(f(X, w) - f(X, last_w)) &lt; different):</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">			</span><br><span class="line">		i_iters += <span class="number">1</span></span><br><span class="line">		</span><br><span class="line">	<span class="keyword">return</span> w</span><br><span class="line">	</span><br><span class="line">w2 = first_component(X2, initial_w, eta)</span><br></pre></td></tr></table></figure>
<p>由向量的正交定理知道，垂直的向量点乘结果为0，所以我们来验证一下<code>w</code>和<code>w2</code>之间的点乘结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w.dot(w2)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3.2666630511712924e-10</span></span><br></pre></td></tr></table></figure>
<p>可以看到结果基于趋近于0。</p>
<p>下面我们封装一个计算n个主成分的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first_n_component</span><span class="params">(n, X, eta=<span class="number">0.01</span>, n_iters=<span class="number">1e4</span>, different=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 拷贝原始样本数据</span></span><br><span class="line">	X_pca = X.copy()</span><br><span class="line">	<span class="comment"># 对样本数据进行均值归一化</span></span><br><span class="line">	X_pca = demean(X_pca)</span><br><span class="line">	<span class="comment"># 存储结果数组</span></span><br><span class="line">	res = []</span><br><span class="line">	<span class="comment"># 希望计算几个主成分就循环几次</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">		<span class="comment"># 每次随机一个初始向量</span></span><br><span class="line">		initial_w = np.random.random(X_pca.shape[<span class="number">1</span>])</span><br><span class="line">		<span class="comment"># 通过获取主成分方法计算出主成分向量</span></span><br><span class="line">		w = first_component(X_pca, initial_w, eta)</span><br><span class="line">		res.append(w)</span><br><span class="line">		</span><br><span class="line">		<span class="comment"># 每次从原始样本数据中除去主成分数据</span></span><br><span class="line">		X_pca = X_pca - X_pca.dot(w).reshape(-<span class="number">1</span>, <span class="number">1</span>) * w</span><br><span class="line">		</span><br><span class="line">	<span class="keyword">return</span> res</span><br><span class="line">	</span><br><span class="line">first_n_component(<span class="number">2</span>, X)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[array([ <span class="number">0.77899988</span>,  <span class="number">0.62702407</span>]), array([-<span class="number">0.62702407</span>,  <span class="number">0.77899988</span>])]</span><br></pre></td></tr></table></figure>
<h2 id="u9AD8_u7EF4_u6570_u636E_u5411_u4F4E_u7EF4_u6570_u636E_u6620_u5C04"><a href="#u9AD8_u7EF4_u6570_u636E_u5411_u4F4E_u7EF4_u6570_u636E_u6620_u5C04" class="headerlink" title="高维数据向低维数据映射"></a>高维数据向低维数据映射</h2><p>我们再来回顾一下PCA降维的基本原理，首先要做的事情就是对样本数据寻找另外一个坐标系，这个坐标系中的每一个轴依次可以表达样本数据的重要程度，既主成分。我们取出前k个主成分，然后就可以将所有的样本数据映射到这k个轴上，既获得了一个低维的数据信息。</p>
<p>$$X=\begin{bmatrix}<br>X_1^{(1)}&amp; X_2^{(1)}&amp; … &amp;X_n^{(1)} \\<br>X_1^{(2)}&amp; X_2^{(2)}&amp; … &amp;X_n^{(2)} \\<br>… \\<br>X_1^{(m)}&amp; X_2^{(m)}&amp; … &amp;X_n^{(m)}<br> \end{bmatrix}$$</p>
<p>上面的$X$是样本数据，该样本数据有m行，n个特征，既是一个n维的样本数据。</p>
<p>$$W_k=\begin{bmatrix}<br>W_1^{(1)}&amp; W_2^{(1)}&amp; … &amp;W_n^{(1)} \\<br>W_1^{(2)}&amp; W_2^{(2)}&amp; … &amp;W_n^{(2)} \\<br>… \\<br>W_1^{(k)}&amp; W_2^{(k)}&amp; … &amp;W_n^{(k)} \\<br>\end{bmatrix}$$</p>
<p>假设上面的$W$是样本数据$X$的主成分向量矩阵，每一行代表一个主成分向量，一共有k个主成分向量，每个主成分向量上有n个值。</p>
<p>我们已经推导出了求映射后的向量的大小，也就是每一行样本数据映射到该主成分上的大小为：</p>
<p>$$||X_p^{(i)}||=X^{(i)} w$$</p>
<p>那如果将一行有n个特征的样本数据分别映射到k个主成分上，既得到有k个值的新向量，既降维后的，有k个特征的新样本数据。所以我们需要的就是$X$矩阵的第一行和$W$矩阵的每一行对应元素相乘然后再相加，$X$矩阵的第二行和$W$矩阵的每一行对应元素相乘然后再相加，以此类推就可以求出降维后的，m行k列的新矩阵数据：</p>
<p>$$X^{‘}=XW_k^T$$</p>
<p>$X^{‘}$就是降维后的数据，既然可以降维，那么我们也可从数学的角度将降维后的数据还原回去。$X^{‘}$是m行k列的矩阵，$W_k$是k行n列的矩阵，所以$X^{‘} W_k$就是还原后的ｍ行ｎ列的原矩阵。那为什么说是从数学角度来说呢，因为毕竟已经从高维降到了低维，那势必会有丢失的数据信息，所以还原回去的数据也不可能和原始数据一样的。</p>
<h3 id="u5728PyCharm_u4E2D_u5C01_u88C5PCA"><a href="#u5728PyCharm_u4E2D_u5C01_u88C5PCA" class="headerlink" title="在PyCharm中封装PCA"></a>在PyCharm中封装PCA</h3><p>我们在<code>myML</code>中新建一个类<code>PCA</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCA</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 初始化PCA</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_components)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> n_components &gt;= <span class="number">1</span>, <span class="string">"至少要有一个主成分"</span></span><br><span class="line">		self.n_components = n_components</span><br><span class="line">		self.component_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 训练主成分矩阵</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, eta=<span class="number">0.01</span>, n_iters=<span class="number">1e4</span>)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> self.n_components &lt;= X.shape[<span class="number">1</span>], <span class="string">"主成分数要小于等于样本数据的特征数"</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 均值归一化</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">demean</span><span class="params">(X)</span>:</span></span><br><span class="line">			<span class="keyword">return</span> X - np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 目标函数</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w, X)</span>:</span></span><br><span class="line">			<span class="keyword">return</span> np.sum((X.dot(w)**<span class="number">2</span>)) / len(X)</span><br><span class="line">		<span class="comment"># 梯度</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">df</span><span class="params">(w, X)</span>:</span></span><br><span class="line">			<span class="keyword">return</span> (X.T.dot(X.dot(w)) * <span class="number">2</span>) / len(X)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 求单位向量</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">direction</span><span class="params">(w)</span>:</span></span><br><span class="line">			<span class="keyword">return</span> w / np.linalg.norm(w)</span><br><span class="line"></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">first_component</span><span class="params">(X, initial_w, eta=<span class="number">0.01</span>, n_iters=<span class="number">1e4</span>, different=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">			<span class="comment"># 转换初始向量为单位向量，既只表明方向</span></span><br><span class="line">			w = direction(initial_w)</span><br><span class="line">			cur_iters = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">while</span> cur_iters &lt; n_iters:</span><br><span class="line">				<span class="comment"># 求出梯度</span></span><br><span class="line">				gradient = df(w, X)</span><br><span class="line">				<span class="comment"># 记录上一个方向向量</span></span><br><span class="line">				last_w = w</span><br><span class="line">				<span class="comment"># 通过梯度上升求下一个方向向量</span></span><br><span class="line">				w = w + eta * gradient</span><br><span class="line">				<span class="comment"># 将新求出的方向向量单位向量化</span></span><br><span class="line">				w = direction(w)</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span>(abs(f(w, X) - f(last_w, X)) &lt; different):</span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">				cur_iters += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 对样本数据的特征数据均值归一化</span></span><br><span class="line">		X_pca = demean(X)</span><br><span class="line">		<span class="comment"># 构建一个空的主成分矩阵，大小和样本数据保持一致</span></span><br><span class="line">		self.component_ = np.empty(shape=(self.n_components, X.shape[<span class="number">1</span>]))</span><br><span class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_components):</span><br><span class="line">			<span class="comment"># 随机生成一个初始向量</span></span><br><span class="line">			initial_w = np.random.random(X_pca.shape[<span class="number">1</span>])</span><br><span class="line">			<span class="comment"># 求第一主成分</span></span><br><span class="line">			w = first_component(X_pca, initial_w, eta, n_iters)</span><br><span class="line">			<span class="comment"># 存储主成分</span></span><br><span class="line">			self.component_[i, :] = w</span><br><span class="line"></span><br><span class="line">			X_pca = X_pca - X_pca.dot(w).reshape(-<span class="number">1</span>, <span class="number">1</span>) * w</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据主成分矩阵降维样本数据</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X.shape[<span class="number">1</span>] == self.component_.shape[<span class="number">1</span>], <span class="string">"样本数据的列数，既特征数要等于主成分矩阵的列数"</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> X.dot(self.component_.T)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据主成分矩阵还原样本数据</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">inverse_transform</span><span class="params">(self, X_pca)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X_pca.shape[<span class="number">1</span>] == self.component_.shape[<span class="number">0</span>], <span class="string">"降维后的样本数据特征数要等于主成分矩阵的行数"</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> X_pca.dot(self.component_)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">"PCA(n_components=%d)"</span> % self.n_components</span><br></pre></td></tr></table></figure>
<h3 id="u5728Jupyter_Notebook_u4E2D_u4F7F_u7528_u5C01_u88C5_u7684PCA"><a href="#u5728Jupyter_Notebook_u4E2D_u4F7F_u7528_u5C01_u88C5_u7684PCA" class="headerlink" title="在Jupyter Notebook中使用封装的PCA"></a>在Jupyter Notebook中使用封装的PCA</h3><p>首先构建样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line"><span class="comment"># 构建一个100行，2列的空矩阵</span></span><br><span class="line">X = np.empty((<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 第一个特征为0到100的随机分布</span></span><br><span class="line">X[:, <span class="number">0</span>] = np.random.uniform(<span class="number">0.</span>, <span class="number">100.</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 第二个特征和第一个特征有一定线性关系，并且增加了0到10的正态分布的噪音</span></span><br><span class="line">X[:, <span class="number">1</span>] = X[:, <span class="number">0</span>] * <span class="number">0.75</span> + <span class="number">3.</span> + np.random.normal(<span class="number">0</span>, <span class="number">10.</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 将特征绘制出来</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/76a44ae0d563f682721b8b41e409a5a0.jpg" alt=""></p>
<p>然后导入我们封装好的PCA类，训练主成分并根据主成分对样本数据降维：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入我们封装的PCA类</span></span><br><span class="line"><span class="keyword">from</span> myML.PCA <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 初始化PCA类，给定只训练一个主成分</span></span><br><span class="line">pca = PCA(n_components=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 训练主成分矩阵</span></span><br><span class="line">pca.fit(X)</span><br><span class="line"><span class="comment"># 查看主成分</span></span><br><span class="line">pca.component_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.7756218</span> ,  <span class="number">0.63119792</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据主成分对样本数据进行降维</span></span><br><span class="line">X_reduction = pca.transform(X)</span><br><span class="line"><span class="comment"># 降维后的数据为一个100行1列的矩阵</span></span><br><span class="line">X_reduction.shape</span><br></pre></td></tr></table></figure>
<p>看到我们非常简单地就把一个二维特征的样本数据根据主成分映射为了一维特征的样本数据。同时我们还可以将其恢复二维特征数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 恢复样本数据维度</span></span><br><span class="line">X_restore = pca.inverse_transform(X_reduction)</span><br><span class="line">X_restore.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始样本数据和从低维恢复后的样本数据绘制出来进行对比</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X_restore[:, <span class="number">0</span>], X_restore[:, <span class="number">1</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/7cfee67ba1e669c885ba225245afe591.jpg" alt=""></p>
<p>在前面提到过，从高维降到低维就已经有部分信息丢失了，所以再恢复回去后势必不会和原始数据一样。从上图中可以看到，恢复后的二维特征数据其实是在一条直线上，而这条直线其实就是原始样本数据的主成分。</p>
<h2 id="Scikit_Learn_u4E2D_u7684PCA"><a href="#Scikit_Learn_u4E2D_u7684PCA" class="headerlink" title="Scikit Learn中的PCA"></a>Scikit Learn中的PCA</h2><p>这一节我们来看看Scikit Learn中封装的PCA如何使用，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 构建样本数据</span></span><br><span class="line"><span class="comment"># 构建一个100行，2列的空矩阵</span></span><br><span class="line">X = np.empty((<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 第一个特征为0到100的随机分布</span></span><br><span class="line">X[:, <span class="number">0</span>] = np.random.uniform(<span class="number">0.</span>, <span class="number">100.</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 第二个特征和第一个特征有一定线性关系，并且增加了0到10的正态分布的噪音</span></span><br><span class="line">X[:, <span class="number">1</span>] = X[:, <span class="number">0</span>] * <span class="number">0.75</span> + <span class="number">3.</span> + np.random.normal(<span class="number">0</span>, <span class="number">10.</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入Scikit Learn中的PCA</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">1</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line">X_reduction = pca.transform(X)</span><br><span class="line">X_reduction.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_restore = pca.inverse_transform(X_reduction)</span><br><span class="line">X_restore.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">plt.scatter(X_restore[:, <span class="number">0</span>], X_restore[:, <span class="number">1</span>], color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c746273510d90c4aa181718a637a4b43.jpg" alt=""></p>
<p>可以看到，我们封装PCA类时，使用标准的机器学习算法的模式，所以在使用Scikit Learn提供的PCA时，几乎是一样的。</p>
<h3 id="u4F7F_u7528_u771F_u5B9E_u7684_u6570_u636E"><a href="#u4F7F_u7528_u771F_u5B9E_u7684_u6570_u636E" class="headerlink" title="使用真实的数据"></a>使用真实的数据</h3><p>这一节我们使用真实的数据来体会一下PCA的威力。我们使用Scikit Learn中提供的手写数字数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">1797</span>, <span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到，Scikit Learn提供的手写数据是一个64维特征的样本数据，一共有1797行，也就是一个1797行，64列的矩阵。</p>
<p>我们先用KNN分类算法来计算这个样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先将样本数据拆分为训练数据集和测试数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line">X_train.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">1347</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后使用Scikit Learn的KNN算法训练分类模型，</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 在训练的时候计一下时</span></span><br><span class="line">%time knn_clf.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">4.61</span> ms, sys: <span class="number">3.25</span> ms, total: <span class="number">7.86</span> ms</span><br><span class="line">Wall time: <span class="number">38.1</span> ms</span><br><span class="line">KNeighborsClassifier(algorithm=<span class="string">'auto'</span>, leaf_size=<span class="number">30</span>, metric=<span class="string">'minkowski'</span>, metric_params=<span class="keyword">None</span>, n_jobs=<span class="number">1</span>, n_neighbors=<span class="number">5</span>, p=<span class="number">2</span>, weights=<span class="string">'uniform'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看分类准确率</span></span><br><span class="line">knn_clf.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 0.98666666666666669</span></span><br></pre></td></tr></table></figure>
<p>从上面的代码可以看出，使用KNN算法对样本数据进行训练时通过网格搜索的邻近点为5个，使用了明可夫斯基距离，但是<code>p</code>是2，所以其实还是欧拉距离，并且没有使用距离权重。训练后的分类准确率为98.7%，在我的电脑上耗时38.1毫秒。</p>
<p>下面我们先简单粗暴的将这个64维特征的样本数据降至2维特征数据，然后再用KNN算法训练一下看看情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用Scikit Learn提供的PCA</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 设置主成分为2</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X_train)</span><br><span class="line"><span class="comment"># 将训练数据和测试数据分别降至2维</span></span><br><span class="line">X_train_reduction = pca.transform(X_train)</span><br><span class="line">X_test_reduction = pca.transform(X_test)</span><br><span class="line">X_train_reduction.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">1347</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对降维后的数据进行KNN分类训练</span></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">%time knn_clf.fit(X_train_reduction, y_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">1.75</span> ms, sys: <span class="number">1.02</span> ms, total: <span class="number">2.77</span> ms</span><br><span class="line">Wall time: <span class="number">1.77</span> ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看分类准确率</span></span><br><span class="line">knn_clf.score(X_test_reduction, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.60666666666666669</span></span><br></pre></td></tr></table></figure>
<p>从上面的代码和结果可以看到，首先使用KNN算法训练的耗时从64维时的38.1毫秒降至了1.77毫秒，所以这验证了PCA降维的其中的减少计算时间的作用。但是当我们查看分类准确率的时候发现非常低，所以说明我们降维度的降的太低，丢失了太多的数据信息。那么PCA中的超参数<code>n_components</code>应该如何取呢？其实Scikit Learn的PCA提供了一个方法就是可以计算出每个主成分代表的方差比率：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca.explained_variance_ratio_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.14566817</span>,  <span class="number">0.13735469</span>])</span><br></pre></td></tr></table></figure>
<p>比如通过<code>explained_variance_ratio_</code>我们可以知道通过PCA分析出的手写数据的前两个主成分的方差比率为14.6%和13.7%，加起来既标识降维后的数据只能保留了原始样本数据38.3%的数据信息，所以自然分类准确率很差了。那么如果我们使用PCA将64维数据计算出64个主成分，然后看看每个主成分的方差比率是如何变化的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=X_train.shape[<span class="number">1</span>])</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">pca.explained_variance_ratio_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([  <span class="number">1.45668166e-01</span>,   <span class="number">1.37354688e-01</span>,   <span class="number">1.17777287e-01</span>,</span><br><span class="line">		 <span class="number">8.49968861e-02</span>,   <span class="number">5.86018996e-02</span>,   <span class="number">5.11542945e-02</span>,</span><br><span class="line">		 <span class="number">4.26605279e-02</span>,   <span class="number">3.60119663e-02</span>,   <span class="number">3.41105814e-02</span>,</span><br><span class="line">		 <span class="number">3.05407804e-02</span>,   <span class="number">2.42337671e-02</span>,   <span class="number">2.28700570e-02</span>,</span><br><span class="line">		 <span class="number">1.80304649e-02</span>,   <span class="number">1.79346003e-02</span>,   <span class="number">1.45798298e-02</span>,</span><br><span class="line">		 <span class="number">1.42044841e-02</span>,   <span class="number">1.29961033e-02</span>,   <span class="number">1.26617002e-02</span>,</span><br><span class="line">		 <span class="number">1.01728635e-02</span>,   <span class="number">9.09314698e-03</span>,   <span class="number">8.85220461e-03</span>,</span><br><span class="line">		 <span class="number">7.73828332e-03</span>,   <span class="number">7.60516219e-03</span>,   <span class="number">7.11864860e-03</span>,</span><br><span class="line">		 <span class="number">6.85977267e-03</span>,   <span class="number">5.76411920e-03</span>,   <span class="number">5.71688020e-03</span>,</span><br><span class="line">		 <span class="number">5.08255707e-03</span>,   <span class="number">4.89020776e-03</span>,   <span class="number">4.34888085e-03</span>,</span><br><span class="line">		 <span class="number">3.72917505e-03</span>,   <span class="number">3.57755036e-03</span>,   <span class="number">3.26989470e-03</span>,</span><br><span class="line">		 <span class="number">3.14917937e-03</span>,   <span class="number">3.09269839e-03</span>,   <span class="number">2.87619649e-03</span>,</span><br><span class="line">		 <span class="number">2.50362666e-03</span>,   <span class="number">2.25417403e-03</span>,   <span class="number">2.20030857e-03</span>,</span><br><span class="line">		 <span class="number">1.98028746e-03</span>,   <span class="number">1.88195578e-03</span>,   <span class="number">1.52769283e-03</span>,</span><br><span class="line">		 <span class="number">1.42823692e-03</span>,   <span class="number">1.38003340e-03</span>,   <span class="number">1.17572392e-03</span>,</span><br><span class="line">		 <span class="number">1.07377463e-03</span>,   <span class="number">9.55152460e-04</span>,   <span class="number">9.00017642e-04</span>,</span><br><span class="line">		 <span class="number">5.79162563e-04</span>,   <span class="number">3.82793717e-04</span>,   <span class="number">2.38328586e-04</span>,</span><br><span class="line">		 <span class="number">8.40132221e-05</span>,   <span class="number">5.60545588e-05</span>,   <span class="number">5.48538930e-05</span>,</span><br><span class="line">		 <span class="number">1.08077650e-05</span>,   <span class="number">4.01354717e-06</span>,   <span class="number">1.23186515e-06</span>,</span><br><span class="line">		 <span class="number">1.05783059e-06</span>,   <span class="number">6.06659094e-07</span>,   <span class="number">5.86686040e-07</span>,</span><br><span class="line">		 <span class="number">7.44075955e-34</span>,   <span class="number">7.44075955e-34</span>,   <span class="number">7.44075955e-34</span>,</span><br><span class="line">		 <span class="number">7.15189459e-34</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到上面64个方差比率是从大到小排序的，而且后面的方差率越来越小，所以从这个数据我们其实已经可以计算出一个合适的主成分个数，使其方差比率之和达到一个极大值。我们将维数和方差率绘制出来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 横轴为维数，纵轴为每个维度对应的之前所有维度的方差率之和</span></span><br><span class="line">plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> range(X_train.shape[<span class="number">1</span>])], [np.sum(pca.explained_variance_ratio_[:i+<span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X_train.shape[<span class="number">1</span>])])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c71f5704cc9ae4e706af966ad3800b46.jpg" alt=""><br>从图中可以看到，当维度数在30左右的时候，方差率上升已经很平缓了，所以从这个图中都可以目测出，我们将64维特征的样本数据降维至30维左右是比较合适的。</p>
<p>其实Scikit Learn的PCA提供了一个参数，就是我们期望达到的总方差率为多少，然后会帮我们自动计算出主成分个数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 传入一个0到1的小数，既总方差率</span></span><br><span class="line">pca = PCA(<span class="number">0.95</span>)</span><br><span class="line">pca.fit(X_train)</span><br><span class="line"><span class="comment"># 查看主成分数</span></span><br><span class="line">pca.n_components_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">28</span></span><br></pre></td></tr></table></figure>
<p>可以看到，我们期望的总方差率为95%时的主成分数为28。然后我们再使用KNN来训练一下降为28维特征的样本数据，看看准确率和时间为多少：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对原始训练数据和原始测试数据进行降维</span></span><br><span class="line">X_train_reduction = pca.transform(X_train)</span><br><span class="line">X_test_reduction = pca.transform(X_test)</span><br><span class="line"></span><br><span class="line">X_train_reduction.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">1347</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对28维特征的数据进行KNN训练</span></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">%time knn_clf.fit(X_train_reduction, y_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">2.25</span> ms, sys: <span class="number">1.54</span> ms, total: <span class="number">3.79</span> ms</span><br><span class="line">Wall time: <span class="number">2.44</span> ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看分类准确率</span></span><br><span class="line">knn_clf.score(X_test_reduction, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.97999999999999998</span></span><br></pre></td></tr></table></figure>
<p>从上面代码的结果可以看到，在使用KNN训练28维特征的数据时耗时也只有2.44毫秒，但是分类准确率达到了98%。比64维特征的数据耗时减少了15倍，但是准确率只减少了0.6%。这个性价比是非常之高的，这就是PCA的威力所在。</p>
<h2 id="u4F7F_u7528PCA_u5BF9_u6570_u636E_u8FDB_u884C_u964D_u566A"><a href="#u4F7F_u7528PCA_u5BF9_u6570_u636E_u8FDB_u884C_u964D_u566A" class="headerlink" title="使用PCA对数据进行降噪"></a>使用PCA对数据进行降噪</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/cc4530b8fc647677b84b0396c36c6322.jpg" alt=""></p>
<p>这张图是之前小节中生成的，其中蓝色的点是我们构建的原始样本数据，红色的点是经过PCA降维后，又通过PCA还原维度的样本数据。对这个图我们可以这样理解，原始样本数据的分布都在这条红色点组成的直线上下，而导致这些蓝色点没有落在红色直线上的原因就是因为数据有噪音，所以通过PCA降维其实是去除了数据的噪音。但这些噪音也是也是数据信息，所以通常我们说使用PCA对数据进行降维后会丢失一些数据信息。</p>
<p>下面我们通过一个实际的例子来看一下PCA的降噪过程。我们依然使用手写识别的例子，我们手写识别的样本数据中加一些噪音，然后看PCA如何去除这些噪音：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建噪音数据，将原始样本数据增加上方差为4的噪音数据</span></span><br><span class="line">nosiy_digits = X + np.random.normal(<span class="number">0</span>, <span class="number">4</span>, size=X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制手写图片</span></span><br><span class="line"><span class="comment"># 取手写数字为0的前十条样本数据</span></span><br><span class="line">example_digits = nosiy_digits[y == <span class="number">0</span>, :][:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 累加循环取手写数字为1到9的前十条样本数据</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">	num_digits = nosiy_digits[y == num, :][:<span class="number">10</span>]</span><br><span class="line">	example_digits = np.vstack([example_digits, num_digits])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前example_digits的结构为，10条手写数字0，10条手写数字1，...， 10条手写数字9。一共100行。 </span></span><br><span class="line">example_digits.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个显示手写数字图片的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(data)</span>:</span></span><br><span class="line">	<span class="comment"># 构建尺寸为10 X 10的画图区域，既每行10个图片，一共10行</span></span><br><span class="line">	plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">	<span class="comment"># 遍历加了噪音的手写数字数据</span></span><br><span class="line">	<span class="keyword">for</span> index, imageData <span class="keyword">in</span> enumerate(data):</span><br><span class="line">		<span class="comment"># 图片分布为10行10列，正好100个图片，第三个参数是每张图片的位置</span></span><br><span class="line">		plt.subplot(<span class="number">10</span>, <span class="number">10</span>, index + <span class="number">1</span>)</span><br><span class="line">		<span class="comment"># 将有64个元素的数组转换为8 X 8的矩阵，既8 X 8个像素的图片</span></span><br><span class="line">		image = np.reshape(imageData, (<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">		<span class="comment"># 图片的ColorMap用灰度显示，为了能更好的观察</span></span><br><span class="line">		plt.imshow(image, cmap = plt.cm.gray)</span><br><span class="line">	</span><br><span class="line">	plt.show()</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 显示加了噪音的手写数字</span></span><br><span class="line">plot_digits(example_digits)</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/aed715f032be849f502a12923db1f792.jpg" alt=""></p>
<p>从图中可以看出，手写数字的识别度非常差。下面我们使用PCA对<code>example_digits</code>进行降噪处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只保留50%的主成分，因为我们之前添加的噪音数据方差比较大，也就是认为噪音比较多，既有50%的噪音</span></span><br><span class="line">pca = PCA(<span class="number">0.5</span>)</span><br><span class="line">pca.fit(nosiy_digits)</span><br><span class="line">pca.n_components_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">12</span></span><br><span class="line"></span><br><span class="line">example_components = pca.transform(example_digits)</span><br><span class="line">example_components.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<p>当我们只保留50%主成分的时候，特征维度从64维降到了12维。然后我们再将其还原为64维，既过滤掉了噪音：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_components_inverse = pca.inverse_transform(example_components)</span><br><span class="line"></span><br><span class="line">example_components_inverse.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">100</span>, <span class="number">64</span>)</span><br><span class="line"><span class="comment"># 显示降噪后的手写数字图片</span></span><br><span class="line">plot_digits(example_components_inverse)</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8a165d8efeeeef3e03b63029fff2d00f.jpg" alt=""></p>
<p>可以看到，此时图片中的手写数字的识别度有明显的提升。这就是PCA降噪的作用。</p>
<h2 id="u4EBA_u8138_u8BC6_u522B"><a href="#u4EBA_u8138_u8BC6_u522B" class="headerlink" title="人脸识别"></a>人脸识别</h2><p>PCA有一个典型的实际应用，就是人脸识别。我们这一节就来简单看看PCA在人脸识别中的应用。</p>
<p>首先我们还是先从PCA的原理来说，PCA就将高维数据降至相对的低维数据，但是这些低维的数据却能反应了原始高维数据的绝大多数主要特征。那么由PCA训练出的这些主成分其实就代表了原始数据的主要特征。那么如果原始高维数据是一张张不同的人脸数据时，那么由PCA训练出的主成分其实就是这一张张人脸的主要特征，称为特征脸。好比，爷爷，爸爸，儿子三个人长的很像，其实是这三张人脸有共同的特征脸。下面我们就使用Scikit Learn中提供的人脸数据来看看特征脸的应用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入人脸数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入人脸数据集(这里需要等一阵，因为需要下载数据)</span></span><br><span class="line">faces = fetch_lfw_people()</span><br><span class="line">faces.data.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">13233</span>, <span class="number">2914</span>)</span><br><span class="line"></span><br><span class="line">faces.images.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">13233</span>, <span class="number">62</span>, <span class="number">47</span>)</span><br></pre></td></tr></table></figure>
<p>如果网络不好的朋友，可以去<a href="http://vis-www.cs.umass.edu/lfw/" target="_blank" rel="external">这里</a>手动下载<strong>All images aligned with funneling</strong>数据集，然后在使用<code>fetch_lfw_people()</code>时需要增加<code>data_home</code>这个参数，指定你存放数据集的目录，比如<code>fetch_lfw_people(data_home=&#39;/XXX/XXX&#39;)</code>然后执行，Scikit Learn就会去你指定的目录下解压<code>lfw_funneled</code>压缩包，抓取数据。从上面代码结果可以看到，这个人脸的数据集一共有13233张人脸图片，每张人脸图片有2914个特征，其实也就是由2914个不同值的像素组成，通过<code>faces.images.shape</code>我们知道这13233张图片的大小是62乘以47像素，相乘的值正好就是2914。</p>
<p>因为这个数据集里人脸的分布并不均匀，所以我们在用之前先将其进行随机处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求一个随机索引</span></span><br><span class="line">random_indexs = np.random.permutation(len(faces.data))</span><br><span class="line">X = faces.data[random_indexs]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取前36张脸进行展示</span></span><br><span class="line">example_faces = X[:<span class="number">36</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个显示人脸图片的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_faces</span><span class="params">(data)</span>:</span></span><br><span class="line">	<span class="comment"># 构建尺寸为12 X 12的画图区域，既每行12个图片，一共12行</span></span><br><span class="line">	plt.figure(figsize=(<span class="number">12</span>, <span class="number">12</span>))</span><br><span class="line">	<span class="comment"># 遍历加了噪音的手写数字数据</span></span><br><span class="line">	<span class="keyword">for</span> index, imageData <span class="keyword">in</span> enumerate(data):</span><br><span class="line">		<span class="comment"># 图片分布为6行6列，正好36张图片，第三个参数是每张图片的位置</span></span><br><span class="line">		plt.subplot(<span class="number">6</span>, <span class="number">6</span>, index + <span class="number">1</span>)</span><br><span class="line">		<span class="comment"># 将有2914个元素的数组转换为62 X 47的矩阵，既62 X 47个像素的图片</span></span><br><span class="line">		image = np.reshape(imageData, (<span class="number">62</span>, <span class="number">47</span>))</span><br><span class="line">		plt.imshow(image, cmap = <span class="string">'bone'</span>)</span><br><span class="line">	</span><br><span class="line">	plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/fe52449ca67f5e31a9635a263892f689.jpg" alt=""></p>
<p>然后我们通过PCA求出这个13233条样本数据，2914维特征数据集的所有主成分。所有主成分要等于小于样本数据的特征数量。下面来求这个样本数据集的最大维度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># 求出样本数据的最大主成分数</span></span><br><span class="line">pca = PCA(svd_solver=<span class="string">'randomized'</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line"></span><br><span class="line">pca.components_.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">2914</span>, <span class="number">2914</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看前36个主成分的方差比率</span></span><br><span class="line">pca.explained_variance_ratio_[:<span class="number">36</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.23333138</span>,  <span class="number">0.10693729</span>,  <span class="number">0.08233336</span>,  <span class="number">0.07026902</span>,  <span class="number">0.03252576</span>,</span><br><span class="line">		<span class="number">0.03012665</span>,  <span class="number">0.0210468</span> ,  <span class="number">0.01823955</span>,  <span class="number">0.01764975</span>,  <span class="number">0.0158582</span> ,</span><br><span class="line">		<span class="number">0.01480933</span>,  <span class="number">0.01337274</span>,  <span class="number">0.01252589</span>,  <span class="number">0.01161669</span>,  <span class="number">0.01068777</span>,</span><br><span class="line">		<span class="number">0.00960774</span>,  <span class="number">0.00873549</span>,  <span class="number">0.00775879</span>,  <span class="number">0.00771184</span>,  <span class="number">0.00693537</span>,</span><br><span class="line">		<span class="number">0.00674195</span>,  <span class="number">0.00626633</span>,  <span class="number">0.00590671</span>,  <span class="number">0.00531572</span>,  <span class="number">0.00512598</span>,</span><br><span class="line">		<span class="number">0.00500887</span>,  <span class="number">0.0048442</span> ,  <span class="number">0.00466389</span>,  <span class="number">0.00435357</span>,  <span class="number">0.0041365</span> ,</span><br><span class="line">		<span class="number">0.00390986</span>,  <span class="number">0.00387675</span>,  <span class="number">0.00366964</span>,  <span class="number">0.0035524</span> ,  <span class="number">0.00342629</span>,</span><br><span class="line">		<span class="number">0.00328901</span>], dtype=float32)</span><br><span class="line">		</span><br><span class="line"><span class="comment"># 绘制前36个主成分，既最能描述人脸特征的成分</span></span><br><span class="line">plot_faces(pca.components_[:<span class="number">36</span>, :])</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/410019ede0a6862e2963c3f98992fe2c.jpg" alt=""></p>
<p>从代码结果可以看出，主成分的方差比率由大到小排序，第一个主成分能表示23.33%的人脸特征，从图上也能看到，第一个图显示了一个人脸的整个脸庞，第二个图显示了人脸的五官位置等。图中的这些脸就是特征脸，可以清晰的看到特征脸依次显示人脸特征的变化过程。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在机器学习的实际使用中，我们都希望有足够多的样本数据，并且有足够的特征来训练我们的模型，所以高维特征数据是经常会用到的，但是高维特征数据同样会带来一些问题：</p>
<ul>
<li>机器学习算法收敛速度下降。</li>
<li>特征难于分辨，很难第一时间认识某个特征代表的意义。</li>
<li>会产生冗余特征，增加模型训练难度，比如说某一品牌型号汽车的特征数据，有从中国采集的，也有从国外采集的，那么就会产生公里/小时和英里/小时这种特征，但其实这两个特征代表的意义是一样的。</li>
<li>无法通过可视化对训练数据进行综合分析。</li>
</ul>
<p>以上问题都是高维特征数据带来的普遍问题，所以将高维特征数据降为低维特征数据就很重要了。这篇笔记主要讲解机器学习中经常用到的降维算法PCA。</p>
<p>PCA是英文Principle Component Analysis的缩写，既主成分分析法。该算法能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。</p>
<h2 id="u7406_u89E3PCA_u7B97_u6CD5_u964D_u7EF4_u7684_u539F_u7406"><a href="#u7406_u89E3PCA_u7B97_u6CD5_u964D_u7EF4_u7684_u539F_u7406" class="headerlink" title="理解PCA算法降维的原理"></a>理解PCA算法降维的原理</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/96db7d87e7af97e05b630366cc02bcf1.jpg" alt=""></p>
<p>我们从二维降一维的场景来理解PCA降维的原理。上面的图示显示了一个二维的特征坐标，横坐标是特征1，纵座标是特征2。图中的五个点就表示了五条特征数据。我们先来想一下最简单粗暴的降维方式就是丢弃掉其中一个特征。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b51d7ec017736d4f47fb7e836c33f074.jpg" alt=""></p>
<p>如上图中显示，将特征2抛弃，这里大家先注意一下这五个点落在特征1轴上的间距。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/aa709dfbb796ada2b6b6dae9d0d43d64.jpg" alt=""></p>
<p>或者如上图所示抛弃特征1，大家再注意一下这五个点落在特征2轴上的间距。能很明显的发现，抛弃特征2，落在特征1轴上的五个点之间间距比较大，并且分布均匀。而抛弃特征1，落在特征2轴上的五个点之间间距大多都比较小，并且分布不均匀。</p>
<p>就这两种简单粗暴的降维方式而言，哪种更好一些呢？这里我们先来看看方差的概念，方差描述的是随机数据的离散程度，也就是离期望值（不严谨的说，期望值等同于均值）的距离。所以方差越大，数据的离散程度越高，约分散，离均值的距离越大。方差越小，数据的离散程度越小，约聚合，离均值的距离约小。那么大家可以想想作为机器学习算法训练的样本数据，每组特征应该尽可能的全，在该特征的合理范围内尽可能的广，这样才能更高的代表真实性，也就是每组特征数据的方差应该尽可能的大才好。所以就上面两种情况来看，抛弃特征2的降维方式更好一些。</p>]]>
    
    </summary>
    
      <category term="PCA" scheme="http://www.devtalking.com/tags/PCA/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记六之梯度下降、优化梯度公式、随机梯度下降]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-6/"/>
    <id>http://www.devtalking.com//articles/machine-learning-6/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-08-27T02:30:37.235Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记主要介绍梯度下降法，梯度下降不是机器学习专属的算法，它是一种基于搜索的最优化方法，也就是通过不断的搜索然后找到损失函数的最小值。像上篇笔记中使用正规方程解实现多元线性回归，基于$X_b\theta$这个模型我们可以推导出$\theta$的数学解，但是很多模型是推导不出数学解的，所以就需要梯度下降法来搜索出最优解。</p>
<h2 id="u68AF_u5EA6_u4E0B_u964D_u6CD5_u6982_u5FF5"><a href="#u68AF_u5EA6_u4E0B_u964D_u6CD5_u6982_u5FF5" class="headerlink" title="梯度下降法概念"></a>梯度下降法概念</h2><p>我们来看看在二维坐标里的一个曲线方程：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8b3433626c22d7f31a236492458a58e0.jpg" alt=""></p>
<p>纵坐标表示损失函数L的值，横坐标表示系数$\theta$。每一个$\theta$的值都会对应一个损失函数L的值，我们希望损失函数收敛，既找到一个$\theta$的值，使损失函数L的值最小。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/7d33b5dbd153029338804e42063e7dae.jpg" alt=""></p>
<p>在曲线上定义一点A，对应一个$\theta$值，一个损失函数L的值，要判断点A是否是损失函数L的最小值，既求该点的导数，在第一篇笔记中我解释过，点A的导数就是直线M的斜率，直线M是点A的切线，所以导数描述了一个函数在某一点附近的变化率，并且导数大于零时，函数在区间内单调递增，导数小于零时函数在区间内单调递减。所以$\frac {d L}{d \theta}$表示损失函数L增大的变化率，$-\frac {d L}{d \theta}$表示损失函数L减小的变化率。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/79181d4455f5a40c4a652a41a743bc7b.jpg" alt=""></p>
<p>再在曲线上定义一点B，在点A的下方，B点的$\theta$值就是A点的$\theta$值加上让损失函数L递减的变化率$-\eta \frac {d L}{d \theta}$，$\eta$称为步长，既B点在$-\frac {d L}{d \theta}$变化率的基础下移动了多少距离，在机器学习中$\eta$这个值也称为学习率。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/12d864def3bdafa3f3ecda683f2950e4.jpg" alt=""></p>
<p>同理还可以再求得点C，然后看是否是损失函数的L的最小值。所以梯度下降法就是基于损失函数在某一点的变化率$-\frac {d L}{d \theta}$，以及寻找下一个点的步长$\eta$，不停的找到下一个点，然后判断该点处的损失函数值是否为最小值的过程。$-\eta \frac {d L}{d \theta}$就称为梯度。</p>
<p>在第一篇笔记中将极值的时候提到过，当曲线或者曲面很复杂时，会有多个驻点，既局部极值，所以如果运行一次梯度下降法寻找损失函数极值的话很有可能找到的只是局部极小值点。所以在实际运用中我们需要多次运行算法，随机化初始点，然后进行比较找到真正的全局极小值点，所以初始点的位置是梯度下降法的一个超参数。</p>
<p>不过在线性回归的场景中，我们的损失函数$\sum_{i=1}^m(y^{(i)}-\hat y^{(i)})^2$是有唯一极小值的，所以暂时不需要多次执行算法搜寻全局极值。</p>
<a id="more"></a>
<h3 id="u68AF_u5EA6_u4E0B_u964D_u7684_u6B65_u957F"><a href="#u68AF_u5EA6_u4E0B_u964D_u7684_u6B65_u957F" class="headerlink" title="梯度下降的步长"></a>梯度下降的步长</h3><p>步长在梯度下降法中非常重要，这里着重说一下。</p>
<ul>
<li>$\eta$在机器学习中称为学习率（Learning rate）。</li>
<li>$\eta$的取值影响获得最优解的速度。想象一下如果$\eta$过小，那么寻找的点就会变得很多，收敛速度下降。如果$\eta$过大可能会不断错过最优解，同样影响收敛速度。</li>
<li>$\eta$取值不合适时甚至得不到最优解。比如$\eta$值过大会造成损失函数值越来越大。</li>
<li>$\eta$也是梯度下降法的一个超参数。可能会有搜索最佳$\eta$的过程。</li>
</ul>
<h2 id="u5B9E_u73B0_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#u5B9E_u73B0_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="实现梯度下降法"></a>实现梯度下降法</h2><p>我们在Jupyter Notebook中来看看如何实现梯度下降法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟theta值，及确定一个损失函数</span></span><br><span class="line">plot_x = np.linspace(-<span class="number">1</span>, <span class="number">6</span>, <span class="number">100</span>)</span><br><span class="line">plot_y = (plot_x-<span class="number">2.5</span>)**<span class="number">2</span>-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将该曲线绘制出来</span></span><br><span class="line">plt.plot(plot_x, plot_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b7dc17cb71005d1d1ef768462ffbc042.jpg" alt=""></p>
<p>下面我们来定义变化率$\frac {d L}{d \theta}$和损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 变化率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dL</span><span class="params">(theta)</span>:</span></span><br><span class="line"><span class="comment"># 对(theta-2.5)**2-1 求导，然后返回</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">2</span>*(theta-<span class="number">2.5</span>)</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(theta)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> (theta-<span class="number">2.5</span>)**<span class="number">2</span>-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>然后来看看实现梯度下降的过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 步长默认设为0.1</span></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 初始点默认设为0.0</span></span><br><span class="line">theta = <span class="number">0.0</span></span><br><span class="line"><span class="comment"># 找到的新的theta值与上一个theta值的差值最小边界</span></span><br><span class="line">difference = <span class="number">1e-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">	<span class="comment"># 求出梯度，既变化率</span></span><br><span class="line">	gradient = dL(theta)</span><br><span class="line">	<span class="comment"># 记录上一个theta值</span></span><br><span class="line">	last_theta = theta</span><br><span class="line">	<span class="comment"># 寻找下一个theta值，既当前的theta值加上乘以步长的使损失函数递减的变化率</span></span><br><span class="line">	theta = theta - eta*gradient</span><br><span class="line">	</span><br><span class="line">	<span class="comment"># 当新找到的theta值与上一个theta值之差小于1e-8时，表明此时变化率已经趋于0了，新的theta值可以使损失函数达到极小值 </span></span><br><span class="line">	<span class="keyword">if</span>(abs(L(theta) - L(last_theta)) &lt; difference):</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">		</span><br><span class="line">print(theta)</span><br><span class="line">print(L(theta))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.499891109642585</span></span><br><span class="line">-<span class="number">0.99999998814289</span></span><br></pre></td></tr></table></figure>
<p>得到的$\theta$值为2.5，损失函数的极小值为-1，代入方程$(\theta - 2.5)^2-1$可验证我们的求解是正确的。</p>
<h3 id="Theta_u7684_u53D8_u5316"><a href="#Theta_u7684_u53D8_u5316" class="headerlink" title="Theta的变化"></a>Theta的变化</h3><p>我们将每次$\theta$的值记下来，然后描绘出来，看看是如何变化的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">difference = <span class="number">1e-8</span></span><br><span class="line">theta = <span class="number">0.0</span></span><br><span class="line">theta_history = [theta]</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">	gradient = dL(theta)</span><br><span class="line">	last_theta = theta</span><br><span class="line">	theta = theta - eta * gradient</span><br><span class="line">	theta_history.append(theta)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(abs(L(theta) - L(last_theta)) &lt; different):</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">		</span><br><span class="line">len(theta_history)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">46</span></span><br><span class="line"></span><br><span class="line">plt.plot(plot_x, L(plot_x))</span><br><span class="line">plt.plot(np.array(theta_history), L(np.array(theta_history)), color=<span class="string">'r'</span>, marker=<span class="string">'+'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c6c1c996f18edb6d77c85d7969ae8833.jpg" alt=""></p>
<p>从上面的示例代码中看到，一共经历了45次的查找得到了让函数达到最小值的$\theta$。并且看到一开始因为曲线比较陡，所以梯度比较大，两个点之间的间隔比较大，到曲线底部的时候因为曲线开始平缓，梯度逐渐变小，每个点之间的间隔就越来越小。</p>
<p>我们将步长调大一些，看看$\theta$的查找轨迹是怎样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.8</span></span><br><span class="line">difference = <span class="number">1e-8</span></span><br><span class="line">theta = <span class="number">0.0</span></span><br><span class="line">theta_history = [theta]</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">	gradient = dL(theta)</span><br><span class="line">	last_theta = theta</span><br><span class="line">	theta = theta - eta * gradient</span><br><span class="line">	theta_history.append(theta)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(abs(L(theta) - L(last_theta)) &lt; difference):</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">		</span><br><span class="line">plt.plot(plot_x, L(plot_x))</span><br><span class="line">plt.plot(np.array(theta_history), L(np.array(theta_history)), color=<span class="string">"r"</span>, marker=<span class="string">"+"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8ad7c792de47d281e7aed54b72d216fd.jpg" alt=""></p>
<p>从图中可以看到，第一次查找的时候就越过了极值点，找到了曲线另一侧的点，不过好在损失函数的值还是递减的状态所以最终还是找到了极值点。</p>
<p>我们将步长再调大点，看会发生什么情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(theta)</span>:</span></span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		<span class="keyword">return</span> (theta-<span class="number">2.5</span>)**<span class="number">2</span>-<span class="number">1</span></span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> float(<span class="string">'inf'</span>)</span><br><span class="line">		</span><br><span class="line">eta = <span class="number">1.1</span></span><br><span class="line">difference = <span class="number">1e-8</span></span><br><span class="line">theta = <span class="number">0.0</span></span><br><span class="line">theta_history = [theta]</span><br><span class="line">n_iters = <span class="number">100</span></span><br><span class="line">i_iter = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i_iter &lt; n_iters:</span><br><span class="line">	gradient = dL(theta)</span><br><span class="line">	last_theta = theta</span><br><span class="line">	theta = theta - eta * gradient</span><br><span class="line">	theta_history.append(theta)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(abs(L(theta) - L(last_theta)) &lt; difference):</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">		</span><br><span class="line">	i_iter += <span class="number">1</span></span><br><span class="line">plt.plot(plot_x, L(plot_x))</span><br><span class="line">plt.plot(np.array(theta_history), L(np.array(theta_history)), color=<span class="string">"r"</span>, marker=<span class="string">"+"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/ce4c29ffc270f8d0a8db5b18b6690e09.jpg" alt=""></p>
<p>从图中看到每次找到的$\theta$会使损失函数越来越大，根本无法找到极小值。所以步长的确定非常重要，不过一般情况下，我们将步长设为0.01是比较保险的做法，但是如果想使算法在各方面达到最优，那么还是需要先对步长这个超参数进行严谨的确定。</p>
<h3 id="u6C42_u5BFC_u6982_u5FF5_u590D_u4E60"><a href="#u6C42_u5BFC_u6982_u5FF5_u590D_u4E60" class="headerlink" title="求导概念复习"></a>求导概念复习</h3><p>在看如何使用梯度下降解决线性回归问题前，我们先来复习一下求导的知识。</p>
<h4 id="u4EE3_u6570_u51FD_u6570_u7684_u6C42_u5BFC"><a href="#u4EE3_u6570_u51FD_u6570_u7684_u6C42_u5BFC" class="headerlink" title="代数函数的求导"></a>代数函数的求导</h4><ul>
<li>$\frac {dM}{dx}=0$</li>
<li>$\frac {dx^n}{dx}=nx^{n-1}$</li>
<li>$\frac {d|x|}{dx}=\frac {x}{|x|}$</li>
</ul>
<h4 id="u4E00_u822C_u6C42_u5BFC_u5B9A_u5219"><a href="#u4E00_u822C_u6C42_u5BFC_u5B9A_u5219" class="headerlink" title="一般求导定则"></a>一般求导定则</h4><h5 id="u7EBF_u6027_u5B9A_u5219"><a href="#u7EBF_u6027_u5B9A_u5219" class="headerlink" title="线性定则"></a>线性定则</h5><p>$$ \frac {d(Mf)}{dx}=M\frac {df}{dx} $$</p>
<p>$$ \frac {d(f\pm g)}{dx}=\frac {df}{dx} \pm \frac {dg}{dx} $$</p>
<h5 id="u4E58_u6CD5_u5B9A_u5219"><a href="#u4E58_u6CD5_u5B9A_u5219" class="headerlink" title="乘法定则"></a>乘法定则</h5><p>$$\frac {dfg}{dx}=\frac {df}{dx}g + f\frac {dg}{dx}$$</p>
<h5 id="u9664_u6CD5_u5B9A_u5219"><a href="#u9664_u6CD5_u5B9A_u5219" class="headerlink" title="除法定则"></a>除法定则</h5><p>$$ \frac {d \frac f g} {dx} = \frac {\frac {df} {dx} g - f \frac {dg} {dx}} {g^2} \ \ \ (g\neq 0) $$</p>
<h5 id="u5012_u6570_u5B9A_u5219"><a href="#u5012_u6570_u5B9A_u5219" class="headerlink" title="倒数定则"></a>倒数定则</h5><p>$$ \frac {d \frac 1 g} {dx} = \frac {-\frac {dg} {dx}} {g^2} \ \ \ (g\neq 0) $$</p>
<h5 id="u590D_u5408_u51FD_u6570_u6C42_u5BFC_u6CD5_u5219"><a href="#u590D_u5408_u51FD_u6570_u6C42_u5BFC_u6CD5_u5219" class="headerlink" title="复合函数求导法则"></a>复合函数求导法则</h5><p>$$ \frac {df[g(x)]} {dx} = \frac {df(g)} {dg} \frac {dg} {dx} = f’[g(x)]g’(x) $$</p>
<h2 id="u7EBF_u6027_u56DE_u5F52_u4E2D_u4F7F_u7528_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#u7EBF_u6027_u56DE_u5F52_u4E2D_u4F7F_u7528_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="线性回归中使用梯度下降法"></a>线性回归中使用梯度下降法</h2><p>上一节通过一维场景解释了梯度下降法，这一节在高维的场景中看看如何使用梯度下降法解决线性回归的问题。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/1fe2d00a45041a7b2857819315291f8f.jpg" alt=""></p>
<p>在线性回归的问题中注意一下三个方面：</p>
<ul>
<li>首先损失函数$\sum_{i=1}^m(y^{(i)}-\hat y^{(i)})^2$是明确的，前面用正规方程解实现的时候已经推导过。</li>
<li>其次系数$\theta$不是一维的了，而是多维的，既$\theta$是一个向量。</li>
<li>最后损失函数不是对一维系数$\theta$求全导，而是对$\theta$向量求偏导，也就是对向量里的每一个$\theta$求导数，记为$\nabla L$，既梯度。</li>
</ul>
<p>在上一篇笔记中我们推导过$\hat y^{(i)}$的函数：</p>
<p>$$ \hat y^{(i)} = \theta _0+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+…+\theta_nX_n^{(i)} $$</p>
<p>所以代入损失函数后就得：</p>
<p>$$\sum_{i=1}^m (y^{(i)}-\theta_0-\theta_1X_1^{(i)}-\theta_2X_2^{(i)}-…-\theta_nX_n^{(i)})^2$$</p>
<p>在讲正规方程解时我们推导过，上面的函数可以转换为：</p>
<p>$$\sum_{i=1}^m(y^{(i)}-X_b^{(i)} \theta)^2$$</p>
<p>我们的目标就是让上面的函数尽可能的小，根据之前讲过的概念，我们知道损失函数的梯度就是对$\theta$向量中的每个$\theta$求导，并且在上篇笔记中我们知道$\theta$向量是一个列向量，根据复合函数求导定则，所以损失函数L的梯度为：</p>
<p>$$ \nabla L(\theta) = \begin{bmatrix}<br> \frac {\partial L} {\partial \theta_0} \\<br> \frac {\partial L} {\partial \theta_1} \\<br> \frac {\partial L} {\partial \theta_2} \\<br> … \\<br> \frac {\partial L}{\partial \theta_n} \\<br> \end{bmatrix} = \begin{bmatrix}<br> \sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-1) \\<br> \sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-X_1^{(i)}) \\<br> \sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-X_2^{(i)}) \\<br> … \\<br> \sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-X_n^{(i)}) \\<br> \end{bmatrix} $$</p>
<p>将2提到外面，然后将后面的负号移进前面的括号里得：</p>
<p>$$\begin{bmatrix}<br>\sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-1) \\<br>\sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-X_1^{(i)}) \\<br>\sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-X_2^{(i)}) \\<br>… \\<br>\sum_{i=1}^m 2(y^{(i)}-X_b^{(i)}\theta)(-X_n^{(i)}) \\<br> \end{bmatrix}=2\begin{bmatrix}<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)}) \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_1^{(i)} \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_2^{(i)} \\<br>… \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<p>可以发现上面公式中每个元素都经过了m求和，这样带来的问题就是梯度受样本数据数量的影响极大，不过在线性回归的评测标准一节中讲到的均方误差MSE就是为了解决这个问题的，所以我们将损失函数直接转变为MSE：</p>
<p>$$\frac 1 {m}\sum_{i=1}^m(y^{(i)}-X_b^{(i)} \theta)^2$$</p>
<p>那么对MSE求梯度的结果自然也是损失函数求梯度后乘以1/m：</p>
<p>$$\frac 2 m\begin{bmatrix}<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)}) \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_1^{(i)} \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_2^{(i)} \\<br>… \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<h2 id="u5B9E_u73B0_u7EBF_u6027_u56DE_u5F52_u4E2D_u7684_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#u5B9E_u73B0_u7EBF_u6027_u56DE_u5F52_u4E2D_u7684_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="实现线性回归中的梯度下降法"></a>实现线性回归中的梯度下降法</h2><p>我们先在Jupyter Notebook中来实现，然后再在PyCharm中进行封装：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机种子</span></span><br><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line"><span class="comment"># 随机构建100个数</span></span><br><span class="line">x = <span class="number">2</span> * np.random.random(size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 拟定一个线性方程，x乘3乘4后再加上随机生成的正态分布数</span></span><br><span class="line">y = x * <span class="number">3.</span> + <span class="number">4.</span> + np.random.normal(size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 先从100行1列的矩阵开始，既样本数据只有一个特征</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/2675d3796567abc2691dd832b368d8d2.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">	<span class="keyword">try</span>:</span><br><span class="line">		<span class="keyword">return</span> np.sum((y - X_b.dot(theta))**<span class="number">2</span>) / len(X_b)</span><br><span class="line">	<span class="keyword">except</span>:</span><br><span class="line">		<span class="keyword">return</span> float(<span class="string">'inf'</span>)</span><br><span class="line">		</span><br><span class="line"><span class="comment"># 定义梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dL</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">	<span class="comment"># 开辟空间，大小为theta向量的大小</span></span><br><span class="line">	gradient = np.empty(len(theta))</span><br><span class="line">	<span class="comment"># 第0元素个特殊处理</span></span><br><span class="line">	gradient[<span class="number">0</span>] = np.sum(X_b.dot(theta) - y)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(theta)):</span><br><span class="line">		<span class="comment"># 矩阵求和可以转换为点乘</span></span><br><span class="line">		gradient[i] = (X_b.dot(theta) - y).dot(X_b[:, i])</span><br><span class="line">	<span class="keyword">return</span> gradient * <span class="number">2</span> / len(X_b)</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 梯度下降法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X_b, y, initial_theta, eta, n_iters = <span class="number">1e4</span>, difference = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">	theta = initial_theta</span><br><span class="line">	i_iter = <span class="number">0</span></span><br><span class="line">	<span class="keyword">while</span> i_iter &lt; n_iters:</span><br><span class="line">		gradient = dL(theta, X_b, y)</span><br><span class="line">		last_theta = theta</span><br><span class="line">		theta = theta - eta * gradient</span><br><span class="line">	</span><br><span class="line">		<span class="keyword">if</span>(abs(L(theta, X_b, y) - L(last_theta, X_b, y)) &lt; difference):</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		</span><br><span class="line">		i_iter += <span class="number">1</span></span><br><span class="line">	<span class="keyword">return</span> theta</span><br><span class="line">	</span><br><span class="line"><span class="comment"># 构建X_b</span></span><br><span class="line">X_b = np.hstack([np.ones((X.shape[<span class="number">0</span>], <span class="number">1</span>)), X])</span><br><span class="line"><span class="comment"># 初始化theta向量为元素全为0的向量</span></span><br><span class="line">initial_theta = np.zeros(X_b.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 设置步长为0.01</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">theta = gradient_descent(X_b, y, initial_theta, eta)</span><br><span class="line">theta</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">4.02145786</span>,  <span class="number">3.00706277</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到我们得到的结果，解决为4，斜率为3，和我们拟定的线性方程是一致的。</p>
<p>下面我们在PyCharm中封装梯度下降法。在<code>LinearRegression</code>类中再增加一个<code>fit_gd</code>方法，和<code>fit_normal</code>方法区分开，表明是用梯度下降法进行训练：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用梯度下降法，根据训练数据集X_train，y_train训练LinearRegression模型</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit_gd</span><span class="params">(self, X_train, y_train, eta=<span class="number">0.01</span>, n_iters=<span class="number">1e4</span>)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \</span><br><span class="line">			<span class="string">"特征数据矩阵的行数要等于样本结果数据的行数"</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 定义损失函数</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">			<span class="keyword">try</span>:</span><br><span class="line">				<span class="keyword">return</span> np.sum((y - X_b.dot(theta)) ** <span class="number">2</span>) / len(X_b)</span><br><span class="line">			<span class="keyword">except</span>:</span><br><span class="line">				<span class="keyword">return</span> float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 定义梯度</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">dL</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">			<span class="comment"># 开辟空间，大小为theta向量的大小</span></span><br><span class="line">			gradient = np.empty(len(theta))</span><br><span class="line">			<span class="comment"># 第0元素个特殊处理</span></span><br><span class="line">			gradient[<span class="number">0</span>] = np.sum(X_b.dot(theta) - y)</span><br><span class="line"></span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(theta)):</span><br><span class="line">				<span class="comment"># 矩阵求和可以转换为点乘</span></span><br><span class="line">				gradient[i] = (X_b.dot(theta) - y).dot(X_b[:, i])</span><br><span class="line"></span><br><span class="line">			<span class="keyword">return</span> gradient * <span class="number">2</span> / len(X_b)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 梯度下降法</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X_b, y, initial_theta, eta, n_iters=<span class="number">1e4</span>, difference=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">			theta = initial_theta</span><br><span class="line">			i_iter = <span class="number">0</span></span><br><span class="line">			<span class="keyword">while</span> i_iter &lt; n_iters:</span><br><span class="line">				gradient = dL(theta, X_b, y)</span><br><span class="line">				last_theta = theta</span><br><span class="line">				theta = theta - eta * gradient</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (abs(L(theta, X_b, y) - L(last_theta, X_b, y)) &lt; difference):</span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">				i_iter += <span class="number">1</span></span><br><span class="line">			<span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 构建X_b</span></span><br><span class="line">		X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br><span class="line">		<span class="comment"># 初始化theta向量为元素全为0的向量</span></span><br><span class="line">		initial_theta = np.zeros(X_b.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">		self._theta = gradient_descent(X_b, y_train, initial_theta, eta)</span><br><span class="line">		self.intercept_ = self._theta[<span class="number">0</span>]</span><br><span class="line">		self.coef_ = self._theta[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>
<h2 id="u4F18_u5316_u68AF_u5EA6_u516C_u5F0F"><a href="#u4F18_u5316_u68AF_u5EA6_u516C_u5F0F" class="headerlink" title="优化梯度公式"></a>优化梯度公式</h2><p>我们先将之前推导出来的梯度公式写出来：</p>
<p>$$\nabla L = \frac 2 m\begin{bmatrix}<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)}) \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_1^{(i)} \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_2^{(i)} \\<br>… \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<p>将展开来看：</p>
<p>$$\nabla L = \frac 2 m\begin{bmatrix}<br>(X_b^{(1)}\theta-y^{(1)})+(X_b^{(2)}\theta-y^{(2)})+…+(X_b^{(m)}\theta-y^{(m)}) \\<br>(X_b^{(1)}\theta-y^{(1)})X_1^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_1^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_1^{(m)} \\<br>(X_b^{(1)}\theta-y^{(1)})X_2^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_2^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_2^{(m)} \\<br>… \\<br>(X_b^{(1)}\theta-y^{(1)})X_n^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_n^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_n^{(m)} \\<br> \end{bmatrix}$$</p>
<p> 将第一行的元素形式统一，每项都乘以$X_0$，并且$X_0$恒等于1：</p>
<p> $$\nabla L = \frac 2 m\begin{bmatrix}<br>(X_b^{(1)}\theta-y^{(1)})X_0^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_0^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_0^{(m)} \\<br>(X_b^{(1)}\theta-y^{(1)})X_1^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_1^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_1^{(m)} \\<br>(X_b^{(1)}\theta-y^{(1)})X_2^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_2^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_2^{(m)} \\<br>… \\<br>(X_b^{(1)}\theta-y^{(1)})X_n^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_n^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_n^{(m)} \\<br> \end{bmatrix}$$</p>
<p>下面我们来两个矩阵，A为一个1行m列的矩阵，B为一个m行n列的矩阵：</p>
<p>$$A=\frac 2 m \begin{bmatrix}X_b^{(1)}\theta-y^{(1)}&amp; X_b^{(2)}\theta-y^{(2)}&amp; … &amp;X_b^{(m)}\theta-y^{(m)}\end{bmatrix}$$</p>
<p>$$B=\begin{bmatrix}<br>X_0^{(1)}&amp; X_1^{(1)}&amp; X_2^{(1)}&amp; … &amp; X_n^{(1)} \\<br>X_0^{(2)}&amp; X_1^{(2)}&amp; X_2^{(2)}&amp; … &amp; X_n^{(2)} \\<br>… \\<br>X_0^{(m)}&amp; X_1^{(m)}&amp; X_2^{(m)}&amp; … &amp; X_n^{(m)}<br>\end{bmatrix}$$</p>
<p>在第二篇笔记中我们复习过矩阵的运算，让A矩阵点乘B矩阵会得到一个1行n列的新矩阵：</p>
<p>$$A \cdot B=\frac 2 m \begin{bmatrix}X_b^{(1)}\theta-y^{(1)}&amp; X_b^{(2)}\theta-y^{(2)}&amp; … &amp;X_b^{(m)}\theta-y^{(m)}\end{bmatrix} \cdot\ \begin{bmatrix}<br>X_0^{(1)}&amp; X_1^{(1)}&amp; X_2^{(1)}&amp; … &amp; X_n^{(1)} \\<br>X_0^{(2)}&amp; X_1^{(2)}&amp; X_2^{(2)}&amp; … &amp; X_n^{(2)} \\<br>… \\<br>X_0^{(m)}&amp; X_1^{(m)}&amp; X_2^{(m)}&amp; … &amp; X_n^{(m)}<br>\end{bmatrix} \\<br>=\begin{bmatrix}<br>(X_b^{(1)}\theta-y^{(1)})X_0^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_0^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_0^{(m)}&amp; \\ (X_b^{(1)}\theta-y^{(1)})X_1^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_1^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_1^{(m)}&amp; \\<br>…\\<br>(X_b^{(1)}\theta-y^{(1)})X_n^{(1)}+(X_b^{(2)}\theta-y^{(2)})X_n^{(2)}+…+(X_b^{(m)}\theta-y^{(m)})X_n^{(m)}<br>\end{bmatrix}$$</p>
<p>注意上面$A \cdot B$的矩阵是1行n列的矩阵，将其转置后就称为了n行1列的矩阵，正是之前展开的梯度$\nabla L$，所以我们的梯度公式可写为：</p>
<p>$$\nabla L = \frac 2 m((X_b \theta - y)^\top X_b)^\top=\frac 2 m X_b^\top(X_b \theta - y)$$</p>
<p>如此一来我们就可以修改一下之前封装的梯度的方法了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义梯度</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">dL</span><span class="params">(theta, X_b, y)</span>:</span></span><br><span class="line">			<span class="comment"># # 开辟空间，大小为theta向量的大小</span></span><br><span class="line">			<span class="comment"># gradient = np.empty(len(theta))</span></span><br><span class="line">			<span class="comment"># # 第0元素个特殊处理</span></span><br><span class="line">			<span class="comment"># gradient[0] = np.sum(X_b.dot(theta) - y)</span></span><br><span class="line">			<span class="comment">#</span></span><br><span class="line">			<span class="comment"># for i in range(1, len(theta)):</span></span><br><span class="line">			<span class="comment">#     # 矩阵求和可以转换为点乘</span></span><br><span class="line">			<span class="comment">#     gradient[i] = (X_b.dot(theta) - y).dot(X_b[:, i])</span></span><br><span class="line"></span><br><span class="line">			<span class="keyword">return</span> X_b.T.dot(X_b.dot(theta) - y) * <span class="number">2</span> / len(X_b)</span><br></pre></td></tr></table></figure>
<p>此时就可以用一行代码取代之前的for循环来实现梯度了。</p>
<h2 id="u7528_u771F_u5B9E_u6570_u636E_u6D4B_u8BD5_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#u7528_u771F_u5B9E_u6570_u636E_u6D4B_u8BD5_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="用真实数据测试梯度下降法"></a>用真实数据测试梯度下降法</h2><p>我们用Scikit Learn提供的波士顿房价来测试一下梯度下降法:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line">X = X[y &lt; <span class="number">50.0</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取前10行数据观察一下</span></span><br><span class="line">X[<span class="number">10</span>:]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"></span><br><span class="line">Out[<span class="number">17</span>]:</span><br><span class="line">array([[  <span class="number">2.24890000e-01</span>,   <span class="number">1.25000000e+01</span>,   <span class="number">7.87000000e+00</span>, ...,</span><br><span class="line">		  <span class="number">1.52000000e+01</span>,   <span class="number">3.92520000e+02</span>,   <span class="number">2.04500000e+01</span>],</span><br><span class="line">	   [  <span class="number">1.17470000e-01</span>,   <span class="number">1.25000000e+01</span>,   <span class="number">7.87000000e+00</span>, ...,</span><br><span class="line">		  <span class="number">1.52000000e+01</span>,   <span class="number">3.96900000e+02</span>,   <span class="number">1.32700000e+01</span>],</span><br><span class="line">	   [  <span class="number">9.37800000e-02</span>,   <span class="number">1.25000000e+01</span>,   <span class="number">7.87000000e+00</span>, ...,</span><br><span class="line">		  <span class="number">1.52000000e+01</span>,   <span class="number">3.90500000e+02</span>,   <span class="number">1.57100000e+01</span>],</span><br><span class="line">	   ..., </span><br><span class="line">	   [  <span class="number">6.07600000e-02</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.19300000e+01</span>, ...,</span><br><span class="line">		  <span class="number">2.10000000e+01</span>,   <span class="number">3.96900000e+02</span>,   <span class="number">5.64000000e+00</span>],</span><br><span class="line">	   [  <span class="number">1.09590000e-01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.19300000e+01</span>, ...,</span><br><span class="line">		  <span class="number">2.10000000e+01</span>,   <span class="number">3.93450000e+02</span>,   <span class="number">6.48000000e+00</span>],</span><br><span class="line">	   [  <span class="number">4.74100000e-02</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.19300000e+01</span>, ...,</span><br><span class="line">		  <span class="number">2.10000000e+01</span>,   <span class="number">3.96900000e+02</span>,   <span class="number">7.88000000e+00</span>]])</span><br></pre></td></tr></table></figure>
<p>从前10行的数据中可以看出来，数据之间的差距非常大，不同于正规方程法的$\theta$有数学解，在梯度下降中会非常影响梯度的值，既影响$\theta$的搜索，从而影响收敛速度和是否能收敛，所以一般在使用梯度下降法前，都需要对数据进行归一化处理，将数据转换到同一尺度下。在第三篇笔记中介绍过数据归一化的方法，Scikit Learn中也提供了数据归一化的方法，我们就使用Scikit Learn中提供的方法对波士顿数据进行归一化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先将样本数据拆分为训练样本数据和测试样本数据</span></span><br><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y, seed=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Scikit Learn提供的数据归一化方法处理训练数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">standard_scalar = StandardScaler()</span><br><span class="line">standard_scalar.fit(X_train)</span><br><span class="line">X_train_standard = standard_scalar.transform(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再来看看归一化后的数据前10行，和未归一化之前的做一下比较</span></span><br><span class="line">X_train_standard[<span class="number">10</span>:]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[-<span class="number">0.3854578</span> , -<span class="number">0.49494584</span>, -<span class="number">0.70629402</span>, ..., -<span class="number">0.5235474</span> ,</span><br><span class="line">		 <span class="number">0.22529916</span>, -<span class="number">1.09634897</span>],</span><br><span class="line">	   [ <span class="number">8.34092707</span>, -<span class="number">0.49494584</span>,  <span class="number">1.03476103</span>, ...,  <span class="number">0.80665081</span>,</span><br><span class="line">		 <span class="number">0.32122168</span>,  <span class="number">1.38375621</span>],</span><br><span class="line">	   [-<span class="number">0.44033902</span>,  <span class="number">1.83594326</span>, -<span class="number">0.83504431</span>, ..., -<span class="number">0.90360404</span>,</span><br><span class="line">		 <span class="number">0.45082029</span>, -<span class="number">0.83197228</span>],</span><br><span class="line">	   ..., </span><br><span class="line">	   [-<span class="number">0.39976896</span>, -<span class="number">0.49494584</span>,  <span class="number">1.58926511</span>, ...,  <span class="number">1.28172161</span>,</span><br><span class="line">		 <span class="number">0.42018591</span>,  <span class="number">0.2101475</span> ],</span><br><span class="line">	   [-<span class="number">0.422702</span>  , -<span class="number">0.49494584</span>, -<span class="number">0.74140773</span>, ...,  <span class="number">0.33158002</span>,</span><br><span class="line">		 <span class="number">0.4131248</span> , -<span class="number">0.41372555</span>],</span><br><span class="line">	   [-<span class="number">0.44280463</span>,  <span class="number">3.05688517</span>, -<span class="number">1.35589775</span>, ..., -<span class="number">0.14349077</span>,</span><br><span class="line">		-<span class="number">0.1499176</span> , -<span class="number">0.02205637</span>]])</span><br></pre></td></tr></table></figure>
<p>可以看到数据都在同一个尺度内了，然后我们用优化后的梯度下降法来训练归一化后的样本数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.LinearRegression <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit_gd(X_train_standard, y_train)</span><br><span class="line">lr.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">21.629336734693847</span></span><br><span class="line"></span><br><span class="line">lr.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([-<span class="number">0.9525182</span> ,  <span class="number">0.55252408</span>, -<span class="number">0.30736822</span>, -<span class="number">0.03926274</span>, -<span class="number">1.37014814</span>,</span><br><span class="line">		<span class="number">2.61387294</span>, -<span class="number">0.82461734</span>, -<span class="number">2.36441751</span>,  <span class="number">2.02340617</span>, -<span class="number">2.17890468</span>,</span><br><span class="line">	   -<span class="number">1.76883751</span>,  <span class="number">0.7438223</span> , -<span class="number">2.25694241</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在计算score前，需要对X_test数据也进行归一化处理</span></span><br><span class="line">X_test_standard = standard_scalar.transform(X_test)</span><br><span class="line">lr1.score(X_test_standard, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80028998868733348</span></span><br></pre></td></tr></table></figure>
<p>看到结果与我们之前使用正规方程法得到的结果是一致的。</p>
<h2 id="u968F_u673A_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#u968F_u673A_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><p>在实际的运用中，训练数据的量级往往都很大，我们目前实现梯度下降法是需要让每一个样本数据都参与梯度计算的，称为批量梯度下降，所以当样本数据量很大的时候，计算梯度的速度就会很慢，所以这一节我们来看看改进这个情况的随机梯度下降法。</p>
<p>以一个山谷的俯视示意图为例，先来看看批量梯度下降法对$\theta$值的查找轨迹：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/815e5ed71aee17484fb96bb11c362b9a.jpg" alt=""></p>
<p>可以看到呈现出的轨迹是平滑下降的，每一个$\theta$值都比上一个$\theta$值小，最终找到使损失函数达到极小值的$\theta$。这是因为每一个$\theta$都会对所有样本进行了计算，为了增加拟合效率，对每一个$\theta$，我们不对所有样本进行计算，而是每次只取一个样本进行计算，取多次来找到最终的$\theta$值，这就是随机梯度下降的基本思想。</p>
<p>那么我们再来看看随机梯度下降法的$\theta$查找轨迹：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/92e28ad8c4fa25fd273d2d0d92c7a1ec.jpg" alt=""></p>
<p>因为每次只随机取一个样本进行计算，所以可以看到随机梯度下降法查找$\theta$的轨迹是也是随机的，很多时候搜索下一个$\theta$的路径并不是最短路径，并且下一个$\theta$的损失函数值比上一个$\theta$的损失函数值还要大，但是最终依然会找到使损失函数达到极小值的$\theta$。</p>
<p>我们来看看梯度公式：</p>
<p>$$\frac 2 m\begin{bmatrix}<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)}) \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_1^{(i)} \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_2^{(i)} \\<br>… \\<br>\sum_{i=1}^m (X_b^{(i)}\theta-y^{(i)})X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<p> 按照随机梯度下降的思路，每次只取一个样本进行计算，也就是i每次只取一个值：</p>
<p> $$2\begin{bmatrix}<br>(X_b^{(i)}\theta-y^{(i)}) \\<br>(X_b^{(i)}\theta-y^{(i)})X_1^{(i)} \\<br>(X_b^{(i)}\theta-y^{(i)})X_2^{(i)} \\<br>… \\<br>(X_b^{(i)}\theta-y^{(i)})X_n^{(i)} \\<br> \end{bmatrix}$$</p>
<p>向量化后可得：</p>
<p>$$2(X_b^{(i)})^\top(X_b^{(i)} \theta - y^{(i)})$$</p>
<p>注意上面这个公式并不是梯度公式，而是搜索$\theta$的某一个方向，批量梯度下降是不放过任何一个搜索$\theta$的方向，而随机梯度下降是每次选择一个方向进行搜索。另外需要注意的是随机梯度下降法中的步长（学习率）并不是固定不变的，不然就无法找补随机路径的不足了，所以随机梯度下降法中的步长是根据搜索次数的增加而降低的，也就是逐渐递减的。那么学习率的公式我们首先想到的就是取搜索次数的倒数：</p>
<p>$$\eta=\frac 1 {i\_iters}$$</p>
<p>但是如果搜索次数很小或者很大的时候都会出现问题，前者会导致学习率下降的幅度过大，而后者会导致学习率下降的幅度过小，所以我们将分母加上一个常数，分子也用一个常数，这样就能保证学习率的递减幅度在一个较为平滑的状态：</p>
<p>$$\eta=\frac a {i\_iters + b}$$</p>
<p>a和b其实就是随机梯度下降法的两个超参数了，不过这里我们不对这两个超参数进行搜索，我们使用最佳实践就好，一般取a为5，b为50。</p>
<p>另外一个关键的超参数是搜索次数，假设取样本数据量的1/3作为搜索次数，因为每次是随机取一个样本数据，那势必会有一部分样本数据取不到，从而不会计算，并且取到的样本数据里也会有重复的数据，这样虽然收敛时间减少了，但是准确率却会打折扣，那么为了训练数据的准确性，我们希望每次搜索到的样本数据不重复，并且能所有的样本数据都希望在某一方向进行计算，所以将搜索次数的概念转变一下，既为随机搜索样本数据，且不重复，且所有样本数据都被搜索到一遍的轮数。这样一来，这个轮数可以很小，一般搜索4轮5轮既可。</p>
<h3 id="u5B9E_u73B0_u968F_u673A_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#u5B9E_u73B0_u968F_u673A_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="实现随机梯度下降法"></a>实现随机梯度下降法</h3><p>我们先模拟10万条样本数据，然后用批量梯度下降法训练一次，看看拟合时间：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 模拟10万条样本数据</span></span><br><span class="line">m = <span class="number">100000</span></span><br><span class="line"><span class="comment"># 随机生成10万个数</span></span><br><span class="line">x = np.random.random(size=m)</span><br><span class="line"><span class="comment"># 转换成10万行1列的矩阵</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 拟定一个线性方程，计算y值，系数为4，截距为3，并且加上一个随机噪音值</span></span><br><span class="line">y = x * <span class="number">4</span> + <span class="number">3</span> + np.random.normal(<span class="number">0</span>, <span class="number">3</span>, size=m)</span><br><span class="line"><span class="comment"># 先使用批量梯度下降法</span></span><br><span class="line"><span class="keyword">from</span> myML.LinearRegression <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">%%time</span><br><span class="line">lr.fit_gd(X, y)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">6.59</span> s, sys: <span class="number">496</span> ms, total: <span class="number">7.08</span> s</span><br><span class="line">Wall time: <span class="number">5.19</span> s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 截距</span></span><br><span class="line">lr.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.9982721096748928</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 系数</span></span><br><span class="line">lr.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">3.99485476</span>])</span><br></pre></td></tr></table></figure>
<p>我们看到使用批量梯度下降法计算出的截距和系数和拟定的线性方程中的截距和系数是差不多的，并且拟合时间用了5.19秒。</p>
<p>下面我们直接在PyCharm中封装随机梯度下降的方法，在<code>LinearRegression</code>类中增加<code>fit_sgd</code>方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用随机梯度下降法，根据训练数据集X_train，y_train训练LinearRegression模型</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit_sgd</span><span class="params">(self, X_train, y_train, n_iters=<span class="number">5</span>, a=<span class="number">5</span>, b=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \</span><br><span class="line">			<span class="string">"特征数据矩阵的行数要等于样本结果数据的行数"</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">assert</span> n_iters &gt;= <span class="number">1</span>, \</span><br><span class="line">			<span class="string">"至少要搜索一轮"</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 定义theta查找方向的函数，这里不是全量的X_b矩阵了，而是X_b矩阵中的一行数据，</span></span><br><span class="line">		<span class="comment"># 既其中的的一个样本数据，对应的y值也只有一个</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">dL_sgd</span><span class="params">(theta, X_b_i, y_i)</span>:</span></span><br><span class="line">			<span class="keyword">return</span> X_b_i.T.dot(X_b_i.dot(theta) - y_i) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">		<span class="comment"># 实现随机梯度下降法</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(X_b, y, initial_theta, n_iters)</span>:</span></span><br><span class="line"></span><br><span class="line">			<span class="comment"># 定义学习率公式</span></span><br><span class="line">			<span class="function"><span class="keyword">def</span> <span class="title">eta</span><span class="params">(iters)</span>:</span></span><br><span class="line">				<span class="keyword">return</span> a / (iters + b)</span><br><span class="line"></span><br><span class="line">			theta = initial_theta</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 样本数量</span></span><br><span class="line">			m = len(X_b)</span><br><span class="line"></span><br><span class="line">			<span class="comment"># 第一层循环是循环轮数</span></span><br><span class="line">			<span class="keyword">for</span> i_inter <span class="keyword">in</span> range(n_iters):</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 在每一轮，随机生成一个乱序数组，个数为m</span></span><br><span class="line">				indexs = np.random.permutation(m)</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 打乱样本数据</span></span><br><span class="line">				X_b_new = X_b[indexs]</span><br><span class="line">				y_new = y[indexs]</span><br><span class="line"></span><br><span class="line">				<span class="comment"># 第二层循环便利所有为乱序的样本数据，既保证样本数据能被随机的，全部的计算到</span></span><br><span class="line">				<span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">					<span class="comment"># 每次用一个随机样本数据计算theta搜索方向</span></span><br><span class="line">					gradient = dL_sgd(theta, X_b_new[i], y_new[i])</span><br><span class="line">					<span class="comment"># 计算下一个theta</span></span><br><span class="line">					theta = theta - eta(i_inter * m + i) * gradient</span><br><span class="line"></span><br><span class="line">				<span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">		<span class="comment"># 构建X_b</span></span><br><span class="line">		X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br><span class="line">		<span class="comment"># 初始化theta向量为元素全为0的向量</span></span><br><span class="line">		initial_theta = np.zeros(X_b.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">		self._theta = sgd(X_b, y_train, initial_theta, n_iters)</span><br><span class="line">		self.intercept_ = self._theta[<span class="number">0</span>]</span><br><span class="line">		self.coef_ = self._theta[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<p>然后在Jupyter Notebook中使用封装好的随机梯度下降法来训练刚才的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr1 = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我们使用默认的5次搜索轮数，轮数越少，拟合时间越短</span></span><br><span class="line">%%time</span><br><span class="line">lr1.fit_sgd(X, y)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">967</span> ms, sys: <span class="number">6.31</span> ms, total: <span class="number">973</span> ms</span><br><span class="line">Wall time: <span class="number">971</span> ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 截距</span></span><br><span class="line">lr1.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.9746308939605761</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 系数</span></span><br><span class="line">lr1.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">3.96594805</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到最终的结果和批量梯度下降法训练出的是差不多的，但是拟合时间只用了971毫秒。</p>
<h2 id="Scikit_Learn_u4E2D_u7684_u968F_u673A_u68AF_u5EA6_u4E0B_u964D_u6CD5"><a href="#Scikit_Learn_u4E2D_u7684_u968F_u673A_u68AF_u5EA6_u4E0B_u964D_u6CD5" class="headerlink" title="Scikit Learn中的随机梯度下降法"></a>Scikit Learn中的随机梯度下降法</h2><p>这一节我们用真实的波士顿房价数据，使用Scikit Learn中的随机梯度下降进行训练看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line">X = X[y &lt; <span class="number">50.0</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y, seed=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">standard_scalar = StandardScaler()</span><br><span class="line">standard_scalar.fit(X_train)</span><br><span class="line">X_train_standard = standard_scalar.transform(X_train)</span><br><span class="line">X_test_standard = standard_scalar.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(n_iter=<span class="number">150</span>)</span><br><span class="line">%time sgd_reg.fit(X_train_standard, y_train)</span><br><span class="line">sgd_reg.score(X_test_standard, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">8.59</span> ms, sys: <span class="number">2.26</span> ms, total: <span class="number">10.8</span> ms</span><br><span class="line">Wall time: <span class="number">8.06</span> ms</span><br><span class="line"><span class="number">0.80003192122081712</span></span><br></pre></td></tr></table></figure>
<p>再来看看使用我们自己封装的随机梯度下降训练波士顿房价数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.LinearRegression <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">%%time</span><br><span class="line">lr.fit_gd(X_train_standard, y_train, n_iters=<span class="number">150</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">256</span> ms, sys: <span class="number">3.43</span> ms, total: <span class="number">259</span> ms</span><br><span class="line">Wall time: <span class="number">257</span> ms</span><br><span class="line"></span><br><span class="line">lr1.score(X_test_standard, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80028998868733348</span></span><br></pre></td></tr></table></figure>
<p>可以看到在同样的搜索轮数，相同的评分值情况下，Scikit Learn中的随机梯度下降法的收敛时间远远小于我们自己实现的随机梯度下降法，这是因为Scikit Learn中实现的时候使用了大量优化的算法，我们只是使用了最核心的思想进行封装，这点需要大家知晓。</p>
<h2 id="u5173_u4E8E_u68AF_u5EA6_u7684_u8C03_u8BD5"><a href="#u5173_u4E8E_u68AF_u5EA6_u7684_u8C03_u8BD5" class="headerlink" title="关于梯度的调试"></a>关于梯度的调试</h2><p>从字面意思就不难看出梯度下降法中梯度的重要性，在线性回归问题中，我们尚且能推导出梯度的公式，但在一些复杂的情况下，推导梯度的公式非常不容易，所以我们推导出的梯度公式是否合理，是否是正确的，就需要有一个方法来验证。这一小节就给大家介绍一个梯度调试的方法。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/abb3840fcba54f337a5ddc63dc52d835.jpg" alt=""></p>
<p>拿一维场景来说，A点的梯度也就是导数是它切线M的斜率。然后我们在直线M的右侧再画出一条平行与直线M的直线N，直线N的斜率与直线M的斜率近乎相等，此时直线N与曲线有两个相交点B和点C，这两个点分别在A点的负方向，也就是下上方，和在A点的正方向，也就是在下方，此时直线M也称为曲线的割线：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/0697eece8d0ea059c5dc25486f9e6190.jpg" alt=""></p>
<p>在第一篇笔记中讲过直线的斜率定义，既对于两个已知点$(x_1,y_1)$和$(x_2,y_2)$，如果$x_1$不等于$x_2$，则经过这两点直线的斜率为$(y_1-y_2)/(x_1-x_2)$。我们假设点B和点C相距点A为$\epsilon$，所以直线N的斜率为：</p>
<p>$$\frac {L(\theta + \epsilon)-L(\theta - \epsilon)}{\theta + \epsilon - (\theta - \epsilon)}=\frac {L(\theta + \epsilon)-L(\theta - \epsilon)}{2\epsilon}$$ </p>
<p>通过这个公式就可以模拟计算出某一点的导数。对于高维的梯度也是同样的道理。</p>
<p>$$\theta = (\theta_0,\theta_1,\theta_2,…,\theta_n)$$<br>$$\frac {\partial L}{\partial \theta}=(\frac {\partial L}{\partial \theta_0},\frac {\partial L}{\partial \theta_1},\frac {\partial L}{\partial \theta_2},…,\frac {\partial L}{\partial \theta_n})$$</p>
<p>高维情况中，$\theta$和梯度如上所示，如果我们要模拟计算$\theta_0$的梯度，首先得出$\theta_0$上方和下方的$\theta$：</p>
<p>$$\theta_0^+ = (\theta_0+\epsilon,\theta_1,\theta_2,…,\theta_n)$$<br>$$\theta_0^- = (\theta_0-\epsilon,\theta_1,\theta_2,…,\theta_n)$$</p>
<p>那么模拟出的$\theta_0$导数为：</p>
<p>$$\frac {\partial L}{\partial \theta_0}=\frac {L(\theta_0^+)-L(\theta_0^-)}{2\epsilon}$$</p>
<p>以此类推，可以模拟计算出所有$\theta$的导数，从而模拟计算出梯度。虽然该方法与损失函数形态无关，但是对每个$\theta$导数的模拟计算都需要将两组$\theta$向量代入损失函数进行计算，时间复杂度还是非常高的，但是有一个优势是可以忽略损失函数形态。所以我们一般用这种方式在一开始投入一些时间算出梯度，因为这个梯度是基于斜率的数学定理计算出的，所以它可以认为是一个合理的梯度，然后将它作为标准来验证通过梯度下降法计算出的梯度的正确性。</p>
<h3 id="u5B9E_u73B0_u68AF_u5EA6_u7684_u8C03_u8BD5_u65B9_u6CD5"><a href="#u5B9E_u73B0_u68AF_u5EA6_u7684_u8C03_u8BD5_u65B9_u6CD5" class="headerlink" title="实现梯度的调试方法"></a>实现梯度的调试方法</h3><p>我们在PyCharm中对我们封装好的梯度下降做一下改动，首先在<code>LinearRegression</code>类中增加一个名为<code>dL_debug()</code>的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dL_debug</span><span class="params">(theta, X_b, y, epsilon=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">			<span class="comment"># 开辟大小与theta向量一致的向量空间</span></span><br><span class="line">			result = np.empty(len(theta))</span><br><span class="line">			<span class="comment"># 便利theta向量中的每一个theta</span></span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta)):</span><br><span class="line">				<span class="comment"># 复制一份theta向量</span></span><br><span class="line">				theta_1 = theta.copy()</span><br><span class="line">				<span class="comment"># 将第i个theta加上一个距离，既求该theta正方向的theta</span></span><br><span class="line">				theta_1[i] += epsilon</span><br><span class="line">				<span class="comment"># 在复制一份theta向量</span></span><br><span class="line">				theta_2 = theta.copy()</span><br><span class="line">				<span class="comment"># 将第i个theta减去同样的距离，既求该theta负方向的theta</span></span><br><span class="line">				theta_2[i] -= epsilon</span><br><span class="line">				<span class="comment"># 求出这两个点连线的斜率，既模拟该theta的导数</span></span><br><span class="line">				result[i] = (L(theta_1, X_b, y) - L(theta_2, X_b, y)) / (<span class="number">2</span> * epsilon)</span><br><span class="line">			<span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>然后对<code>fit_gd()</code>方法增加一个参数<code>is_debug</code>，默认值为<code>False</code>，然后对<code>gradient_descent()</code>进行修改，让其当<code>is_debug</code>为<code>True</code>时走Debug的求梯度的方法，反之走梯度公式的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 实现批量梯度下降法</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X_b, y, initial_theta, eta, difference=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">			theta = initial_theta</span><br><span class="line">			i_iter = <span class="number">0</span></span><br><span class="line">			<span class="keyword">while</span> i_iter &lt; n_iters:</span><br><span class="line">				<span class="comment"># 当is_debug为True时走debug的求梯度的方法，反之走梯度公式的方法</span></span><br><span class="line">				<span class="keyword">if</span> is_debug:</span><br><span class="line">					gradient = dL_debug(theta, X_b, y)</span><br><span class="line">				<span class="keyword">else</span>:</span><br><span class="line">					gradient = dL(theta, X_b, y)</span><br><span class="line">				last_theta = theta</span><br><span class="line">				theta = theta - eta * gradient</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (abs(L(theta, X_b, y) - L(last_theta, X_b, y)) &lt; difference):</span><br><span class="line">					<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">				i_iter += <span class="number">1</span></span><br><span class="line">			<span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<p>下面我们使用波士顿房价的数据验证一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line">X = X[y &lt; <span class="number">50.0</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y, seed=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">standard_scalar = StandardScaler()</span><br><span class="line">standard_scalar.fit(X_train)</span><br><span class="line">X_train_standard = standard_scalar.transform(X_train)</span><br><span class="line">X_test_standard = standard_scalar.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> myML.LinearRegression <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line"><span class="comment"># 使用debug方式训练</span></span><br><span class="line">%time lr.fit_gd(X_train_standard, y_train, is_debug=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">2.55</span> s, sys: <span class="number">11.4</span> ms, total: <span class="number">2.57</span> s</span><br><span class="line">Wall time: <span class="number">2.57</span> s</span><br><span class="line"></span><br><span class="line">lr.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">21.629336734693936</span></span><br><span class="line"></span><br><span class="line">lr.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([-<span class="number">0.9525182</span> ,  <span class="number">0.55252408</span>, -<span class="number">0.30736822</span>, -<span class="number">0.03926274</span>, -<span class="number">1.37014814</span>,</span><br><span class="line">		<span class="number">2.61387294</span>, -<span class="number">0.82461734</span>, -<span class="number">2.36441751</span>,  <span class="number">2.02340617</span>, -<span class="number">2.17890468</span>,</span><br><span class="line">	   -<span class="number">1.76883751</span>,  <span class="number">0.7438223</span> , -<span class="number">2.25694241</span>])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 用梯度公式方式训练</span></span><br><span class="line">% time lr.fit_gd(X_train_standard, y_train)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">CPU times: user <span class="number">241</span> ms, sys: <span class="number">4.12</span> ms, total: <span class="number">245</span> ms</span><br><span class="line">Wall time: <span class="number">242</span> ms</span><br><span class="line"></span><br><span class="line">lr.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">21.629336734693936</span></span><br><span class="line"></span><br><span class="line">lr.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([-<span class="number">0.9525182</span> ,  <span class="number">0.55252408</span>, -<span class="number">0.30736822</span>, -<span class="number">0.03926274</span>, -<span class="number">1.37014814</span>,</span><br><span class="line">		<span class="number">2.61387294</span>, -<span class="number">0.82461734</span>, -<span class="number">2.36441751</span>,  <span class="number">2.02340617</span>, -<span class="number">2.17890468</span>,</span><br><span class="line">	   -<span class="number">1.76883751</span>,  <span class="number">0.7438223</span> , -<span class="number">2.25694241</span>])</span><br></pre></td></tr></table></figure>
<p>可以看到我们使用Debug方式训练数据时用了2.57秒，不过求了准确合理的梯度。然后用梯度公式法训练了数据，耗时242毫米，但结果相同，说明梯度计算的没有问题。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记主要介绍梯度下降法，梯度下降不是机器学习专属的算法，它是一种基于搜索的最优化方法，也就是通过不断的搜索然后找到损失函数的最小值。像上篇笔记中使用正规方程解实现多元线性回归，基于$X_b\theta$这个模型我们可以推导出$\theta$的数学解，但是很多模型是推导不出数学解的，所以就需要梯度下降法来搜索出最优解。</p>
<h2 id="u68AF_u5EA6_u4E0B_u964D_u6CD5_u6982_u5FF5"><a href="#u68AF_u5EA6_u4E0B_u964D_u6CD5_u6982_u5FF5" class="headerlink" title="梯度下降法概念"></a>梯度下降法概念</h2><p>我们来看看在二维坐标里的一个曲线方程：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8b3433626c22d7f31a236492458a58e0.jpg" alt=""></p>
<p>纵坐标表示损失函数L的值，横坐标表示系数$\theta$。每一个$\theta$的值都会对应一个损失函数L的值，我们希望损失函数收敛，既找到一个$\theta$的值，使损失函数L的值最小。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/7d33b5dbd153029338804e42063e7dae.jpg" alt=""></p>
<p>在曲线上定义一点A，对应一个$\theta$值，一个损失函数L的值，要判断点A是否是损失函数L的最小值，既求该点的导数，在第一篇笔记中我解释过，点A的导数就是直线M的斜率，直线M是点A的切线，所以导数描述了一个函数在某一点附近的变化率，并且导数大于零时，函数在区间内单调递增，导数小于零时函数在区间内单调递减。所以$\frac {d L}{d \theta}$表示损失函数L增大的变化率，$-\frac {d L}{d \theta}$表示损失函数L减小的变化率。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/79181d4455f5a40c4a652a41a743bc7b.jpg" alt=""></p>
<p>再在曲线上定义一点B，在点A的下方，B点的$\theta$值就是A点的$\theta$值加上让损失函数L递减的变化率$-\eta \frac {d L}{d \theta}$，$\eta$称为步长，既B点在$-\frac {d L}{d \theta}$变化率的基础下移动了多少距离，在机器学习中$\eta$这个值也称为学习率。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/12d864def3bdafa3f3ecda683f2950e4.jpg" alt=""></p>
<p>同理还可以再求得点C，然后看是否是损失函数的L的最小值。所以梯度下降法就是基于损失函数在某一点的变化率$-\frac {d L}{d \theta}$，以及寻找下一个点的步长$\eta$，不停的找到下一个点，然后判断该点处的损失函数值是否为最小值的过程。$-\eta \frac {d L}{d \theta}$就称为梯度。</p>
<p>在第一篇笔记中将极值的时候提到过，当曲线或者曲面很复杂时，会有多个驻点，既局部极值，所以如果运行一次梯度下降法寻找损失函数极值的话很有可能找到的只是局部极小值点。所以在实际运用中我们需要多次运行算法，随机化初始点，然后进行比较找到真正的全局极小值点，所以初始点的位置是梯度下降法的一个超参数。</p>
<p>不过在线性回归的场景中，我们的损失函数$\sum_{i=1}^m(y^{(i)}-\hat y^{(i)})^2$是有唯一极小值的，所以暂时不需要多次执行算法搜寻全局极值。</p>]]>
    
    </summary>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降" scheme="http://www.devtalking.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记五之线性回归、评测标准、多元线性回归]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-5/"/>
    <id>http://www.devtalking.com//articles/machine-learning-5/</id>
    <published>2018-02-04T16:00:00.000Z</published>
    <updated>2018-08-27T02:30:08.142Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记主要介绍线性回归算法，对在第一篇笔记中介绍过的线性回归算法进行实现。kNN算法主要解决的是分类问题，并且它的结果不具备良好的解释性。线性回归算法主要解决回归问题，它的结果具有良好的可解释性，和kNN算法的介绍过程一样，线性回归算法也蕴含了机器学习中很多重要的思想，并且它是许多强大的非线性模型的基础。</p>
<h2 id="u7B80_u5355_u7EBF_u6027_u56DE_u5F52"><a href="#u7B80_u5355_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="简单线性回归"></a>简单线性回归</h2><p>在第一篇笔记中，我们举过房屋面积大小和价格例子，将其绘制在二维坐标图上，横轴表示房屋面积，纵轴表示房屋价格，这里样本特征数据只有一个，那就是房屋面积，而在kNN算法的分类问题中，二维坐标图上横纵轴表示的都是样本特征数据，这是一个比较明显的区别。如果线性回归问题要在图中表示两种样本特征数据的话就需要三维空间坐标来表示。我们一般将只有一种样本特征数据的线性回归问题称为简单线性回归问题。</p>
<h3 id="u56DE_u987E_u7EBF_u6027_u56DE_u5F52"><a href="#u56DE_u987E_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="回顾线性回归"></a>回顾线性回归</h3><p>线性回归其实就是寻找一条直线，最大程度的拟合样本特征和样本输出标记之间的关系。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a9e2a5cbcffbf0037e8bf53f49cc414e.jpg" alt=""></p>
<p>我们来看上面这张图，大家知道直线的方程是$y=ax+b$，那么点A肯定是在一条直线上，该条直线方程为$y^{(i)}=ax^{(i)}+b$，那么点A的横轴值为$x^{(i)}$，既是A的样本特征值，纵轴值为$y^{(i)}$，既是A的样本输出值。我们假设图中的红线就是拟合直线，方程为$\hat y^{(i)}=ax^{(i)}+b$，也就是将$x^{(i)}$代入这条红线，会得到一个预测的纵轴值$\hat y^{(i)}$。我们希望真值$y^{(i)}$和预测值$\hat y^{(i)}$的差值越小，说明我们的拟合直线拟合的越好。</p>
<p>因为差值有正有负，为了保证都是正数，所以将差值进行平方，之所以不用绝对值，是为了方便求导数。</p>
<blockquote>
<p>方程求导的知识可参阅<a href="http://www.devtalking.com/articles/machine-learning-1/"> 《机器学习笔记一之机器学习定义、导数、最小二乘》 </a>。</p>
</blockquote>
<p>$$ (y^{(i)} - \hat y^{(i)})^2 $$</p>
<p>将所有样本特征都考虑到，既将所有真值和预测值的差值求和：</p>
<p>$$ \sum_{i=1}^m(y^{(i)} - \hat y^{(i)})^2 $$</p>
<p>将$ax^{(i)}+b$代入上面的公式就得到：</p>
<p>$$\sum_{i=1}^m(y^{(i)} - ax^{(i)}-b)^2$$</p>
<p>上面的公式我们称为损失函数（Loss Function），损失函数值越小，我们的拟合直线越好。在Loss函数中，$a$和$b$是变量，所以我们要做的就是找到使Loss函数值最小的$a$和$b$。这个套路是近乎所有参数学习算法常用的套路，既通过分析问题，确定问题的损失函数，通过最优化损失函数获得机器学习的模型。像线性回归、多项式回归、逻辑回归、SVM、神经网络等都是这个套路。</p>
<a id="more"></a>
<p>上述的Loss函数是一个典型的最小二乘法的问题，既通过最小化误差的平方和寻找数据的最佳函数匹配。求函数的最小值就会用到导数这个数学工具，具体如何推导上面的Loss函数可以参见第一篇学习笔记，这里不再累赘。最后得出a和b的求解公式为：</p>
<p>$$a=\frac {\overline x \ \overline y-\overline {xy}} {(\overline x)^2-\overline {x^2}}$$</p>
<p>$$b=\overline y-a\overline x$$</p>
<h2 id="u5B9E_u73B0_u7B80_u5355_u7EBF_u6027_u56DE_u5F52"><a href="#u5B9E_u73B0_u7B80_u5355_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="实现简单线性回归"></a>实现简单线性回归</h2><p>我们先在Jupyter Notebook中实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先模拟一组简单的样本特征数据和样本输出数据</span></span><br><span class="line">x = np.array([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>])</span><br><span class="line">y = np.array([<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">5.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将这组数据绘制出来</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b49f4c063a4d0bcbc9d823e21dd4c858.jpg" alt=""></p>
<p>然后我们使用上面推导出的公式求出$a$和$b$：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = (np.mean(x)*np.mean(y) - np.mean(x*y))/(np.mean(x)**<span class="number">2</span> - np.mean(x**<span class="number">2</span>))</span><br><span class="line">a</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80000000000000071</span></span><br><span class="line"></span><br><span class="line">b = np.mean(y) - a*np.mean(x)</span><br><span class="line">b</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.39999999999999769</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用a和b绘制出拟合直线</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.plot(x, a*x+b, color=<span class="string">"r"</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">6</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/563679f7b0fbded6d73dfb56c866bacb.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 新来一个特征值，利用拟合直线计算输出值</span></span><br><span class="line">x_predict = <span class="number">6</span></span><br><span class="line">y_predict = a*x_predict + b</span><br><span class="line">y_predict</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">5.200000000000002</span></span><br></pre></td></tr></table></figure>
<h3 id="u5C01_u88C5_u7B80_u5355_u7EBF_u6027_u56DE_u5F52_u65B9_u6CD5"><a href="#u5C01_u88C5_u7B80_u5355_u7EBF_u6027_u56DE_u5F52_u65B9_u6CD5" class="headerlink" title="封装简单线性回归方法"></a>封装简单线性回归方法</h3><p>我们在PyCharm中封装我们自己的简单线性回归方法：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/418584ad3f1588463ae05f7aca5a27e5.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLinearRegression</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.a_ = <span class="keyword">None</span></span><br><span class="line">		self.b_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据训练数据集x_train和y_train训练简单线性回归模型</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x_train, y_train)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> x_train.ndim == <span class="number">1</span>, \</span><br><span class="line">			<span class="string">"简单线性回归只能处理一个样本特征数据，所以x_train必须是一维向量"</span></span><br><span class="line">		<span class="keyword">assert</span> len(x_train) == len(y_train), \</span><br><span class="line">			<span class="string">"x_train和y_train的数量必须要对应"</span></span><br><span class="line"></span><br><span class="line">		self.a_ = (np.mean(x_train) * np.mean(y_train) - np.mean(x_train * y_train)) / (np.mean(x_train) ** <span class="number">2</span> - np.mean(x_train ** <span class="number">2</span>))</span><br><span class="line">		self.b_ = np.mean(y_train) - self.a_ * np.mean(x_train)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 给定待预测数据集x_predict，返回预测输出结果向量</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x_predict)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> x_predict.ndim == <span class="number">1</span>, <span class="string">"因为是简单线性回归，所以待预测数据集必须是一维向量"</span></span><br><span class="line">		<span class="keyword">assert</span> self.a_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.b_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">"必须先执行fit方法计算a和b"</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> np.array([self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> x_predict])</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 给定单个待预测数据x_single，返回x_single的预测结果</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x_single)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> self.a_ * x_single + self.b_</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">"SimpleLinearRegression()"</span></span><br></pre></td></tr></table></figure>
<p>然后我们就可以在Jupyter Notebook中使用我们封装的简单线性回归方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.SimpleLinearRegression <span class="keyword">import</span> SimpleLinearRegression</span><br><span class="line">slr = SimpleLinearRegression()</span><br><span class="line">slr.fit(x, y)</span><br><span class="line">slr.predict(np.array([x_predict]))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">5.2</span>])</span><br><span class="line"></span><br><span class="line">slr.a_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80000000000000071</span></span><br><span class="line"></span><br><span class="line">slr.b_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.39999999999999769</span></span><br></pre></td></tr></table></figure>
<h2 id="u7EBF_u6027_u56DE_u5F52_u7B97_u6CD5_u7684_u8BC4_u6D4B_u6807_u51C6"><a href="#u7EBF_u6027_u56DE_u5F52_u7B97_u6CD5_u7684_u8BC4_u6D4B_u6807_u51C6" class="headerlink" title="线性回归算法的评测标准"></a>线性回归算法的评测标准</h2><p>在讲kNN算法时，我们分类问题的评测标准是基于将样本数据拆分为训练数据和测试数据的前提下的，那么在线性回归算法中也是一样的。我们使用训练数据计算出<code>a</code>和<code>b</code>的值，然后将测试数据代入拟合直线方程算出结果，进行比较。我们通过公式来看一下。</p>
<p>$$ \sum_{i=1}^m(y_{train}^{(i)} - ax_{train}^{(i)} -b)^2 = \sum_{i=1}^m(y_{train}^{(i)} - \hat y_{train}^{(i)})^2 $$</p>
<p>将训练数据代入公式算出<code>a</code>和<code>b</code>，然后代入拟合直线方程算出$\hat y_{test}^{(i)}$：</p>
<p>$$ \hat y_{test}^{(i)} = ax_{test}^{(i)} + b $$</p>
<p>此时我们的衡量标准既为：</p>
<p>$$ \sum_{i=1}^m(y_{test}^{(i)} - \hat y_{test}^{(i)})^2 $$</p>
<p>也就是上面的公式值越小说明我们拟合的越好。</p>
<h3 id="u5747_u65B9_u8BEF_u5DEE_uFF08MSE_uFF09"><a href="#u5747_u65B9_u8BEF_u5DEE_uFF08MSE_uFF09" class="headerlink" title="均方误差（MSE）"></a>均方误差（MSE）</h3><p>上面这个公式有一个问题，那就是最终值受$m$的影响，比如某个算法10个样本数据求出的值为80，另一个算法10000个样本数据求出的值为100，也不能表明第一个算法就比第二个算法好，因为样本数据量相差巨大，所以将上面公式改变一下，将值除以$m$：</p>
<p>$$ \frac 1 m \sum_{i=1}^m(y_{test}^{(i)} - \hat y_{test}^{(i)})^2$$</p>
<p>这个衡量标准称为均方误差（MSE, Mean Squared Error）</p>
<h3 id="u5747_u65B9_u6839_u8BEF_u5DEE_uFF08RMSE_uFF09"><a href="#u5747_u65B9_u6839_u8BEF_u5DEE_uFF08RMSE_uFF09" class="headerlink" title="均方根误差（RMSE）"></a>均方根误差（RMSE）</h3><p>在对量纲不敏感的情况下，使用均方误差没什么问题，但是在一些对量纲比较敏感的场景下，均方误差就会有问题，因为均方误差的量纲为XX平方，比如房屋面积售价的例子，均方误差的量纲就成了$元^2$，所以就有了均方根误差（RMSE, Root Mean Squared Error）用以统一量纲：</p>
<p>$$ \sqrt {\frac 1 m \sum_{i=1}^m(y_{test}^{(i)} - \hat y_{test}^{(i)})^2}= \sqrt {MSE_{test}} $$</p>
<h3 id="u5E73_u5747_u7EDD_u5BF9_u8BEF_u5DEE_uFF08MAE_uFF09"><a href="#u5E73_u5747_u7EDD_u5BF9_u8BEF_u5DEE_uFF08MAE_uFF09" class="headerlink" title="平均绝对误差（MAE）"></a>平均绝对误差（MAE）</h3><p>另外一个能统一量纲的衡量公式为平均绝对误差，既将真实值与预测值的差取绝对值而不是平方：</p>
<p>$$ \frac 1 m \sum _{i=1}^m |y_{test}^{(i)} - \hat y_{test}^{(i)}| $$</p>
<h3 id="u5B9E_u73B0MSE_uFF0C_RMSE_uFF0C_MAE"><a href="#u5B9E_u73B0MSE_uFF0C_RMSE_uFF0C_MAE" class="headerlink" title="实现MSE， RMSE， MAE"></a>实现MSE， RMSE， MAE</h3><p>实现这三个指标我们使用Scikit Learn中提供的波士顿房价的数据集，我们先使用简单线性回归进行预测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">boston.feature_names</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="string">'CRIM'</span>, <span class="string">'ZN'</span>, <span class="string">'INDUS'</span>, <span class="string">'CHAS'</span>, <span class="string">'NOX'</span>, <span class="string">'RM'</span>, <span class="string">'AGE'</span>, <span class="string">'DIS'</span>, <span class="string">'RAD'</span>,</span><br><span class="line">	   <span class="string">'TAX'</span>, <span class="string">'PTRATIO'</span>, <span class="string">'B'</span>, <span class="string">'LSTAT'</span>],</span><br><span class="line">	  dtype=<span class="string">'&lt;U7'</span>)</span><br><span class="line">	  </span><br><span class="line"><span class="comment"># 波士顿房价数据提供了13个特征数据，因为是简单线性回归，所以我们只使用房间数量这个特征来预测房价   </span></span><br><span class="line">x = boston.data[:, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据绘制回来</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9cfafd69a87ddcc44ef322682086c423.jpg" alt=""></p>
<p>从图中我们可以看到在顶部有一些似乎到最大值的数据，这是因为在真实的数据中有一些类似50万以上这类数据，都会被归为数据集最大值一类，这些数据对我们的预测不但没有帮助，反而会有影响，所以我们将这些数据去掉：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x[y &lt; <span class="number">50</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入我们封装好的简单线性回归对象和训练/测试数据拆分的对象</span></span><br><span class="line"><span class="keyword">from</span> myML.SimpleLinearRegression <span class="keyword">import</span> SimpleLinearRegression</span><br><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">x_train, y_train, x_test, y_test = train_test_split(x, y, seed=<span class="number">666</span>)</span><br><span class="line">slr = SimpleLinearRegression()</span><br><span class="line">slr.fit(x_train, y_train)</span><br><span class="line">y_train_predict = slr.predict(x_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制出拟合直线</span></span><br><span class="line">plt.scatter(x_train, y_train)</span><br><span class="line">plt.plot(x_train, y_train_predict, color=<span class="string">"r"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c3b54a24eab2e408288f0b29284148c7.jpg" alt=""></p>
<p>下面我们来计算MSE：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_test_predict = slr.predict(x_test)</span><br><span class="line">mse_test = np.sum((y_test - y_test_predict)**<span class="number">2</span>) / len(y_test)</span><br><span class="line">mse_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">24.156602134387402</span></span><br></pre></td></tr></table></figure>
<p>再来计算RMSE：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rmse_test = np.sqrt(mse_test)</span><br><span class="line">rmse_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">4.9149366358466313</span></span><br></pre></td></tr></table></figure>
<p>再来看看MAE：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mae_test = np.sum(np.absolute(y_test - y_test_predict)) / len(y_test)</span><br><span class="line">mae_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3.5430974409463842</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看出RMSE比MAE的值要大，那是因为RMSE是差值先平方然后求和然后再开方，所以如果有某个真实值和预测值之间差距比较大的时候，平方操作就会放大数据的量级。所以一般我们使用RMSE更有实际意义，因为RMSE的值小，说明了最大误差比较小。</p>
<h3 id="u5C01_u88C5MSE_uFF0C_RMSE_uFF0C_MAE"><a href="#u5C01_u88C5MSE_uFF0C_RMSE_uFF0C_MAE" class="headerlink" title="封装MSE， RMSE， MAE"></a>封装MSE， RMSE， MAE</h3><p>我们将这三个衡量指标封装起来，在<code>metrics.py</code>文件中增加三个方法：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/edadd7058cfb2834d5c4e8d262b29765.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> y_true.shape[<span class="number">0</span>] == y_predict.shape[<span class="number">0</span>], \</span><br><span class="line">		<span class="string">"y_true 和 y_predict 数据的行数必须一致"</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.sum(y_true == y_predict) / len(y_predict)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict), <span class="string">"y_true与y_predict的数量必须一致"</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.sum((y_true - y_predict)**<span class="number">2</span>) / len(y_true)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">root_mean_squared_error</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> np.sqrt(mean_squared_error(y_true, y_predict))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_absolute_error</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict), <span class="string">"y_true与y_predict的数量必须一致"</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.sum(np.absolute(y_true - y_predict)) / len(y_true)</span><br></pre></td></tr></table></figure>
<p>这样就可以在Jupyter Notebook中方便的使用了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> myML.metrics <span class="keyword">import</span> root_mean_squared_error</span><br><span class="line"><span class="keyword">from</span> myML.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line">mean_squared_error(y_test, y_test_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">24.156602134387402</span></span><br><span class="line"></span><br><span class="line">root_mean_squared_error(y_test, y_test_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">4.9149366358466313</span></span><br><span class="line"></span><br><span class="line">mean_absolute_error(y_test, y_test_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3.5430974409463842</span></span><br></pre></td></tr></table></figure>
<h2 id="R_Squared"><a href="#R_Squared" class="headerlink" title="R Squared"></a>R Squared</h2><p>在之前的分类问题中，衡量标准的值都是在0/1之间，1表示最好，0表示最差，这个值和量纲无关。但是在线性回归问题中衡量标准的值是带有量纲的，比如某个算法在预测房价的场景中RMSE是5万，但在预测学生分数的场景中RMSE是10分，那么这个算法是在预测房价场景中好呢还是在预测学生分数场景中好呢？这个是无法判断的，这就是RMSE和MAE的局限性。那么为解决这个问题，就出现了一个新的衡量指标R Squared，也就是$R^2$。这个指标也是目前机器学习算法使用比较广泛的一个指标。我们先来看看$R^2$的公式：</p>
<p>$$ R^2 = 1 - \frac {\sum _{i=1}^m(\hat y^{(i)} - y^{(i)})^2} {\sum _{i=1}^m(\bar y  - y^{(i)})^2} $$</p>
<p>这个公式的分子其实就是简单线性回归的模型预测产生的错误，既Loss函数。分母中的$\bar y$是均值，其实均值也是一种线性模型，只不过是比较粗糙的线性模型，所以分母是使用均值模型预测产生的错误。那么如果这个比值远远小于1，说明我们的模型的质量远远超出均值模型，那么$R^2$就无限接近于1，说明我们的模型拟合的非常好。如果比值接近1，说明我们的模型和均值模型没差多少，表明我们的模型比较烂，此时$R^2$就会接近0。那么综上，$R^2$的值小于等于1，值越大表示模型越好，当值为负数时表明我们的模型还不如均值模型，也表明了我们分析的数据之间可能根本就没有线性关系。</p>
<p>我们对$R^2$的公式再处理一下，将1后面的分数分子分母各除以$m$：</p>
<p>$$ R^2 = 1 - \frac {\sum _{i=1}^m(\hat y^{(i)} - y^{(i)})^2} {\sum _{i=1}^m(\bar y  - y^{(i)})^2} = 1 - \frac {(\sum _{i=1}^m(\hat y^{(i)} - y^{(i)})^2)/m} {(\sum _{i=1}^m(\bar y  - y^{(i)})^2)/m} $$</p>
<p>此时，分子其实就是上文中讲到的MSE，而分母就是我们在第二篇笔记中讲过的方差，所以$R^2$又可以写为：</p>
<p>$$ R^2 = 1 - \frac {MSE(\hat y, y)} {Var(y)} $$</p>
<h3 id="u5B9E_u73B0R_Squared"><a href="#u5B9E_u73B0R_Squared" class="headerlink" title="实现R Squared"></a>实现R Squared</h3><p>实现$R^2$其实是非常简单的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r_square = <span class="number">1</span> - mean_squared_error(y_test, y_test_predict)/np.var(y_test)</span><br><span class="line">r_square</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.6129316803937328</span></span><br></pre></td></tr></table></figure>
<p>我们同样将$R^2$的方法封装起来：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/75c9953456143a71552806bc66e369d6.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_score</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> y_true.shape[<span class="number">0</span>] == y_predict.shape[<span class="number">0</span>], \</span><br><span class="line">		<span class="string">"y_true 和 y_predict 数据的行数必须一致"</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.sum(y_true == y_predict) / len(y_predict)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict), <span class="string">"y_true与y_predict的数量必须一致"</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.sum((y_true - y_predict)**<span class="number">2</span>) / len(y_true)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">root_mean_squared_error</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> np.sqrt(mean_squared_error(y_true, y_predict))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_absolute_error</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> len(y_true) == len(y_predict), <span class="string">"y_true与y_predict的数量必须一致"</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> np.sum(np.absolute(y_true - y_predict)) / len(y_true)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">r2_score</span><span class="params">(y_true, y_predict)</span>:</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">1</span> - mean_squared_error(y_true, y_predict) / np.var(y_true)</span><br></pre></td></tr></table></figure>
<p>这样就可以在Jupyter Notebook中方便的使用了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2_score = r2_score(y_test, y_test_predict)</span><br><span class="line">r2_score</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.6129316803937328</span></span><br></pre></td></tr></table></figure>
<p>Scikit Learn中的$R^2$用法也是一样的:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">r2_score(y_test, y_test_predict)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.6129316803937328</span></span><br></pre></td></tr></table></figure>
<h2 id="u591A_u5143_u7EBF_u6027_u56DE_u5F52"><a href="#u591A_u5143_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>上面我们讲了简单线性回归，也就是只关注样本数据的一个特征，这一节我们来看看多元线性回归，既关注样本数据的多个特征。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/b55f7c48e5be4a651545246d79ba4441.jpg" alt=""></p>
<p>像上图中展示的，简单线性回归我们只关注$x^{(i)}$一个特征，假如$x^{(i)}$是一个特征向量，那此时就变成了关注多个特征的多元线性回归问题，如下图所示：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/50af80aeb0622679748071ac2390f042.jpg" alt=""></p>
<p>此时相对与简单线性回归的公式$y=ax+b$，因为$x$成为了特征行向量，所以我们将$a$也看作特征系数列向量，那么公式展开后可以写成这样：</p>
<p>$$ y = a_1x_1+a_2x_2+…+a_nx_n+b $$</p>
<p>我们按惯例，将多元线性回归公式中的特征系数称为$\theta$：</p>
<p>$$ y = \theta_1x_1+\theta_2x_2+…+\theta_nx_n+\theta _0 $$</p>
<p>那么描述每行样本数据中每个特征和其预测值的公式为：</p>
<p>$$ \hat y^{(i)} = \theta _0+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+…+\theta_nX_n^{(i)} $$</p>
<p>我们对上面的公式再进一步做一下处理，将截距$\theta_ 0$也乘以一个特征$X_0^{(i)}$，不过该特征值恒等于1:</p>
<p>$$ \hat y^{(i)} = \theta _0X_0^{(i)}+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+…+\theta_nX_n^{(i)} $$</p>
<p>我们将上面公式用矩阵的方式表示为（用到矩阵相乘的知识，1行n列矩阵乘以n行1列矩阵为1行1列矩阵，既一个标量）：</p>
<p>$$ \hat y^{(i)} = \begin{bmatrix} X_0^{(i)}&amp; X_1^{(i)}&amp;X_2^{(i)}&amp; … &amp;X_n^{(i)}\end{bmatrix}\begin{bmatrix}<br> \theta_ 0 \\<br> \theta_ 1 \\<br> \theta_ 2 \\<br> … \\<br> \theta_ n \\<br>\end{bmatrix} $$</p>
<p>我们可以直接将上面公式表示为：</p>
<p>$$ \hat y = X_b \theta $$</p>
<p>我们知道简单线性回归就是使$\sum_{i=1}^m(y^{(i)}-\hat y^{(i)})^2$尽可能小，那多元线性回归也是一样的，只不过多元线性回归中的$\hat y^{(i)}$的公式不一样而已，将上面的公式代入：</p>
<p>$$\sum_{i=1}^m(y-X_b \theta)^2$$</p>
<p>将上面公式里的平方展开：</p>
<p>$$\sum_{i=1}^m(y-X_b \theta)(y-X_b \theta)$$</p>
<p>矩阵的相乘求和可以转换为如下形式（矩阵或向量的相同元素相乘再求和等于该向量或矩阵的转置乘以该向量或矩阵）：</p>
<p>$$(y-X_b \theta)^T(y-X_b \theta)$$</p>
<p>所以多元线性回归就是求使上面公式尽可能小的$\theta$列向量。</p>
<p>上面的公式出现了矩阵乘积转置，看看下面这张手记就能明白如何转换矩阵乘积转置了：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/af676eb4086d1d6c3aacbccc84c4f028.jpg" alt=""></p>
<p>如此，上面的公式可以转换为：</p>
<p>$$ F_{loss}=(y^T-\theta^T X_b^T)(y-X_b \theta)  = y^Ty-y^TX_b\theta-\theta^TX_b^Ty+\theta^TX_b^TX_b\theta$$</p>
<p>要求$F_{loss}$函数的最小值既对该函数中的每项$theta$求偏导数：</p>
<p>$$\frac {\partial F_{loss}}{\partial \theta}=\frac {\partial y^Ty}{\partial \theta}-\frac {\partial y^TX_b\theta}{\partial \theta}-\frac {\partial \theta^TX_b^Ty}{\partial \theta}+\frac {\partial \theta^TX_b^TX_b\theta}{\partial \theta}$$</p>
<p>这里用到了矩阵求导的知识，具体可见下面的手记：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8b9e1069e189ca944744cb286b0e61ce.jpg" alt=""></p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9a7cbfc5f10db88e5cef682e79ece6f7.jpg" alt=""></p>
<p>上面两张手记简要介绍了矩阵求导的一种方法，根据求导的类型，然后确定是分子布局还是分母布局，最后查阅常用的<a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="external">矩阵求导表</a>确定结果。</p>
<ul>
<li>$\frac {\partial y^Ty}{\partial \theta}$：分子是标量，分母是列向量，是标量/向量类型，并且是分母布局，查表满足$\frac{\partial a}{\partial \mathbf{x}}$条件，故可得结果为0。</li>
<li>$\frac {\partial y^TX_b\theta}{\partial \theta}$：分子是标量，分母是列向量，是标量/向量类型，并且是分母布局，查表满足${\frac  {\partial {\mathbf  {b}}^{\top }{\mathbf  {A}}{\mathbf  {x}}}{\partial {\mathbf  {x}}}}$条件，故可得结果为$X_b^Ty$。</li>
<li>$\frac {\partial \theta^TX_b^Ty}{\partial \theta}$：分子是矩阵，分母是列向量，是矩阵/向量类型，并且是分母布局，但是根据矩阵乘积转置的规则，可以将分子转换为$\frac {\partial y^TX_b\theta}{\partial \theta}$，和第二项偏导表达式相同，故结果为$X_b^Ty$。</li>
<li>$\frac {\partial \theta^TX_b^TX_b\theta}{\partial \theta}$：分子为标量，分布为列向量，是标量/向量类型，并且是分母布局，因为$X_b^TX_b$还是矩阵，所以查表满足${\frac  {\partial {\mathbf  {x}}^{\top }{\mathbf  {A}}{\mathbf  {x}}}{\partial {\mathbf  {x}}}}$条件，故可得结果为$2X_b^TX_b\theta$。</li>
</ul>
<p>所以$F_{loss}$函数的最小值为：</p>
<p>$$F(min)_{loss}=\frac {\partial F_{loss}}{\partial \theta}=\frac {\partial y^Ty}{\partial \theta}-\frac {\partial y^TX_b\theta}{\partial \theta}-\frac {\partial \theta^TX_b^Ty}{\partial \theta}+\frac {\partial \theta^TX_b^TX_b\theta}{\partial \theta}=0-2X_b^Ty+2X_b^TX_b\theta$$</p>
<p>便可求得$\theta$的值为：</p>
<p>$$-2X_b^Ty+2X_b^TX_b\theta=0$$<br>$$\theta=\frac {X_b^Ty}{X_b^TX_b}$$</p>
<p>根据逆矩阵的转换规则可得：</p>
<p>$$\theta=(X_b^TX_b)^{-1}X_b^Ty$$</p>
<p>上面的公式就是多元线性回归的正规方程解 (Normal Equation)。</p>
<h3 id="u4F7F_u7528_u6B63_u89C4_u65B9_u7A0B_u89E3_u5B9E_u73B0_u591A_u5143_u7EBF_u6027_u56DE_u5F52"><a href="#u4F7F_u7528_u6B63_u89C4_u65B9_u7A0B_u89E3_u5B9E_u73B0_u591A_u5143_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="使用正规方程解实现多元线性回归"></a>使用正规方程解实现多元线性回归</h3><p>在实现之前，我们先明确一下$\theta$，它是一个列向量：</p>
<p>$$\theta=\begin{bmatrix}<br> \theta_ 0\<br> \theta_ 1\<br> \theta_ 2\<br> …\<br> \theta_ n\<br>\end{bmatrix}$$</p>
<p>其中$\theta_0$是多元线性方程的截距（intercept），$\theta_1$到$\theta_n$才是系数（coefficients）。</p>
<p>在PyCharm中，我们创建一个类<code>LinearRegression</code>，然后构造函数如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="comment"># 截距theta0</span></span><br><span class="line">		self.intercept_ = <span class="keyword">None</span></span><br><span class="line">		<span class="comment"># 系数，theta1 ... thetaN</span></span><br><span class="line">		self.coef_ = <span class="keyword">None</span></span><br><span class="line">		<span class="comment"># theta列向量</span></span><br><span class="line">		self._theta = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">"LinearRegression()"</span></span><br></pre></td></tr></table></figure>
<p>然后我们来看训练的过程，既<code>fit</code>方法，这里因为是用正规方程解实现的，所以我们将训练方法的名称取为<code>fit_normal()</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_normal</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">	<span class="comment"># 根据训练数据集X_train，y_train训练LinearRegression模型</span></span><br><span class="line">	<span class="keyword">assert</span> X_train.shape[<span class="number">0</span>] == y_train.shape[<span class="number">0</span>], \</span><br><span class="line">		<span class="string">"特征数据矩阵的行数要等于样本结果数据的行数"</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 计算X_b矩阵，既将X_train矩阵前面加一列，元素都为一</span></span><br><span class="line">	X_b = np.hstack([np.ones((len(X_train), <span class="number">1</span>)), X_train])</span><br><span class="line">	<span class="comment"># 实现正规方式解</span></span><br><span class="line">	self._theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 取到截距和系数</span></span><br><span class="line">	self.intercept_ = self._theta[<span class="number">0</span>]</span><br><span class="line">	self.coef_ = self._theta[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> self</span><br></pre></td></tr></table></figure>
<p>训练的实现过程很简单，首先求出<code>X_b</code>，也就是将<code>X_train</code>矩阵前面填加元素为1的一列，这里我们使用到了Numpy的<code>np.hstack</code>方法，也就是在水平方向组合矩阵，另外使用了<code>np.ones</code>这个快捷创建元素为1的矩阵的方法。然后实现了上文中推导出的正规方程解，其中<code>np.linalg.inv</code>是对矩阵求逆的方法。</p>
<p>然后来看预测的过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定待预测数据集X_predict，返回表示X_predict的结果向量</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_predict)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> self.intercept_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.coef_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">		<span class="string">"截距和系数都不为空，表示已经经过了fit方法"</span></span><br><span class="line">		<span class="keyword">assert</span> X_predict.shape[<span class="number">1</span>] == len(self.coef_), \</span><br><span class="line">		<span class="string">"要预测的特征数据集列数要与theta的系数数量相等"</span></span><br><span class="line"></span><br><span class="line">		X_b = np.hstack([np.ones((len(X_predict), <span class="number">1</span>)), X_predict])</span><br><span class="line">		<span class="keyword">return</span> X_b.dot(self._theta)</span><br></pre></td></tr></table></figure>
<p>首先有两个断言增加健壮性，然后同样是计算出<code>X_b</code>，最后乘以训练出的$\theta$就得到了预测的结果。</p>
<p>最后同样增加一个评分的方法，使用我们之前实现的$R^2$方法来计算评分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据测试数据集X_test和y_test确定当前模型的准确度</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self, X_test, y_test)</span>:</span></span><br><span class="line">		y_predict = self.predict(X_test)</span><br><span class="line">		<span class="keyword">return</span> r2_score(y_test, y_predict)</span><br></pre></td></tr></table></figure>
<h3 id="u5728Jupyter_Notebook_u4E2D_u4F7F_u7528LinearRegression"><a href="#u5728Jupyter_Notebook_u4E2D_u4F7F_u7528LinearRegression" class="headerlink" title="在Jupyter Notebook中使用LinearRegression"></a>在Jupyter Notebook中使用LinearRegression</h3><p>我们使用Scikit Learn中提供的波士顿房价数据来验证我们实现的基于正规方程解的多元线性回归。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line">X = X[y &lt; <span class="number">50.0</span>]</span><br><span class="line">y = y[y &lt; <span class="number">50.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分训练数据和测试数据</span></span><br><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y, seed=<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用我们实现的类</span></span><br><span class="line"><span class="keyword">from</span> myML.LinearRegression <span class="keyword">import</span> LinearRegression</span><br><span class="line">reg = LinearRegression()</span><br><span class="line">reg.fit_normal(X_train, y_train)</span><br><span class="line"></span><br><span class="line">reg.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ -<span class="number">1.02162853e-01</span>,   <span class="number">2.51989824e-02</span>,  -<span class="number">4.45146218e-02</span>,</span><br><span class="line">		-<span class="number">1.71466010e-01</span>,  -<span class="number">1.17374943e+01</span>,   <span class="number">4.01098742e+00</span>,</span><br><span class="line">		-<span class="number">2.93108282e-02</span>,  -<span class="number">1.12226201e+00</span>,   <span class="number">2.34868501e-01</span>,</span><br><span class="line">		-<span class="number">1.30958162e-02</span>,  -<span class="number">8.32044126e-01</span>,   <span class="number">8.27684963e-03</span>,</span><br><span class="line">		-<span class="number">3.18223340e-01</span>])</span><br><span class="line">		</span><br><span class="line">reg.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">29.513591626754184</span></span><br><span class="line"></span><br><span class="line">reg.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80030886154058956</span></span><br></pre></td></tr></table></figure>
<p>我们再来看看Scikit Learn中提供的线性回归的使用方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">lin_reg.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ -<span class="number">1.02162853e-01</span>,   <span class="number">2.51989824e-02</span>,  -<span class="number">4.45146218e-02</span>,</span><br><span class="line">		-<span class="number">1.71466010e-01</span>,  -<span class="number">1.17374943e+01</span>,   <span class="number">4.01098742e+00</span>,</span><br><span class="line">		-<span class="number">2.93108282e-02</span>,  -<span class="number">1.12226201e+00</span>,   <span class="number">2.34868501e-01</span>,</span><br><span class="line">		-<span class="number">1.30958162e-02</span>,  -<span class="number">8.32044126e-01</span>,   <span class="number">8.27684963e-03</span>,</span><br><span class="line">		-<span class="number">3.18223340e-01</span>])</span><br><span class="line">		</span><br><span class="line">lin_reg.intercept_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">29.513591626752053</span></span><br><span class="line"></span><br><span class="line">lin_reg.score(X_test, y_test)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.80030886154069325</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到使用Scikit Learn中的线性回归和我们实现的线性回归结果是一样的，但这里要注意的是Scikit Learn中的线性回归实现方式并不是正规方程解，只是因为样本数据量比较少，所以得到了相同的结果。下一篇笔记会介绍另外一种实现多元线性回归的方式。</p>
<h2 id="u7EBF_u6027_u56DE_u5F52_u7684_u53EF_u89E3_u91CA_u6027"><a href="#u7EBF_u6027_u56DE_u5F52_u7684_u53EF_u89E3_u91CA_u6027" class="headerlink" title="线性回归的可解释性"></a>线性回归的可解释性</h2><p>我们将全量的波士顿房价进行预测，求出系数来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line">lin_reg.coef_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ -<span class="number">1.05574295e-01</span>,   <span class="number">3.52748549e-02</span>,  -<span class="number">4.35179251e-02</span>,</span><br><span class="line">		 <span class="number">4.55405227e-01</span>,  -<span class="number">1.24268073e+01</span>,   <span class="number">3.75411229e+00</span>,</span><br><span class="line">		-<span class="number">2.36116881e-02</span>,  -<span class="number">1.21088069e+00</span>,   <span class="number">2.50740082e-01</span>,</span><br><span class="line">		-<span class="number">1.37702943e-02</span>,  -<span class="number">8.38888137e-01</span>,   <span class="number">7.93577159e-03</span>,</span><br><span class="line">		-<span class="number">3.50952134e-01</span>])</span><br></pre></td></tr></table></figure>
<p>我们看到系数有正有负，表示了正相关和负相关，既正系数越大的特征对结果的影响越大，负系数越小对结果影响越大，我们来看看影响波士顿房价的特征都是什么，先将系数按索引从小到大排序：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.argsort(lin_reg.coef_)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">4</span>,  <span class="number">7</span>, <span class="number">10</span>, <span class="number">12</span>,  <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">6</span>,  <span class="number">9</span>, <span class="number">11</span>,  <span class="number">1</span>,  <span class="number">8</span>,  <span class="number">3</span>,  <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按索引顺序排列出对应的房价特征</span></span><br><span class="line">boston.feature_names[np.argsort(lin_reg.coef_)]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="string">'NOX'</span>, <span class="string">'DIS'</span>, <span class="string">'PTRATIO'</span>, <span class="string">'LSTAT'</span>, <span class="string">'CRIM'</span>, <span class="string">'INDUS'</span>, <span class="string">'AGE'</span>, <span class="string">'TAX'</span>,</span><br><span class="line">	   <span class="string">'B'</span>, <span class="string">'ZN'</span>, <span class="string">'RAD'</span>, <span class="string">'CHAS'</span>, <span class="string">'RM'</span>],</span><br><span class="line">	  dtype=<span class="string">'&lt;U7'</span>)</span><br></pre></td></tr></table></figure>
<p>通过<code>boston.DESCR</code>我们可以知道正系数最大的特征是RM，既房屋面积，第二大的正系数特征是CHAS，既房屋临Charles河的距离，离河越近房屋价格越高，从这两项都可以看出这是合理的情况。再来看看最小的系数特征NOX，它的系数是负的，这个特征是房屋周围的一氧化碳浓度，浓度约小房价越高，可见这都是合理的。综上，就解释了线性回归的可解释性，它可以让我们更好的采集收集样本数据，提高数据质量，提升预测准确性。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>这篇笔记介绍了线性回归和多元线性回归的概念、公式推导、评价标准以及实现。是解决一些预测类问题的基本知识。下篇笔记将学习机器学习算法中重要的一个算法梯度下降。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记主要介绍线性回归算法，对在第一篇笔记中介绍过的线性回归算法进行实现。kNN算法主要解决的是分类问题，并且它的结果不具备良好的解释性。线性回归算法主要解决回归问题，它的结果具有良好的可解释性，和kNN算法的介绍过程一样，线性回归算法也蕴含了机器学习中很多重要的思想，并且它是许多强大的非线性模型的基础。</p>
<h2 id="u7B80_u5355_u7EBF_u6027_u56DE_u5F52"><a href="#u7B80_u5355_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="简单线性回归"></a>简单线性回归</h2><p>在第一篇笔记中，我们举过房屋面积大小和价格例子，将其绘制在二维坐标图上，横轴表示房屋面积，纵轴表示房屋价格，这里样本特征数据只有一个，那就是房屋面积，而在kNN算法的分类问题中，二维坐标图上横纵轴表示的都是样本特征数据，这是一个比较明显的区别。如果线性回归问题要在图中表示两种样本特征数据的话就需要三维空间坐标来表示。我们一般将只有一种样本特征数据的线性回归问题称为简单线性回归问题。</p>
<h3 id="u56DE_u987E_u7EBF_u6027_u56DE_u5F52"><a href="#u56DE_u987E_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="回顾线性回归"></a>回顾线性回归</h3><p>线性回归其实就是寻找一条直线，最大程度的拟合样本特征和样本输出标记之间的关系。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a9e2a5cbcffbf0037e8bf53f49cc414e.jpg" alt=""></p>
<p>我们来看上面这张图，大家知道直线的方程是$y=ax+b$，那么点A肯定是在一条直线上，该条直线方程为$y^{(i)}=ax^{(i)}+b$，那么点A的横轴值为$x^{(i)}$，既是A的样本特征值，纵轴值为$y^{(i)}$，既是A的样本输出值。我们假设图中的红线就是拟合直线，方程为$\hat y^{(i)}=ax^{(i)}+b$，也就是将$x^{(i)}$代入这条红线，会得到一个预测的纵轴值$\hat y^{(i)}$。我们希望真值$y^{(i)}$和预测值$\hat y^{(i)}$的差值越小，说明我们的拟合直线拟合的越好。</p>
<p>因为差值有正有负，为了保证都是正数，所以将差值进行平方，之所以不用绝对值，是为了方便求导数。</p>
<blockquote>
<p>方程求导的知识可参阅<a href="http://www.devtalking.com/articles/machine-learning-1/"> 《机器学习笔记一之机器学习定义、导数、最小二乘》 </a>。</p>
</blockquote>
<p>$$ (y^{(i)} - \hat y^{(i)})^2 $$</p>
<p>将所有样本特征都考虑到，既将所有真值和预测值的差值求和：</p>
<p>$$ \sum_{i=1}^m(y^{(i)} - \hat y^{(i)})^2 $$</p>
<p>将$ax^{(i)}+b$代入上面的公式就得到：</p>
<p>$$\sum_{i=1}^m(y^{(i)} - ax^{(i)}-b)^2$$</p>
<p>上面的公式我们称为损失函数（Loss Function），损失函数值越小，我们的拟合直线越好。在Loss函数中，$a$和$b$是变量，所以我们要做的就是找到使Loss函数值最小的$a$和$b$。这个套路是近乎所有参数学习算法常用的套路，既通过分析问题，确定问题的损失函数，通过最优化损失函数获得机器学习的模型。像线性回归、多项式回归、逻辑回归、SVM、神经网络等都是这个套路。</p>]]>
    
    </summary>
    
      <category term="MAE" scheme="http://www.devtalking.com/tags/MAE/"/>
    
      <category term="MSE" scheme="http://www.devtalking.com/tags/MSE/"/>
    
      <category term="R Squared" scheme="http://www.devtalking.com/tags/R-Squared/"/>
    
      <category term="RMSE" scheme="http://www.devtalking.com/tags/RMSE/"/>
    
      <category term="多元线性回归" scheme="http://www.devtalking.com/tags/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记四之kNN算法、超参数、数据归一化]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-4/"/>
    <id>http://www.devtalking.com//articles/machine-learning-4/</id>
    <published>2018-01-26T16:00:00.000Z</published>
    <updated>2018-08-27T02:29:34.355Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Notebook中调用，以及计算器算法的封装规范，kNN的<code>k</code>值如何计算，如何使用Scikit Learn中的kNN算法，还有机器学习算法中的一些主要概念，比如训练数据集、测试数据集，分类准确度，超参数，数据归一化。另外会具体用代码实现第一篇笔记中介绍过的线性回归算法。</p>
<h2 id="u5C01_u88C5kNN_u7B97_u6CD5"><a href="#u5C01_u88C5kNN_u7B97_u6CD5" class="headerlink" title="封装kNN算法"></a>封装kNN算法</h2><p>上一篇笔记中我们对kNN算法在Jupyter Notebook中进行了实现，但是想要复用这个算法就很不方便，所以我们来看看如何在PyCharm中封装算法，并且在Jupyter Notebook中进行调用。</p>
<p>PyCharm的配置这里我就不再累赘，如图所示，我们创建了一个Python文件<code>kNN.py</code>，然后定义了<code>kNNClassify</code>方法，该方法有4个参数，分别是kNN算法的<code>k</code>值，训练样本特征数据集<code>XTrain</code>，训练样本类别数据集<code>yTrain</code>，预测特征数据集<code>x</code>。该方法中的实现和在Jupyter Notebook中实现的一模一样，只不过加了三个断言，让方法的健壮性更好一点。我们给出<strong>N维欧拉定理</strong>：</p>
<p>$$ \sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} $$</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/574075757e0d0a820e2a8a7ef37f79ec.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kNN.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNNClassify</span><span class="params">(k, XTrain, yTrain, x)</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> <span class="number">1</span> &lt;= k &lt;= XTrain.shape[<span class="number">0</span>], <span class="string">"k 的取值范围不正确"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] == yTrain.shape[<span class="number">0</span>], <span class="string">"训练样本数据行数应该与训练结果集行数相同"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">1</span>] == x.shape[<span class="number">0</span>], <span class="string">"训练样本数据特性个数应该与被预测数据特性个数相同"</span></span><br><span class="line"></span><br><span class="line">	distances = [sqrt(np.sum((xTrain - x) ** <span class="number">2</span>)) <span class="keyword">for</span> xTrain <span class="keyword">in</span> XTrain]</span><br><span class="line">	nearest = np.argsort(distances)</span><br><span class="line"></span><br><span class="line">	topKy = [yTrain[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:k]]</span><br><span class="line">	votes = Counter(topKy)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这样我们就在PyCharm中封装好了kNN算法的方法，我们再来看看如何在Jupyter Notebook中调用封装好的方法呢，这就需要使用<code>%run</code>这个命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">raw_data_X = [[<span class="number">3.393533211</span>, <span class="number">2.331273381</span>],</span><br><span class="line">			  [<span class="number">3.110073483</span>, <span class="number">1.781539638</span>],</span><br><span class="line">			  [<span class="number">1.343808831</span>, <span class="number">3.368360954</span>],</span><br><span class="line">			  [<span class="number">3.582294042</span>, <span class="number">4.679179110</span>],</span><br><span class="line">			  [<span class="number">2.280362439</span>, <span class="number">2.866990263</span>],</span><br><span class="line">			  [<span class="number">7.423436942</span>, <span class="number">4.696522875</span>],</span><br><span class="line">			  [<span class="number">5.745051997</span>, <span class="number">3.533989803</span>],</span><br><span class="line">			  [<span class="number">9.172168622</span>, <span class="number">2.511101045</span>],</span><br><span class="line">			  [<span class="number">7.792783481</span>, <span class="number">3.424088941</span>],</span><br><span class="line">			  [<span class="number">7.939820817</span>, <span class="number">0.791637231</span>]</span><br><span class="line">			 ]</span><br><span class="line">raw_data_y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">XTrain = np.array(raw_data_X)</span><br><span class="line">yTrain = np.array(raw_data_y)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">8.093607318</span>, <span class="number">3.365731514</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用%run命令可以引入Python文件，并可使用该Python文件中定义的属性和方法</span></span><br><span class="line">%run ../pycharm/kNN.py</span><br><span class="line">predicty = kNNClassify(<span class="number">6</span>, XTrain, yTrain, x)</span><br><span class="line">predicty</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="u673A_u5668_u5B66_u4E60_u6D41_u7A0B"><a href="#u673A_u5668_u5B66_u4E60_u6D41_u7A0B" class="headerlink" title="机器学习流程"></a>机器学习流程</h2><p>这一小节我们来看看机器学习的大概流程是怎样的，如下图所示：<br><img src="http://paxigrdp0.bkt.clouddn.com/13a8bbc81eef52f649e82539248d29ad.jpg" alt=""></p>
<p>监督学习算法首先需要的是训练数据集，然后通过一个机器学习算法生成一个模型，最后就可以用这个模型来预测新的数据得到结果。通常，我们将使用机器学习生成模型的过程用fit来表示，使用模型预测新的数据的过程用predict来表示。这就是机器学习最基本的一个流程。</p>
<p>在第一篇笔记中，介绍了线性回归的概念，我们最后得到了一个二元线性回归的公式：$ F(a,b) = \sum_{i=1}^n(y_i-(ax_i + b))^2 $。这个公式其实就是通过线性回归算法得到的模型，通过fit过程，训练模型得到<code>a</code>，<code>b</code>，然后通过predict过程预测新的样例数据得到结果。</p>
<p>但是我们发现kNN算法不存在训练模型的过程，因为新的样例数据其实是需要通过训练数据集来进行预测的，所以换个角度来看，kNN算法的模型就是它的训练数据集，在上图中模型阶段其实就是把训练数据集复制了一份作为模型来使用，那么对于fit和predict过程而言，kNN算法的predict过程其实是核心，而fit过程非常简单。</p>
<h2 id="u4F7F_u7528Scikit_Learn_u4E2D_u7684kNN_u7B97_u6CD5"><a href="#u4F7F_u7528Scikit_Learn_u4E2D_u7684kNN_u7B97_u6CD5" class="headerlink" title="使用Scikit Learn中的kNN算法"></a>使用Scikit Learn中的kNN算法</h2><p>这一节我们来看看如何使用Scikit Learn中封装的kNN算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入Scikit Learn中的kNN算法的类库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment"># 初始化kNN算法分类器的实例，参数n_neighbors就是k值</span></span><br><span class="line">kNNClassifier = KNeighborsClassifier(n_neighbors=<span class="number">6</span>)</span><br><span class="line"><span class="comment"># 训练，拟合模型</span></span><br><span class="line">kNNClassifier.fit(XTrain, yTrain)</span><br><span class="line"><span class="comment"># 预测新的样例数据，该方法接受的参数类型为二维数组，如果只有一行也需要转换为一行的二维数组</span></span><br><span class="line">kNNClassifier.predict(x.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>从示例代码中可以看出，Scikit Learn中封装的kNN算法严格遵从了上一节介绍的机器学习的基本流程，其实不止是kNN算法，Scikit Learn中的所有机器学习算法都遵从这个基本流程。</p>
<h2 id="u91CD_u65B0_u5C01_u88C5kNN_u7B97_u6CD5"><a href="#u91CD_u65B0_u5C01_u88C5kNN_u7B97_u6CD5" class="headerlink" title="重新封装kNN算法"></a>重新封装kNN算法</h2><p>所以我们可以优化一下我们之前封装的kNN算法的方法，将其封装为类似Scikit Learn中的方式：<br><img src="http://paxigrdp0.bkt.clouddn.com/ab2b99d15c26edf50171a5dbb8e8cecb.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNNClassifier</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 初始化kNN分类器</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> k &gt;= <span class="number">1</span>, <span class="string">"k 值不能小于1"</span></span><br><span class="line"></span><br><span class="line">		self.k = k</span><br><span class="line">		self._XTrain = <span class="keyword">None</span></span><br><span class="line">		self._yTrain = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据训练数据集XTrain和yTrain训练kNN分类器，在kNN中这一步就是复制训练数据集</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, XTrain, yTrain)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] == yTrain.shape[<span class="number">0</span>], \</span><br><span class="line">			<span class="string">"训练样本特征数据集的行数要与训练样本分类结果数据集的行数相同"</span></span><br><span class="line">		<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] &gt;= self.k, \</span><br><span class="line">			<span class="string">"训练样本特征数据集的行数，既样本点的数量要大于等于k值"</span></span><br><span class="line"></span><br><span class="line">		self._XTrain = XTrain</span><br><span class="line">		self._yTrain = yTrain</span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 输入样本数据，根据模型进行预测</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, XPredict)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> self._XTrain <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._yTrain <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">			<span class="string">"在执行predict方法前必须先执行fit方法"</span></span><br><span class="line">		<span class="keyword">assert</span> XPredict.shape[<span class="number">1</span>] == self._XTrain.shape[<span class="number">1</span>], \</span><br><span class="line">			<span class="string">"被预测数据集的特征数，既列数必须与模型数据集中的特征数相同"</span></span><br><span class="line"></span><br><span class="line">		ypredict = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> XPredict]</span><br><span class="line">		<span class="keyword">return</span> np.array(ypredict)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 实现私有的预测方法，kNN算法的核心代码</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> x.shape[<span class="number">0</span>] == self._XTrain.shape[<span class="number">1</span>], \</span><br><span class="line">			<span class="string">"输入的样本数据的特征数量必须等于模型数据，既训练样本数据的特征数量"</span></span><br><span class="line"></span><br><span class="line">		distance = [sqrt(np.sum((xTrain - x) ** <span class="number">2</span>)) <span class="keyword">for</span> xTrain <span class="keyword">in</span> self._XTrain]</span><br><span class="line">		nearest = np.argsort(distance)</span><br><span class="line">		topK = [self._yTrain[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]]</span><br><span class="line">		votes = Counter(topK)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">"kNN(k=%d)"</span> % self.k</span><br></pre></td></tr></table></figure>
<p>上面的代码清晰的定义了<code>fit</code>和<code>predict</code>方法，至于<code>_predict</code>这个私有方法可以随意，可以将逻辑直接写在<code>predict</code>方法里，也可以拆分出来。然后我们在Jupyter Notebook中再来使用一下我们封装的kNN算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%run ../pycharm/kNN/kNN.py</span><br><span class="line">myKNNClassifier = KNNClassifier(<span class="number">6</span>)</span><br><span class="line">myKNNClassifier.fit(XTrain, yTrain)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">kNN(k=<span class="number">6</span>)</span><br><span class="line">xTrain = x.reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">myKNNClassifier.predict(xTrain)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h2 id="u5224_u65AD_u673A_u5668_u5B66_u4E60_u7B97_u6CD5_u7684_u6027_u80FD"><a href="#u5224_u65AD_u673A_u5668_u5B66_u4E60_u7B97_u6CD5_u7684_u6027_u80FD" class="headerlink" title="判断机器学习算法的性能"></a>判断机器学习算法的性能</h2><p>现在大家应该知道机器算法的目的主要是训练出模型，然后输入样本，通过模型来预测结果，可见这个模型是非常关键的，模型的好坏直接影响预测结果的准确性，继而对实际运用会产生巨大的影响。模型的训练除了机器学习算法以外，对它影响比较大的还有训练样本数据，我们在实现kNN算法时，是将所有的样本数据用于训练模型，那么模型训练出来后就已经没有数据供我们验证模型的好坏了，只能直接投入真实环境使用，这样的风险是很大的。</p>
<p>所以为了避免上述这种情况，最简单的做法是将所有训练样本数据进行切分，将大部分数据用于训练模型，而另外一小部分数据用来测试训练出的模型，这样如果我们用测试数据发现这个模型不够好，那么我们就有机会在将模型投入真实环境使用之前改进算法，训练出更好的模型。</p>
<p>我们来看看如何封装拆分训练数据的方法：<br><img src="http://paxigrdp0.bkt.clouddn.com/30144371e3f3e8726b2f83c5cef76564.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练样本数据 X 和 y 按照 test_radio 分割成 X_train, y_train, X_test, y_test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span><span class="params">(X, y, test_radio = <span class="number">0.2</span>, seed = None)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> X.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>], \</span><br><span class="line">		<span class="string">"训练样本特征数据集的行数要与训练样本分类结果数据集的行数相同"</span></span><br><span class="line">	<span class="keyword">assert</span> <span class="number">0.0</span> &lt;= test_radio &lt;= <span class="number">1.0</span>, \</span><br><span class="line">		<span class="string">"test_radio 的值必须在 0 到 1 之间"</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 如果 seed 有值，将其设置进numpy的随机函数中</span></span><br><span class="line">	<span class="keyword">if</span> seed:</span><br><span class="line">		np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">	shuffled_indexes = np.random.permutation(len(X))</span><br><span class="line">	test_size = int(len(X) * test_radio)</span><br><span class="line">	test_indexes = shuffled_indexes[:test_size]</span><br><span class="line">	train_indexes = shuffled_indexes[test_size:]</span><br><span class="line"></span><br><span class="line">	X_train = X[train_indexes]</span><br><span class="line">	y_train = y[train_indexes]</span><br><span class="line"></span><br><span class="line">	X_test = X[test_indexes]</span><br><span class="line">	y_test = y[test_indexes]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> X_train, y_train, X_test, y_test</span><br></pre></td></tr></table></figure>
<p>我们来解读一下上面的代码：</p>
<ul>
<li>首先<code>train_test_split</code>函数有四个参数，两个必填参数，两个非必填有默认值的参数。<code>X</code>是训练样本特征数据集，<code>y</code>是训练样本分类结果数据集，<code>test_radio</code>是设置训练数据和测试数据的比例，<code>seed</code>就很好理解了，就是NumPy的随机函数提供的随机种子机制。</li>
<li>上面代码中有一个方法大家之前应该没见过，那就是<code>permutation(x)</code>，该方法表示返回一个乱序的一维向量，元素从0到x，所以<code>shuffled_indexes</code>是一个乱序的一维向量数组，它的元素总数为训练样本数据的总数，既训练样本数据矩阵的行数，元素的范围从0到训练样本数据的总数。</li>
<li>根据<code>test_radio</code>计算出需要分割出的测试数据数量<code>test_size</code>。</li>
<li>根据<code>test_size</code>从<code>shuffled_indexes</code>中取出<code>test_indexes</code>和<code>train_indexes</code>，这两个数组中存的元素就是作为索引来用的。</li>
<li>根据<code>test_indexes</code>和<code>train_indexes</code>从<code>X</code>和<code>y</code>中得到<code>X_train</code>、<code>y_train</code>、<code>X_test</code>、<code>y_test</code>。</li>
</ul>
<p>之前在Jupyter Notebook中我们使用<code>%run</code>命令使用我们封装的代码 ，这一节我们来看看如何使用<code>import</code>的方式使用我们自己封装的代码。其实这和Jupyter Notebook没多大关系，我们需要做的只是给Python设置一个搜索包的路径而已，这里这会对MacOS，以及安装了Anaconda的环境作以说明，Windows系统大同小异。</p>
<p>首先找到路径<code>/anaconda3/lib/python3.6/site-packages</code>，在该路径下创建一个文件<code>XXX.pth</code>，该文件的扩展名必须为<code>pth</code>，文件名称可以随意。然后在该文件中输入你希望Python搜索包的绝对路径即可。</p>
<p>设置完搜索路径后，我们需要修改一下PyCharm中的目录结构：<br><img src="http://paxigrdp0.bkt.clouddn.com/42bf4dc0e2e8cf0d11afd17037d16b59.jpg" alt=""></p>
<p>我新建了一个目录名为<code>myML</code>，<code>kNN.py</code>是我们之前封装的kNN算法相关的方法，<code>modelSelection.py</code>里就是我们刚才封装好的拆分训练和测试数据的方法，另外还增加了一个<code>__init__.py</code>的文件，因为有了这个文件，<code>myML</code>就变为了一个包。<code>__init__.py</code>的作用这里不做过多解释。</p>
<p>这样我们就可以在Jupyter Notebook中用<code>import</code>的方式导入我们封装的模块了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y)</span><br><span class="line">X_train.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">120</span>, <span class="number">4</span>)</span><br><span class="line">y_train.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">120</span>,)</span><br><span class="line">X_test.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">30</span>, <span class="number">4</span>)</span><br><span class="line">y_test.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">30</span>,)</span><br></pre></td></tr></table></figure>
<p>这样就可以很方便的使用我们封装的模块了，下面我们来看看怎么判断我们封装的kNN算法的好坏程度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先用训练数据训练模型，然后输入测试样本特征数据，得到预测结果</span></span><br><span class="line"><span class="keyword">from</span> myML.kNN <span class="keyword">import</span> KNNClassifier</span><br><span class="line">my_knn_classifier = KNNClassifier(<span class="number">6</span>)</span><br><span class="line">my_knn_classifier.fit(X_train, y_train)</span><br><span class="line">my_y_test = my_knn_classifier.predict(X_test)</span><br><span class="line">my_y_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">	   <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 用预测出的结果和测试样本分类结果数据做对比，得出准确率</span></span><br><span class="line">y_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">	   <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">	   </span><br><span class="line">sum(my_y_test == y_test) / len(y_test)</span><br><span class="line"><span class="comment"># 结果，准确率为96.67%</span></span><br><span class="line"><span class="number">0.96666666666666667</span></span><br></pre></td></tr></table></figure>
<p>这样我们就得出了一个算法的好坏程度。</p>
<h2 id="u8D85_u53C2_u6570"><a href="#u8D85_u53C2_u6570" class="headerlink" title="超参数"></a>超参数</h2><p>目前我们在使用kNN算法时，<code>k</code>的值都是我们给定的值，这个作为算法的参数值称为超参数，也就是在运行机器学习算法之前需要指定的参数。还有一类参数称为模型参数，既在算法过程中学习的参数，但是大家已经知道kNN算法实际是没有模型的，所以也不存在模型参数，但是<code>k</code>值是一个典型的超参数。</p>
<h3 id="u5BFB_u627E_u6700_u597D_u7684k_u503C"><a href="#u5BFB_u627E_u6700_u597D_u7684k_u503C" class="headerlink" title="寻找最好的k值"></a>寻找最好的k值</h3><p>Scikit Learn中kNN算法的<code>k</code>值默认是5，有时候这个值并不是最优的值，那么我们可以通过一个简单的方式来寻找到最优的<code>k</code>值，那就是给定一个<code>k</code>值的范围，然后循环传入算法求训练分数最好的那个<code>k</code>值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先我们使用scikit learn中的手写数字数据集，并将其拆分为训练数据集和测试数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后通过循环的方式寻找最好的k值</span></span><br><span class="line">best_score = <span class="number">0.0</span></span><br><span class="line">best_k = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">	knn_clf = KNeighborsClassifier(n_neighbors = k)</span><br><span class="line">	knn_clf.fit(X_train, y_train)</span><br><span class="line">	score = knn_clf.score(X_test, y_test)</span><br><span class="line">	<span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">		best_k = k</span><br><span class="line">		best_score = score</span><br><span class="line">		</span><br><span class="line">print(<span class="string">"best_k = "</span>, best_k)</span><br><span class="line">print(<span class="string">"best_score = "</span>, best_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">best_k =  <span class="number">4</span></span><br><span class="line">best_score =  <span class="number">0.991666666667</span></span><br></pre></td></tr></table></figure>
<p>从上面的代码示例中可以看到，在1到10这个范围的<code>k</code>值中，4是训练分数最高的<code>k</code>值。不过这里需要注意的是，如果求出<code>k</code>为10，那么我们需要再扩大范围进行寻找，因为有可能10并不是最优的<code>k</code>值，只因为我们给定的范围最大到10，所以这种情况下，我们需要根据实际情况对8至20的<code>k</code>值范围再进行计算，如果结果仍然为10，那么才认定10为最优<code>k</code>值。</p>
<h3 id="u8DDD_u79BB_u7684_u6743_u91CD"><a href="#u8DDD_u79BB_u7684_u6743_u91CD" class="headerlink" title="距离的权重"></a>距离的权重</h3><p><img src="http://paxigrdp0.bkt.clouddn.com/b29f73d52a86bfcc04a919471f6e730f.jpg" alt=""></p>
<p>上面这张图如果用之前我们了解过的kNN算法来分析的话，绿色的点肯定是属于蓝色点分类的，但是我们之前都一直忽略了一个问题，<strong>那就是当找到<code>k</code>个相邻的点后，在投票时是没有再考虑未知分类点与相邻点之间的距离的</strong>。就比如上图，如果考虑了3个最近相邻点与绿色点之间的距离的话，那么绿色点的分类就会属于红色点的分类，因为在计算距离权重时是取距离的倒数，所以绿色点与红色点的距离权重为1，绿色点与两个蓝色点的距离权重为1/3 + 1/4 = 7/12。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/53a1c3214fa97bb5c905e80898cbad63.jpg" alt=""></p>
<p>上图的情况如果不考虑距离权重的话，就会出现平票的情况，那么只能随机在三个分类中选一个作为绿色点的分类，如果加上距离权重，就能确定得出绿色点的分类了。</p>
<p>所以与相邻点的距离权重是kNN算法的另一个重要的超参数，大家可以看一下Scikit Learn的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank" rel="external">kNN</a>官网，<code>KNeighborsClassifier</code>的构造函数中有一个参数<code>weights</code>，这就是距离权重参数，默认值为<code>uniform</code>，既不考虑距离权重，如果要考虑距离权重的话，需要设置值为<code>distance</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(n_neighbors = <span class="number">4</span>, weights = <span class="string">'distance'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="u8DDD_u79BB_u7684_u7C7B_u578B"><a href="#u8DDD_u79BB_u7684_u7C7B_u578B" class="headerlink" title="距离的类型"></a>距离的类型</h3><p>到目前为止，我们自己封装的kNN算法使用的距离公式是欧拉距离，其实还有其他的距离公式，比如<strong>曼哈顿距离</strong>:</p>
<p>$$\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|$$</p>
<p>其实曼哈顿距离和欧拉距离在数学公式表现形式上是有一定相似性的，我们可以将欧拉距离做以转换：</p>
<p>$$ \sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} = \sqrt {\sum_{i=1}^n|x_i^{(a)}-x_i^{(b)}|^2} = (\sum_{i=1}^n|x_i^{(a)}-x_i^{(b)}|^2)^\frac 1 2 $$ </p>
<p>对曼哈顿距离也做以转换：</p>
<p>$$ \sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}| = （\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|^1）^\frac 1 1 $$</p>
<p>通过上面两个公式可以得到一个共性的公式：</p>
<p>$$ （\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|^p）^\frac 1 p $$</p>
<p>这个公式就称之为<strong>明可夫斯基距离（Minkowski Distance）</strong>。</p>
<p>既当<code>p</code>为1时为曼哈顿距离，当<code>p</code>为2时为欧拉距离，当<code>p</code>大于2时表示其他距离，所以<code>p</code>又是一个kNN算法的超参数，在<code>KNeighborsClassifier</code>的构造函数中同样有一个参数<code>p</code>就是表示使用的距离类型，默认为2，既默认为欧拉距离。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_score = <span class="number">0.0</span></span><br><span class="line">best_k = -<span class="number">1</span></span><br><span class="line">best_p = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">	<span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">		knn_clf = KNeighborsClassifier(n_neighbors = k, weights = <span class="string">"distance"</span>, p = p)</span><br><span class="line">		knn_clf.fit(X_train, y_train)</span><br><span class="line">		score = knn_clf.score(X_test, y_test)</span><br><span class="line">		<span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">			best_k = k</span><br><span class="line">			best_score = score</span><br><span class="line">			best_p = p</span><br><span class="line"></span><br><span class="line">print(<span class="string">"best_p = "</span>, best_p)            </span><br><span class="line">print(<span class="string">"best_k = "</span>, best_k)</span><br><span class="line">print(<span class="string">"best_score = "</span>, best_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">best_p =  <span class="number">2</span></span><br><span class="line">best_k =  <span class="number">3</span></span><br><span class="line">best_score =  <span class="number">0.988888888889</span></span><br></pre></td></tr></table></figure>
<p>从上面代码运行的结果来看，最优的<code>p</code>值为2，也就是欧拉距离，考虑了距离权重后，最优<code>k</code>值为3。而且一些超参数是组合使用的，比如当使用超参数<code>p</code>时，距离权重的超参数<code>weights</code>的取值就必须是<code>distance</code>。并且<code>k</code>和<code>p</code>这两个超参数双重嵌套循环，就组成了一个类似网格的搜索方式，所幸Scikit Learn提供了封装好的网格搜索的方法供我们使用。</p>
<h2 id="u7F51_u683C_u641C_u7D22_u8D85_u53C2_u6570"><a href="#u7F51_u683C_u641C_u7D22_u8D85_u53C2_u6570" class="headerlink" title="网格搜索超参数"></a>网格搜索超参数</h2><p>在使用网格搜索前，我们需要先将各种超参数的组合定义出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = [</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="string">'weights'</span>: [<span class="string">'uniform'</span>],</span><br><span class="line">		<span class="string">'n_neighbors'</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>)]</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="string">'weights'</span>: [<span class="string">'distance'</span>],</span><br><span class="line">		<span class="string">'n_neighbors'</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>)],</span><br><span class="line">		<span class="string">'p'</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">	&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>我们定义了一个<code>param_grid</code>数组，元素为字典，每个字典描述了一种超参数的组合，下面我们使用Scikit Learn提供的<code>GridSearchCV</code>来使用我们定义好的超参数组合：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line">new_knn_clf = grid_search.best_estimator_</span><br><span class="line">new_knn_clf</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">KNeighborsClassifier(algorithm=<span class="string">'auto'</span>, leaf_size=<span class="number">30</span>, metric=<span class="string">'minkowski'</span>,</span><br><span class="line">		   metric_params=<span class="keyword">None</span>, n_jobs=<span class="number">1</span>, n_neighbors=<span class="number">3</span>, p=<span class="number">3</span>,</span><br><span class="line">		   weights=<span class="string">'distance'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的示例代码不难理解，我们使用构建出的kNN分类器<code>knn_clf</code>和超参数组合<code>param_grid</code>构造出了网格搜索对象<code>grid_search</code>，通过它进行<code>fit</code>操作，这个过程就是根据我们提供的超参数组合进行搜寻，找到最优的超参数组合。通过<code>best_estimator_</code>返回新的，已经设置了最优超参数组合的kNN分类器对象。从输出结果其实已经可以看到首先是选择了考虑距离权重的超参数组合，然后求出了<code>k</code>值，也就是<code>n_neighbors</code>为3，<code>p</code>值为3。</p>
<p><code>GridSearchCV</code>也提供了几个属性，可以让我们方便的查看超参数和模型评分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid_search.best_params_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">&#123;<span class="string">'n_neighbors'</span>: <span class="number">3</span>, <span class="string">'p'</span>: <span class="number">3</span>, <span class="string">'weights'</span>: <span class="string">'distance'</span>&#125;</span><br><span class="line"></span><br><span class="line">grid_search.best_score_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.98538622129436326</span></span><br></pre></td></tr></table></figure>
<h3 id="GridSearchCV_u7684_u5176_u4ED6_u53C2_u6570"><a href="#GridSearchCV_u7684_u5176_u4ED6_u53C2_u6570" class="headerlink" title="GridSearchCV的其他参数"></a>GridSearchCV的其他参数</h3><p>在构造<code>GridSearchCV</code>对象时除了kNN分类器和超参数组合外，还有几个比较有用的参数：</p>
<ul>
<li><code>n_jobs</code>：该参数决定了在进行网格搜索时使用当前计算机的CPU核数，1就是使用1个核，2就是使用2个核，如果设置为-1，那么代表使用所有的核进行搜索。</li>
<li><code>verbose</code>：该参数决定了在网格搜索时的日志输出级别。</li>
</ul>
<h2 id="u6570_u636E_u5F52_u4E00_u5316"><a href="#u6570_u636E_u5F52_u4E00_u5316" class="headerlink" title="数据归一化"></a>数据归一化</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/43b6730765831fcc1dbecb24069ed177.jpg" alt=""></p>
<p>大家先看看上面表格中的样本数据，两个样本的肿瘤大小相差有5倍，从医学角度来讲这个差距已经是非常大了，但从实际数值差距来讲并不是很大。再看看发现时间，两个样本之间相差100天，在数值上的差距远远大于肿瘤大小的差距。所以如果使用kNN算法，用欧拉距离计算的话，两个样本发现时间之差远远大于肿瘤大小之差，所以就会主导样本间的距离，这个显然是有问题的，对预测的结果是有偏差的。</p>
<p>所以我们就需要对样本数据进行数据归一化，将所有的数据映射到同一尺度。比较简便的方式就是<strong>最值归一化</strong>，既用下面的公式把所有数据映射到0-1之间：</p>
<p>$$ x_{scale} = \frac {x - x_{min}} {x_{max} - x_{min}} $$</p>
<p>最值归一化虽然简便，但是是有一定适用范围的，那就是适用于样本数据有明显分布边界的情况，比如学生的考试分数，从0到100分，或者像素值，从0到255等。假如像人的月收入这种没有边界的样本数据集，就不能使用最值归一化了，此时就需要用到另外一个数据归一化的方法<strong>均值方差归一化</strong>，该方法就是把所有数据归一到均值为0方差为1的分布中，公式如下：</p>
<p>$$ x_{scale} = \frac {x - x_{mean}} S $$</p>
<p>就是将每个值减去均值，然后除以方差，通过均值方差归一化后的数据不一定在0-1之间，但是他们的均值为0，方差为1。</p>
<p>下面我们来分别实现一下这两个数据归一化方法。先来看看最值归一化的实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成从0到100，一共100个元素的数组</span></span><br><span class="line">x = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, size = <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 变更数组元素的类型</span></span><br><span class="line">x = np.array(x, dtype = float)</span><br><span class="line">x_scale = (x - np.min(x)) / (np.max(x) - np.min(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成50行，2列的矩阵，元素在0到100之间</span></span><br><span class="line">X = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 对每一列数据进行最值归一化</span></span><br><span class="line">X[:, <span class="number">0</span>] = (X[:, <span class="number">0</span>] - np.min(X[:, <span class="number">0</span>])) / (np.max(X[:, <span class="number">0</span>]) - np.min(X[:, <span class="number">0</span>]))</span><br><span class="line">X[:, <span class="number">1</span>] = (X[:, <span class="number">1</span>] - np.min(X[:, <span class="number">1</span>])) / (np.max(X[:, <span class="number">1</span>]) - np.min(X[:, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用matplotlib将X展示出来</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/96cc0403babcacae118a05b14b5d9730.jpg" alt=""></p>
<p>可以看到最值归一化后数据都在0到1之间。我们再来看看均值方差归一化的实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X2 = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line">X2 = np.array(X2, dtype = float)</span><br><span class="line">X2[:, <span class="number">0</span>] = (X2[:, <span class="number">0</span>] - np.mean(X2[:, <span class="number">0</span>])) / np.std(X2[:, <span class="number">0</span>])</span><br><span class="line">X2[:, <span class="number">1</span>] = (X2[:, <span class="number">1</span>] - np.mean(X2[:, <span class="number">1</span>])) / np.std(X2[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均值接近0</span></span><br><span class="line">np.mean(X2[:, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">6.2172489379008772e-17</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方差接近1</span></span><br><span class="line">np.std(X2[:, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.99999999999999989</span></span><br><span class="line"></span><br><span class="line">plt.scatter(X2[:, <span class="number">0</span>], X2[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f4dbe1f0c876372c0263013bd3e27043.jpg" alt=""></p>
<h2 id="u5982_u4F55_u5BF9_u6D4B_u8BD5_u6570_u636E_u96C6_u8FDB_u884C_u5F52_u4E00_u5316"><a href="#u5982_u4F55_u5BF9_u6D4B_u8BD5_u6570_u636E_u96C6_u8FDB_u884C_u5F52_u4E00_u5316" class="headerlink" title="如何对测试数据集进行归一化"></a>如何对测试数据集进行归一化</h2><p>之前我们说过会对样本数据进行拆分，拆分为训练数据和测试数据，对于训练数据我们可以直接使用最值归一化或均值方法归一化，但是对测试数据我们就不能直接使用归一化的方法了，因为测试数据其实充当了真实环境中需要预测的数据，很多时候需要预测的数据只有一组，这时候我们是没办法对一组数据进行归一化的，因为无法得到均值和方差，所以我们需要结合归一化后训练数据归一化测试数据：<code>(x_test - mean_train) / std_train</code>。那么我们就需要保存训练数据归一化后的数据，此时我们就可以用到Scikit Learn提供的数据归一化的对象<code>Scalar</code>。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c962c7f93465af5cff9be84f884c97fd.jpg" alt=""></p>
<p><code>Scalar</code>的使用流程和机器学习算法的使用流程很像，输入训练数据集，进行<code>fit</code>操作，这里的<code>fit</code>操作就不是训练模型了，而是进行数据归一化处理，然后是<code>transform</code>，既对需要预测的数据进行归一化。我们来看看如何使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用鸢尾花数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割出训练数据集和测试数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入StandardScaler，也就是均值方差归一化的对象</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">standardScaler = StandardScaler()</span><br><span class="line">standardScaler.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将特征训练数据集和特征测试数据集进行归一化处理</span></span><br><span class="line">X_train_standard = standardScaler.transform(X_train)</span><br><span class="line">X_test_standard = standardScaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用kNN</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors = <span class="number">3</span>)</span><br><span class="line">knn_clf.fit(X_train_standard, y_train)</span><br><span class="line">knn_clf.score(X_test_standard, y_test)</span><br></pre></td></tr></table></figure>
<h2 id="u5C01_u88C5_u81EA_u5DF1_u7684_u6570_u636E_u5F52_u4E00_u5316_u65B9_u6CD5"><a href="#u5C01_u88C5_u81EA_u5DF1_u7684_u6570_u636E_u5F52_u4E00_u5316_u65B9_u6CD5" class="headerlink" title="封装自己的数据归一化方法"></a>封装自己的数据归一化方法</h2><p><img src="http://paxigrdp0.bkt.clouddn.com/a446676198ff1ff737c340099f48c7cb.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StandardScaler</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.mean_ = <span class="keyword">None</span></span><br><span class="line">		self.scaler_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取训练数据集的平均值和方差</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X.ndim == <span class="number">2</span>, <span class="string">"X 的维度必须为2，既X是一个矩阵"</span></span><br><span class="line"></span><br><span class="line">		self.mean_ = np.array([np.mean(X[:, i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])])</span><br><span class="line">		self.scaler_ = np.array([np.std(X[:, i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 进行均值方差归一化处理</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X.ndim == <span class="number">2</span>, <span class="string">"X 的维度必须为2，既X是一个矩阵"</span></span><br><span class="line">		<span class="keyword">assert</span> self.mean_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.scaler_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">"均值和方差不能为空"</span></span><br><span class="line">		<span class="keyword">assert</span> X.shape[<span class="number">1</span>] == len(self.mean_), <span class="string">"训练数据集矩阵的列数必须等于均值数组的元素个数"</span></span><br><span class="line">		<span class="keyword">assert</span> X.shape[<span class="number">1</span>] == len(self.scaler_), <span class="string">"训练数据集矩阵的列数必须等于方差数组的元素个数"</span></span><br><span class="line"></span><br><span class="line">		X_transform = np.empty(shape=X.shape, dtype=float)</span><br><span class="line">		<span class="keyword">for</span> col <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">			X_transform[:, col] = (X[:, col] - self.mean_[col]) / self.scaler_[col]</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> X_transform</span><br></pre></td></tr></table></figure>
<p>这样我们就封装好了自己的均值方差归一化的方法，另外，Scikit Learn也提供了最值归一化的对象<code>MinMaxScaler</code>，使用流程都是一样的，大家也可是试试看。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>这一篇笔记主要介绍了kNN算法实现逻辑以外的概念，但也是机器学习中非常重要的一些概念，以后也会经常看到它们的身影。通过两篇笔记的介绍，我们知道kNN算法是一个解决多分类问题的算法，而且算法实现相对比较简单，但效果很强大。下一篇我们来实现第一篇笔记中介绍过的线性回归法。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Notebook中调用，以及计算器算法的封装规范，kNN的<code>k</code>值如何计算，如何使用Scikit Learn中的kNN算法，还有机器学习算法中的一些主要概念，比如训练数据集、测试数据集，分类准确度，超参数，数据归一化。另外会具体用代码实现第一篇笔记中介绍过的线性回归算法。</p>
<h2 id="u5C01_u88C5kNN_u7B97_u6CD5"><a href="#u5C01_u88C5kNN_u7B97_u6CD5" class="headerlink" title="封装kNN算法"></a>封装kNN算法</h2><p>上一篇笔记中我们对kNN算法在Jupyter Notebook中进行了实现，但是想要复用这个算法就很不方便，所以我们来看看如何在PyCharm中封装算法，并且在Jupyter Notebook中进行调用。</p>
<p>PyCharm的配置这里我就不再累赘，如图所示，我们创建了一个Python文件<code>kNN.py</code>，然后定义了<code>kNNClassify</code>方法，该方法有4个参数，分别是kNN算法的<code>k</code>值，训练样本特征数据集<code>XTrain</code>，训练样本类别数据集<code>yTrain</code>，预测特征数据集<code>x</code>。该方法中的实现和在Jupyter Notebook中实现的一模一样，只不过加了三个断言，让方法的健壮性更好一点。我们给出<strong>N维欧拉定理</strong>：</p>
<p>$$ \sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} $$</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/574075757e0d0a820e2a8a7ef37f79ec.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kNN.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNNClassify</span><span class="params">(k, XTrain, yTrain, x)</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> <span class="number">1</span> &lt;= k &lt;= XTrain.shape[<span class="number">0</span>], <span class="string">"k 的取值范围不正确"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] == yTrain.shape[<span class="number">0</span>], <span class="string">"训练样本数据行数应该与训练结果集行数相同"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">1</span>] == x.shape[<span class="number">0</span>], <span class="string">"训练样本数据特性个数应该与被预测数据特性个数相同"</span></span><br><span class="line"></span><br><span class="line">	distances = [sqrt(np.sum((xTrain - x) ** <span class="number">2</span>)) <span class="keyword">for</span> xTrain <span class="keyword">in</span> XTrain]</span><br><span class="line">	nearest = np.argsort(distances)</span><br><span class="line"></span><br><span class="line">	topKy = [yTrain[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:k]]</span><br><span class="line">	votes = Counter(topKy)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这样我们就在PyCharm中封装好了kNN算法的方法，我们再来看看如何在Jupyter Notebook中调用封装好的方法呢，这就需要使用<code>%run</code>这个命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">raw_data_X = [[<span class="number">3.393533211</span>, <span class="number">2.331273381</span>],</span><br><span class="line">			  [<span class="number">3.110073483</span>, <span class="number">1.781539638</span>],</span><br><span class="line">			  [<span class="number">1.343808831</span>, <span class="number">3.368360954</span>],</span><br><span class="line">			  [<span class="number">3.582294042</span>, <span class="number">4.679179110</span>],</span><br><span class="line">			  [<span class="number">2.280362439</span>, <span class="number">2.866990263</span>],</span><br><span class="line">			  [<span class="number">7.423436942</span>, <span class="number">4.696522875</span>],</span><br><span class="line">			  [<span class="number">5.745051997</span>, <span class="number">3.533989803</span>],</span><br><span class="line">			  [<span class="number">9.172168622</span>, <span class="number">2.511101045</span>],</span><br><span class="line">			  [<span class="number">7.792783481</span>, <span class="number">3.424088941</span>],</span><br><span class="line">			  [<span class="number">7.939820817</span>, <span class="number">0.791637231</span>]</span><br><span class="line">			 ]</span><br><span class="line">raw_data_y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">XTrain = np.array(raw_data_X)</span><br><span class="line">yTrain = np.array(raw_data_y)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">8.093607318</span>, <span class="number">3.365731514</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用%run命令可以引入Python文件，并可使用该Python文件中定义的属性和方法</span></span><br><span class="line">%run ../pycharm/kNN.py</span><br><span class="line">predicty = kNNClassify(<span class="number">6</span>, XTrain, yTrain, x)</span><br><span class="line">predicty</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="数据归一化" scheme="http://www.devtalking.com/tags/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记三之NumPy、Matplotlib、kNN算法]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-3/"/>
    <id>http://www.devtalking.com//articles/machine-learning-3/</id>
    <published>2018-01-19T16:00:00.000Z</published>
    <updated>2018-08-27T02:29:03.756Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h2><p>NumPy是Python中的一个类库，它支持高阶维度数组（矩阵）的创建及各种操作、运算，是我们在机器学习中经常会使用的一个类库。这一节再看一些NumPy的矩阵用法。</p>
<h3 id="numpy-random"><a href="#numpy-random" class="headerlink" title="numpy.random"></a>numpy.random</h3><p>NumPy也提供了生成随机数和随机元素数组的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成从0到10之间的随机数</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成元素从0到10，一共4个随机元素的数组</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成元素随机从0到10，3行5列的矩阵</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">6</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<p>如果我们希望每次使用随机方法生成的结果都是一样的，一般调试时候有这个需求，此时NumPy的<code>random()</code>方法也提供了方便简单的方式，既随机种子的概念：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成随机矩阵前给定一个种子</span></span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"><span class="comment"># 然后生成随机矩阵</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 再次生成随机矩阵时，只要传入相同的种子，就可以得到相同结果的矩阵</span></span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 默认范围是从0.0到1.0，返回值为float型</span></span><br><span class="line">np.random.random()</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.18249173045349998</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 传入的参数是数组的大小</span></span><br><span class="line">np.random.random(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.17545176</span>,  <span class="number">0.53155137</span>,  <span class="number">0.53182759</span>,  <span class="number">0.63440096</span>,  <span class="number">0.84943179</span>,</span><br><span class="line">		<span class="number">0.72445532</span>,  <span class="number">0.61102351</span>,  <span class="number">0.72244338</span>,  <span class="number">0.32295891</span>,  <span class="number">0.36178866</span>])</span><br><span class="line">		</span><br><span class="line"><span class="comment"># 创建4行5列，元素值的范围从0.0到1.0的矩阵</span></span><br><span class="line">np.random.random((<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.22826323</span>,  <span class="number">0.29371405</span>,  <span class="number">0.63097612</span>,  <span class="number">0.09210494</span>,  <span class="number">0.43370117</span>],</span><br><span class="line">	   [ <span class="number">0.43086276</span>,  <span class="number">0.4936851</span> ,  <span class="number">0.42583029</span>,  <span class="number">0.31226122</span>,  <span class="number">0.42635131</span>],</span><br><span class="line">	   [ <span class="number">0.89338916</span>,  <span class="number">0.94416002</span>,  <span class="number">0.50183668</span>,  <span class="number">0.62395295</span>,  <span class="number">0.1156184</span> ],</span><br><span class="line">	   [ <span class="number">0.31728548</span>,  <span class="number">0.41482621</span>,  <span class="number">0.86630916</span>,  <span class="number">0.25045537</span>,  <span class="number">0.48303426</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u6307_u5B9A_u5747_u503C_u548C_u6807_u51C6_u5DEE_u751F_u6210_u968F_u673A_u6570_u6570_u7EC4_u6216_u77E9_u9635"><a href="#u6307_u5B9A_u5747_u503C_u548C_u6807_u51C6_u5DEE_u751F_u6210_u968F_u673A_u6570_u6570_u7EC4_u6216_u77E9_u9635" class="headerlink" title="指定均值和标准差生成随机数数组或矩阵"></a>指定均值和标准差生成随机数数组或矩阵</h3><p>我们先来看看均值、方差、标准差的概念。均值很好理解，就是所有样本数据的平均值，描述了样本集合的中间点：</p>
<p>$$ \overline X=\frac{\sum_{i=1}^nX_i}n $$</p>
<p>方差是衡量样本点和样本期望值相差的度量值：</p>
<p>$$ S^2 = \frac{\sum_{i=1}^n(X_i-\overline X)^2} n $$</p>
<p>标准差描述的是样本集合的各个样本点到均值的距离之平均：</p>
<p>$$ S = \sqrt {\frac{\sum_{i=1}^n(X_i-\overline X)^2} n } $$</p>
<p>标准差也就是对方差开根号。举个例子，<code>[0, 8, 12, 20]</code>和<code>[8, 9, 11, 12]</code>，两个集合的均值都是10，但显然两个集合的差别是很大的，计算两者的标准差，前者是8.3后者是1.8，显然后者较为集中，标准差描述的就是这种散布度或者叫做波动大小。综上，方差的意义在于描述随机变量稳定与波动、集中与分散的状况。标准差则体现随机变量取值与其期望值的偏差。</p>
<a id="more"></a>
<p>NumPy也提供了指定均值和标准差生成随机数的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一个参数是均值，第二个参数是标准差</span></span><br><span class="line">np.random.normal(<span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">53.781947121910044</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建均值为10，方差为100的3行5列矩阵</span></span><br><span class="line">np.random.normal(<span class="number">10</span>, <span class="number">100</span>, size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">124.10915759</span>,   <span class="number">27.14517732</span>, -<span class="number">144.95788359</span>,  -<span class="number">87.40234817</span>,</span><br><span class="line">		 -<span class="number">94.91106048</span>],</span><br><span class="line">	   [ -<span class="number">36.4834381</span> ,  -<span class="number">39.05598871</span>,  <span class="number">110.07456975</span>,  <span class="number">224.85141913</span>,</span><br><span class="line">		 <span class="number">153.24092557</span>],</span><br><span class="line">	   [  -<span class="number">3.33533336</span>,   <span class="number">10.57740526</span>,  -<span class="number">56.76208107</span>,  -<span class="number">84.06189149</span>,</span><br><span class="line">		 <span class="number">103.08098119</span>]])</span><br><span class="line">		 </span><br><span class="line"><span class="comment"># 创建一个正态分布的3行5列矩阵，既均值为0，标准差为1</span></span><br><span class="line">np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[-<span class="number">0.94574322</span>,  <span class="number">2.0742057</span> ,  <span class="number">0.34477911</span>,  <span class="number">0.1375712</span> ,  <span class="number">0.45385364</span>],</span><br><span class="line">	   [-<span class="number">2.07928914</span>,  <span class="number">1.26474497</span>,  <span class="number">1.56236822</span>, -<span class="number">1.0032234</span> , -<span class="number">0.14807477</span>],</span><br><span class="line">	   [ <span class="number">0.01992922</span>,  <span class="number">0.3924738</span> , -<span class="number">0.11268871</span>,  <span class="number">2.04509319</span>,  <span class="number">0.01095378</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u67E5_u770B_u6570_u7EC4_u7EF4_u5EA6"><a href="#u67E5_u770B_u6570_u7EC4_u7EF4_u5EA6" class="headerlink" title="查看数组维度"></a>查看数组维度</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成10个元素的一维数组和3行5列的矩阵</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.arange(<span class="number">10</span>)</span><br><span class="line">X = np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">x</span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">	   [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 查看x的维度</span></span><br><span class="line">x.ndim</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看X的维度</span></span><br><span class="line">X.ndim</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数组每个维度的具体信息</span></span><br><span class="line">x.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">10</span>,)</span><br><span class="line"></span><br><span class="line">X.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">3</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="numpy-array_u7684_u6570_u636E_u8BBF_u95EE"><a href="#numpy-array_u7684_u6570_u636E_u8BBF_u95EE" class="headerlink" title="numpy.array的数据访问"></a>numpy.array的数据访问</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一维数组访问第1个元素</span></span><br><span class="line">x[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一维数组访问最后一个元素</span></span><br><span class="line">x[-<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组访问第3行，第4列的元素</span></span><br><span class="line">X[<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">13</span></span><br></pre></td></tr></table></figure>
<h3 id="u5207_u7247"><a href="#u5207_u7247" class="headerlink" title="切片"></a>切片</h3><p>Python中，有一个获取数组片段非常方便的方法，叫做切片，<code>numpy.array</code>中同样支持切片，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取x数组中从第1个元素到第5个元素的片段</span></span><br><span class="line">x[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果冒号前不指定位置，那么默认从第一个元素开始</span></span><br><span class="line">x[:<span class="number">3</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果冒号后面不指定位置，那么默认取到最后一个元素</span></span><br><span class="line">x[<span class="number">3</span>:]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切片也支持步长</span></span><br><span class="line">x[<span class="number">0</span>:<span class="number">10</span>:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">x[::<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取X矩阵的前2行，前3列</span></span><br><span class="line">X[:<span class="number">2</span>, :<span class="number">3</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 对于每个维度都可以指定步长</span></span><br><span class="line">X[:<span class="number">2</span>, ::<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
<p>一般将高维矩阵降为低维矩阵其实也是使用切片来处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取X矩阵所有行的第一列</span></span><br><span class="line">X[:, <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">5</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>另外需要注意的是通过切片获取NumPy的数组或者矩阵的子数组，子矩阵是通过引用方式的，而Python中的数组通过切片获取的子数组是拷贝方式的。NumPy主要是考虑到性能效率问题。我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取X矩阵的前2行，前3列作为子矩阵</span></span><br><span class="line">subX = X[:<span class="number">2</span>, :<span class="number">3</span>]</span><br><span class="line">subX</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 给subX矩阵的第1行，第1列的元素赋值</span></span><br><span class="line">subX[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line">subX</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">100</span>,   <span class="number">1</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">5</span>,   <span class="number">6</span>,   <span class="number">7</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 再看看X矩阵</span></span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">100</span>,   <span class="number">1</span>,   <span class="number">2</span>,   <span class="number">3</span>,   <span class="number">4</span>],</span><br><span class="line">	   [  <span class="number">5</span>,   <span class="number">6</span>,   <span class="number">7</span>,   <span class="number">8</span>,   <span class="number">9</span>],</span><br><span class="line">	   [ <span class="number">10</span>,  <span class="number">11</span>,  <span class="number">12</span>,  <span class="number">13</span>,  <span class="number">14</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 复制子矩阵</span></span><br><span class="line">subX1 = X[:<span class="number">2</span>, :<span class="number">3</span>].copy()</span><br><span class="line">subX1</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">100</span>,   <span class="number">1</span>,   <span class="number">2</span>],</span><br><span class="line">	   [  <span class="number">5</span>,   <span class="number">6</span>,   <span class="number">7</span>]])</span><br><span class="line">	   </span><br><span class="line">subX1[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">subX1</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">	   </span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">100</span>,   <span class="number">1</span>,   <span class="number">2</span>,   <span class="number">3</span>,   <span class="number">4</span>],</span><br><span class="line">	   [  <span class="number">5</span>,   <span class="number">6</span>,   <span class="number">7</span>,   <span class="number">8</span>,   <span class="number">9</span>],</span><br><span class="line">	   [ <span class="number">10</span>,  <span class="number">11</span>,  <span class="number">12</span>,  <span class="number">13</span>,  <span class="number">14</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u6539_u53D8_u6570_u7EC4_u7EF4_u5EA6"><a href="#u6539_u53D8_u6570_u7EC4_u7EF4_u5EA6" class="headerlink" title="改变数组维度"></a>改变数组维度</h3><p>NumPy也提供了修改数组维度的方法，我们来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x是一个一维数组</span></span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x改为二维数组，既2行5列的矩阵</span></span><br><span class="line">x.reshape(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 如果想让NumPy自动计算某个维度的数，比如我只想将x转换为只有2列的矩阵，有多少行交给NumPy处理</span></span><br><span class="line">x.reshape(-<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">	   [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">	   [<span class="number">8</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u6570_u7EC4_u5408_u5E76_u64CD_u4F5C"><a href="#u6570_u7EC4_u5408_u5E76_u64CD_u4F5C" class="headerlink" title="数组合并操作"></a>数组合并操作</h3><p>NumPy也提供两个数组合并的操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">5</span>)</span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">y = np.arange(<span class="number">5</span>)</span><br><span class="line">y</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x，y这两个一维数组合并</span></span><br><span class="line">np.concatenate([x, y])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">z = np.array([<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">z</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x，y，z三个一维数组合并</span></span><br><span class="line">np.concatenate([x, y, z])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>多维数组也支持合并：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2行3列的矩阵</span></span><br><span class="line">V = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">V</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">8</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 将两个V矩阵合并</span></span><br><span class="line">np.concatenate([V, V])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">8</span>, <span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">8</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 合并多维数组时，可以设置按照哪个维度合并，axis参数为0时按照行合并，axis参数为1时按照列合并，默认axis为0</span></span><br><span class="line">np.concatenate([V, V], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">8</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<p>上面的示例都是同维度的数组进行合并，那么不同维度的数组如何合并呢，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># z为一个一维数组</span></span><br><span class="line">z = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">z</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># V为一个2行3列的矩阵</span></span><br><span class="line">V</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">9</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">8</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 直接将V和z合并会抛异常</span></span><br><span class="line">np.concatenate([V, z])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">ValueError: all the input arrays must have same number of dimensions</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在合并时将一维数组z转变为二维数组</span></span><br><span class="line">np.concatenate([V, z.reshape(<span class="number">1</span>, -<span class="number">1</span>)])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">7</span>, <span class="number">3</span>, <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<p>其实NumPy提供了更智能的不同维度数组合并的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按垂直方向合并</span></span><br><span class="line">np.vstack([V, z])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">8</span>, <span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">7</span>, <span class="number">3</span>, <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 创建一个2行2列的矩阵</span></span><br><span class="line">V1 = np.full((<span class="number">2</span>, <span class="number">2</span>), <span class="number">10</span>)</span><br><span class="line">V1</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">	   [<span class="number">10</span>, <span class="number">10</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按水平方向合并</span></span><br><span class="line">np.hstack([V, V1])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">8</span>,  <span class="number">2</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">	   [ <span class="number">7</span>,  <span class="number">3</span>,  <span class="number">8</span>, <span class="number">10</span>, <span class="number">10</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u6570_u7EC4_u5206_u5272_u64CD_u4F5C"><a href="#u6570_u7EC4_u5206_u5272_u64CD_u4F5C" class="headerlink" title="数组分割操作"></a>数组分割操作</h3><p>有合并自然就会有分割，我们来看看NumPy提供的分割方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">10</span>)</span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对x进行分割，然后传入分割点，如下有两个分割点，所以将会把x分割为3个数组</span></span><br><span class="line">x1, x2, x3 = np.split(x, [<span class="number">3</span>, <span class="number">7</span>])</span><br><span class="line">x1</span><br><span class="line">x2</span><br><span class="line">x3</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">array([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">array([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于多维数组也是一样</span></span><br><span class="line">X = np.arange(<span class="number">16</span>).reshape((<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 传入一个分割点，既将X矩阵分割为两个矩阵</span></span><br><span class="line">X1, X2 = np.split(X, [<span class="number">2</span>])</span><br><span class="line">X1</span><br><span class="line">X2</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">array([[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 分割多维数组同样可以设定按照哪个维度分割，axis默认为0，既按行分割</span></span><br><span class="line"><span class="comment"># axis为1时按列分割</span></span><br><span class="line">X3, X4 = np.split(X, [<span class="number">2</span>], axis=<span class="number">1</span>)</span><br><span class="line">X3</span><br><span class="line">X4</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>]])</span><br><span class="line">array([[ <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [<span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">14</span>, <span class="number">15</span>]])</span><br></pre></td></tr></table></figure>
<p>和合并一样，分割也有更快接的方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按垂直方向分割，既按行分割</span></span><br><span class="line">X5, X6 = np.vsplit(X, [<span class="number">2</span>])</span><br><span class="line">X5</span><br><span class="line">X6</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line">array([[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 按水平方向分割，既按列分割</span></span><br><span class="line">X7, X8 = np.hsplit(X, [<span class="number">2</span>])</span><br><span class="line">X7</span><br><span class="line">X8</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>]])</span><br><span class="line">array([[ <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [<span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">14</span>, <span class="number">15</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u77E9_u9635_u8FD0_u7B97"><a href="#u77E9_u9635_u8FD0_u7B97" class="headerlink" title="矩阵运算"></a>矩阵运算</h3><p>NumPy中提供了完整的矩阵的运算，我们从加减法来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># A为一个2行5列的矩阵</span></span><br><span class="line">A = np.arange(<span class="number">10</span>).reshape(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">A</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># B也是一个2行5列的矩阵</span></span><br><span class="line">B = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">B</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">4</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 矩阵加常数</span></span><br><span class="line">A + <span class="number">1</span></span><br><span class="line"><span class="comment">#结果</span></span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">	   [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵减常数</span></span><br><span class="line">A - <span class="number">1</span></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[-<span class="number">1</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 矩阵加矩阵</span></span><br><span class="line">A + B</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">4</span>,  <span class="number">9</span>,  <span class="number">5</span>,  <span class="number">8</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">14</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 矩阵减矩阵</span></span><br><span class="line">A - B</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[-<span class="number">4</span>, -<span class="number">7</span>, -<span class="number">1</span>, -<span class="number">2</span>, -<span class="number">3</span>],</span><br><span class="line">	   [-<span class="number">4</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<p>下面我们再来看看数乘：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 矩阵乘常数</span></span><br><span class="line"><span class="number">2</span>*A</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">6</span>,  <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># C为5行3列的矩阵</span></span><br><span class="line">C = np.arange(<span class="number">15</span>).reshape(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">C</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">	   [ <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">	   [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],</span><br><span class="line">	   [ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 矩阵乘矩阵，真正的矩阵相乘</span></span><br><span class="line">A.dot(C)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">90</span>, <span class="number">100</span>, <span class="number">110</span>],</span><br><span class="line">	   [<span class="number">240</span>, <span class="number">275</span>, <span class="number">310</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 矩阵中每个对应元素相乘</span></span><br><span class="line">A*B</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">8</span>,  <span class="number">6</span>, <span class="number">15</span>, <span class="number">28</span>],</span><br><span class="line">	   [<span class="number">45</span>, <span class="number">36</span>, <span class="number">42</span>, <span class="number">48</span>, <span class="number">54</span>]])</span><br></pre></td></tr></table></figure>
<p>我们再来看看矩阵的转置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A.T</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">	   [<span class="number">1</span>, <span class="number">6</span>],</span><br><span class="line">	   [<span class="number">2</span>, <span class="number">7</span>],</span><br><span class="line">	   [<span class="number">3</span>, <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u805A_u5408_u64CD_u4F5C"><a href="#u805A_u5408_u64CD_u4F5C" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>NumPy中有很多对数组的聚合操作方法，我们先来看看一维数组：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机取10个元素的 一维数组</span></span><br><span class="line">D = np.random.random(<span class="number">10</span>)</span><br><span class="line">D</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.70908471</span>,  <span class="number">0.29268356</span>,  <span class="number">0.69885019</span>,  <span class="number">0.28796429</span>,  <span class="number">0.04189265</span>,</span><br><span class="line">		<span class="number">0.36932107</span>,  <span class="number">0.0641322</span> ,  <span class="number">0.63989077</span>,  <span class="number">0.02753356</span>,  <span class="number">0.0605743</span> ])</span><br><span class="line">	 </span><br><span class="line"><span class="comment"># 求每个元素的和        </span></span><br><span class="line">np.sum(D)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3.1919272951030706</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求元素的最小值</span></span><br><span class="line">np.min(D)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.027533561561906672</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求元素最大值</span></span><br><span class="line">np.max(D)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.70908470606410545</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求元素的均值</span></span><br><span class="line">np.mean(D)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.31919272951030708</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 求元素的标准差</span></span><br><span class="line">np.std(D)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.26402525382852743</span></span><br></pre></td></tr></table></figure>
<p>我们再来看看矩阵的聚合操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># X为2行3列的矩阵</span></span><br><span class="line">X = np.arange(<span class="number">6</span>).reshape(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">	   [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 矩阵中所有元素的和       </span></span><br><span class="line">np.sum(X)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵中所有元素的乘积</span></span><br><span class="line">np.prod(X)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"></span><br><span class="line">np.prod(X + <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">720</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵元素的均值</span></span><br><span class="line">np.mean(X)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵元素的中位数，median可有效避免元素中出现极值，从而导致均值不准的问题</span></span><br><span class="line">np.median(X)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵元素的方差</span></span><br><span class="line">np.var(X)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2.9166666666666665</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># np.std(X)</span></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1.707825127659933</span></span><br></pre></td></tr></table></figure>
<h3 id="u7D22_u5F15_u548C_u6392_u5E8F_u7684_u76F8_u5173_u64CD_u4F5C"><a href="#u7D22_u5F15_u548C_u6392_u5E8F_u7684_u76F8_u5173_u64CD_u4F5C" class="headerlink" title="索引和排序的相关操作"></a>索引和排序的相关操作</h3><p>NumPy提供了一系列对数组索引操作的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机一维数组</span></span><br><span class="line">x = np.random.random(<span class="number">10</span>)</span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.17035458</span>,  <span class="number">0.8968506</span> ,  <span class="number">0.01007584</span>,  <span class="number">0.45925501</span>,  <span class="number">0.6838149</span> ,</span><br><span class="line">		<span class="number">0.32393039</span>,  <span class="number">0.53746647</span>,  <span class="number">0.68561243</span>,  <span class="number">0.66195346</span>,  <span class="number">0.32696068</span>])</span><br><span class="line">		</span><br><span class="line"><span class="comment"># x中元素最小值</span></span><br><span class="line">np.min(x)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.010075835471876626</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x中最小值元素所在的索引位置</span></span><br><span class="line">np.argmin(x)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>我们再来看看排序：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对x进行排序</span></span><br><span class="line">np.sort(x)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.01007584</span>,  <span class="number">0.17035458</span>,  <span class="number">0.32393039</span>,  <span class="number">0.32696068</span>,  <span class="number">0.45925501</span>,</span><br><span class="line">		<span class="number">0.53746647</span>,  <span class="number">0.66195346</span>,  <span class="number">0.6838149</span> ,  <span class="number">0.68561243</span>,  <span class="number">0.8968506</span> ])</span><br><span class="line">		</span><br><span class="line"><span class="comment"># 获取排序后的索引，返回的数组中的元素是索引</span></span><br><span class="line">np.argsort(x)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">2</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对矩阵排序</span></span><br><span class="line">X1 = np.random.randint(<span class="number">0</span>, <span class="number">15</span>, size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">X1</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">8</span>,  <span class="number">1</span>,  <span class="number">8</span>, <span class="number">12</span>,  <span class="number">4</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">6</span>,  <span class="number">6</span>,  <span class="number">6</span>, <span class="number">13</span>],</span><br><span class="line">	   [<span class="number">13</span>,  <span class="number">2</span>,  <span class="number">5</span>, <span class="number">11</span>,  <span class="number">4</span>]])</span><br><span class="line">	   </span><br><span class="line">np.sort(X1)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">8</span>,  <span class="number">8</span>, <span class="number">12</span>],</span><br><span class="line">	   [ <span class="number">6</span>,  <span class="number">6</span>,  <span class="number">6</span>,  <span class="number">8</span>, <span class="number">13</span>],</span><br><span class="line">	   [ <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">5</span>, <span class="number">11</span>, <span class="number">13</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 获取排序后的索引</span></span><br><span class="line">np.argsort(X1)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">	   [<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="NumPy_u7684Fancy_Indexing"><a href="#NumPy_u7684Fancy_Indexing" class="headerlink" title="NumPy的Fancy Indexing"></a>NumPy的Fancy Indexing</h3><p>一般情况下我们访问NumPy数组的数据，可以使用索引，甚至可以用步长来取：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">16</span>)</span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取从0到6范围内，步长为2的元素</span></span><br><span class="line">x[<span class="number">0</span>:<span class="number">6</span>:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>但是有时候我们需要取数组中没有什么规律的元素，比如元素之间步长不等的，这就需要用到NumPy提供的Fancy Indexing机制来获取了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将我们需要访问的索引生产一个数组，然后将索引数组传入x数组</span></span><br><span class="line">ind = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>]</span><br><span class="line">x[ind]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵也是同样的，先生成索引矩阵</span></span><br><span class="line">ind1 = np.array([[<span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">				[<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">x[ind1]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 将x转换为4行4列的矩阵X</span></span><br><span class="line">X = x.reshape(<span class="number">4</span>, -<span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成希望查询的行和列的索引矩阵，然后传入矩阵X</span></span><br><span class="line">row = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">col = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">X[row, col]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">1</span>,  <span class="number">6</span>, <span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者指定前两行</span></span><br><span class="line">X[:<span class="number">2</span>, col]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p>除了使用指定索引以外，我们还可以使用布尔数组或者矩阵来使用Fancy Indexing，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个布尔数组，True表示感兴趣的索引，False表示不感兴趣的索引</span></span><br><span class="line">col = [<span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">True</span>]</span><br><span class="line"><span class="comment"># 然后传入矩阵X，比如我们要获取前三行，第1列，第3列，第4列的元素</span></span><br><span class="line">X[<span class="number">1</span>:<span class="number">3</span>, col]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">4</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [ <span class="number">8</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line">	   </span><br><span class="line">x</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># x数组中的元素小于2的有几个</span></span><br><span class="line">np.sum(x &lt; <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x数组中的元素大于3小于10的有几个</span></span><br><span class="line">np.sum((x &gt; <span class="number">3</span>) &amp; (x &lt; <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断x数组中的所有元素是否满足一个条件，若有一个满足返回True，若都不满足返回False</span></span><br><span class="line">np.any(x == <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">np.any(x &lt; <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断x数组中的所有元素是否满足一个条件，若有所有元素都满足返回True，若有一个元素不满足返回False</span></span><br><span class="line">np.all(X &gt; <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">np.all(X &gt;= <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取x数组中元素小于5的是哪几个元素</span></span><br><span class="line">x[x &lt; <span class="number">5</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取x数组中元素是偶数的是哪几个元素</span></span><br><span class="line">x[x % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">6</span>,  <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>])</span><br><span class="line"></span><br><span class="line">X</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">	   [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line">	   </span><br><span class="line">X[:, <span class="number">3</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">3</span>,  <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>])</span><br><span class="line"></span><br><span class="line">X[:, <span class="number">3</span>] % <span class="number">3</span> == <span class="number">0</span></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">False</span>,  <span class="keyword">True</span>], dtype=bool)</span><br><span class="line"></span><br><span class="line">X[X[:, <span class="number">3</span>] % <span class="number">3</span> == <span class="number">0</span>, :]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>在Python中，除了有NumPy这种对数组操作的类库，还有一个类一个在机器学习中使用比较广泛的类库是Matplotlib，这是一个绘制二维图像的类库，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先导入matplotlib的类库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数组x，元素从0到10，一共100个元素</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 对x数组求sin，获得siny</span></span><br><span class="line">siny = np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用matplotlib将x数组和y数组中的元素绘制出来</span></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/27546352e3ae5661ec64447e1d79c987.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以同时绘制两条线</span></span><br><span class="line">cosy = np.cos(x)</span><br><span class="line">plt.plot(x, siny)</span><br><span class="line">plt.plot(x, cosy)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/2339610f3ab865f00c07ef4cbfbf38d8.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以指定某条线的颜色</span></span><br><span class="line">plt.plot(x, siny)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/abda87e7c2568b71197b6444ab11cc18.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以指定线的样式</span></span><br><span class="line">plt.plot(x, siny)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/66dcd44d1f15a884b3a95646db3171f6.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以指定x轴，y轴的区间</span></span><br><span class="line">plt.plot(x, siny)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.xlim(-<span class="number">5</span>, <span class="number">15</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d2b019bfa3a5ce4f6a7513f644a53554.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 另一种指定x轴，y轴区间的方法</span></span><br><span class="line">plt.plot(x, siny)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.axis([-<span class="number">1</span>, <span class="number">11</span>, -<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/120a7176de45b5e6a0a4ec3ea2b3be64.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给x轴和y轴加说明</span></span><br><span class="line">plt.plot(x, siny)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">plt.axis([-<span class="number">1</span>, <span class="number">11</span>, -<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">plt.xlabel(<span class="string">"x axis"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y value"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/a8d6391854497052d4b1314fff71b1bf.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加图例</span></span><br><span class="line">plt.plot(x, siny, label=<span class="string">"sin(x)"</span>)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>, label=<span class="string">"cos(x)"</span>)</span><br><span class="line">plt.axis([-<span class="number">1</span>, <span class="number">11</span>, -<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">plt.xlabel(<span class="string">"x axis"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y value"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/3a58abde79b6a6b05f124fccfd50bb5d.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加标题</span></span><br><span class="line">plt.plot(x, siny, label=<span class="string">"sin(x)"</span>)</span><br><span class="line">plt.plot(x, cosy, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>, label=<span class="string">"cos(x)"</span>)</span><br><span class="line">plt.axis([-<span class="number">1</span>, <span class="number">11</span>, -<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">plt.xlabel(<span class="string">"x axis"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y value"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"Welcome to ML!"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/06cfd0975eb93a74f9ce68f1e0d0bb2d.jpg" alt=""></p>
<p>以上都是利用matplotlib画折线图，下面来看看如何画散点图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x, siny)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/48690e5fe48a9928c6f205c349f72df7.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.scatter(x, siny)</span><br><span class="line">plt.scatter(x, cosy)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/c7bf4cd8614cbe1c530ed207299d6926.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">y = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/f364d28c40938463ab9717ce2233b311.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置点的透明度</span></span><br><span class="line">plt.scatter(x, y, alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/fecd3d8099c73f72d6f15e1f7746fbbb.jpg" alt=""></p>
<h2 id="u57FA_u4E8EScikit_Learn_u7684_u6570_u636E_u63A2_u7D22"><a href="#u57FA_u4E8EScikit_Learn_u7684_u6570_u636E_u63A2_u7D22" class="headerlink" title="基于Scikit Learn的数据探索"></a>基于Scikit Learn的数据探索</h2><p>Scikit-learn是Python语言中专门针对机器学习应用而发展起来的一款开源框架，其中有一个模块叫Datasets，它提供了机器学习的一些常用的数据集以及产生数据集的方法，比如波士顿房价数据集、乳腺癌数据集、糖尿病数据集、手写字体数据集、鸢尾花数据集等等。这一小节我们就通过Scikit Learn的Datasets来初步对机器学习的数据进行探索。</p>
<p>我们使用NumPy和Matplotlib对Scikit Learn Datasets中的鸢尾花这个数据集进行探索：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 我们只导入Scikit Learn中的datasets模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集，获取到的iris的数据结构是一个字典</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 看看字典的key都有什么</span></span><br><span class="line">iris.keys()</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">dict_keys([<span class="string">'data'</span>, <span class="string">'target'</span>, <span class="string">'target_names'</span>, <span class="string">'DESCR'</span>, <span class="string">'feature_names'</span>])</span><br></pre></td></tr></table></figure>
<p>从上面的示例可以看到鸢尾花这个字典一共包含五种信息，我们逐一来看看这五种信息：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"># 先看一下DESCR信息，该信息解释了鸢尾花这个数据集</span><br><span class="line">print(iris.DESCR)</span><br><span class="line"># 结果</span><br><span class="line">Iris Plants Database</span><br><span class="line">====================</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">-----</span><br><span class="line">Data Set Characteristics:</span><br><span class="line">	:Number of Instances: 150 (50 in each of three classes)</span><br><span class="line">	:Number of Attributes: 4 numeric, predictive attributes and the class</span><br><span class="line">	:Attribute Information:</span><br><span class="line">		- sepal length in cm</span><br><span class="line">		- sepal width in cm</span><br><span class="line">		- petal length in cm</span><br><span class="line">		- petal width in cm</span><br><span class="line">		- class:</span><br><span class="line">				- Iris-Setosa</span><br><span class="line">				- Iris-Versicolour</span><br><span class="line">				- Iris-Virginica</span><br><span class="line">	:Summary Statistics:</span><br><span class="line"></span><br><span class="line">	============== ==== ==== ======= ===== ====================</span><br><span class="line">					Min  Max   Mean    SD   Class Correlation</span><br><span class="line">	============== ==== ==== ======= ===== ====================</span><br><span class="line">	sepal length:   4.3  7.9   5.84   0.83    0.7826</span><br><span class="line">	sepal width:    2.0  4.4   3.05   0.43   -0.4194</span><br><span class="line">	petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)</span><br><span class="line">	petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)</span><br><span class="line">	============== ==== ==== ======= ===== ====================</span><br><span class="line">.....</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>
<p><code>DESCR</code>详细的描述了鸢尾花这个数据集一共有150组数据，每组数据有4个特征，分别是萼片的长度和厚度、花瓣的长度和厚度，还有3种鸢尾花的类别以及这些数据的统计信息和详细的解释说明。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 再来看看data</span></span><br><span class="line">iris.data</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">5.1</span>,  <span class="number">3.5</span>,  <span class="number">1.4</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">4.9</span>,  <span class="number">3.</span> ,  <span class="number">1.4</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">4.7</span>,  <span class="number">3.2</span>,  <span class="number">1.3</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">4.6</span>,  <span class="number">3.1</span>,  <span class="number">1.5</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">5.</span> ,  <span class="number">3.6</span>,  <span class="number">1.4</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">5.4</span>,  <span class="number">3.9</span>,  <span class="number">1.7</span>,  <span class="number">0.4</span>],</span><br><span class="line">	   [ <span class="number">4.6</span>,  <span class="number">3.4</span>,  <span class="number">1.4</span>,  <span class="number">0.3</span>],</span><br><span class="line">	   [ <span class="number">5.</span> ,  <span class="number">3.4</span>,  <span class="number">1.5</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">4.4</span>,  <span class="number">2.9</span>,  <span class="number">1.4</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">4.9</span>,  <span class="number">3.1</span>,  <span class="number">1.5</span>,  <span class="number">0.1</span>],</span><br><span class="line">	   [ <span class="number">5.4</span>,  <span class="number">3.7</span>,  <span class="number">1.5</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   [ <span class="number">4.8</span>,  <span class="number">3.4</span>,  <span class="number">1.6</span>,  <span class="number">0.2</span>],</span><br><span class="line">	   ...</span><br><span class="line">	   ...</span><br><span class="line">	   [ <span class="number">5.9</span>,  <span class="number">3.</span> ,  <span class="number">5.1</span>,  <span class="number">1.8</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 看看data这个数组的行列情况</span></span><br><span class="line">iris.data.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到<code>data</code>中的数据就是萼片长度、厚度，花瓣长度、厚度的值。是一个150行，4列的矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># feature_names的值就是4个特征的说明</span></span><br><span class="line">iris.feature_names</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="string">'sepal length (cm)'</span>,</span><br><span class="line"> <span class="string">'sepal width (cm)'</span>,</span><br><span class="line"> <span class="string">'petal length (cm)'</span>,</span><br><span class="line"> <span class="string">'petal width (cm)'</span>]</span><br><span class="line"> </span><br><span class="line"> <span class="comment"># target描述了每一行鸢尾花的数据是哪个类别的</span></span><br><span class="line"> iris.target</span><br><span class="line"> <span class="comment"># 结果</span></span><br><span class="line"> array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">	   <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">	   <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">	   <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">	   <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">	   <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>,</span><br><span class="line">	   <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># target是一个一维数组</span></span><br><span class="line">iris.target.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">150</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># target_names就是类别名称</span></span><br><span class="line">iris.target_names</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="string">'setosa'</span>, <span class="string">'versicolor'</span>, <span class="string">'virginica'</span>],</span><br><span class="line">	  dtype=<span class="string">'&lt;U10'</span>)</span><br></pre></td></tr></table></figure>
<p>下面我们用Matplotlib，用图将鸢尾花的数据展示出来，这样就能更直观的来分析这些数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 因为matplotlib只能绘制二维图像，所以我们先来看看鸢尾花萼片的数据，取data的所有行，前2列</span></span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 将鸢尾花萼片的长度和宽度用散点图绘制出来</span></span><br><span class="line">plt.scatter(X[:, :<span class="number">1</span>], X[:, <span class="number">1</span>:<span class="number">2</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/9a08dffcd0e8a7fdf3639f1d9f3d874b.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们再来看看这150组鸢尾花数据从萼片维度的类别分类情况</span></span><br><span class="line">y = iris.target</span><br><span class="line">plt.scatter(X[y == <span class="number">0</span>, <span class="number">0</span>], X[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">"red"</span>)</span><br><span class="line">plt.scatter(X[y == <span class="number">1</span>, <span class="number">0</span>], X[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">"blue"</span>)</span><br><span class="line">plt.scatter(X[y == <span class="number">2</span>, <span class="number">0</span>], X[y == <span class="number">2</span>, <span class="number">1</span>], color=<span class="string">"green"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/d0124866e86843c1dffb61149fa4635b.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取data矩阵的所有行，后2列的数据，既鸢尾花的花瓣长度和宽度信息</span></span><br><span class="line">X1 = iris.data[:, <span class="number">2</span>:]</span><br><span class="line">plt.scatter(X1[:, :<span class="number">1</span>], X1[:, <span class="number">1</span>:<span class="number">2</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/99997a90dc85df95b77e4a5ab5abe444.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们再来看看这150组鸢尾花数据从花瓣维度的类别分类情况</span></span><br><span class="line">plt.scatter(X1[y == <span class="number">0</span>, <span class="number">0</span>], X1[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">"red"</span>)</span><br><span class="line">plt.scatter(X1[y == <span class="number">1</span>, <span class="number">0</span>], X1[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">"blue"</span>)</span><br><span class="line">plt.scatter(X1[y == <span class="number">2</span>, <span class="number">0</span>], X1[y == <span class="number">2</span>, <span class="number">1</span>], color=<span class="string">"green"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/8eee090275f82623f8794f8fc1dcf7af.jpg" alt=""></p>
<h2 id="kNN_u7B97_u6CD5"><a href="#kNN_u7B97_u6CD5" class="headerlink" title="kNN算法"></a>kNN算法</h2><p>kNN算法又称k近邻算法，是k-Nearest Neighbors的简称，该算法是监督学习中解决分类问题的算法，也是需要数据知识最少的一个算法，但是效果往往不差，能较好的解释机器学习算法使用过程中的很多细节问题，并且能很好的刻画机器学习应用的流程。</p>
<h3 id="kNN_u7B97_u6CD5_u89E3_u91CA"><a href="#kNN_u7B97_u6CD5_u89E3_u91CA" class="headerlink" title="kNN算法解释"></a>kNN算法解释</h3><p><img src="http://paxigrdp0.bkt.clouddn.com/8a94067b9758b0189df1881425d22ab5.jpg" alt=""></p>
<p>上图描述了肿瘤大小和时间的二维关系图，圆点的颜色表示肿瘤的性质是良性还是恶性。</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/1ba0838f5f698887a031814ce2655907.jpg" alt=""></p>
<p>此时又有一个病人的数据采集到，那么我们如何判断这个病人的肿瘤是良性还是恶性呢？</p>
<p>首先我们必须取一个<code>k</code>值，至于这个<code>k</code>值是该如何取后续会讲，这里比如我们取<code>k=3</code>，这个<code>k</code>值的作用就是基于新来的这个点，找到离它最近的<code>k</code>个点，这里也就是找到离绿色点最近的三个点：</p>
<p><img src="http://paxigrdp0.bkt.clouddn.com/28dadf5ffe30432c477b34876eb2a255.jpg" alt=""></p>
<p>然后根据这三个代表的特征进行投票，票数最多的特征就是这个绿色点的特征，这个示例中离绿色点最近的三个点都是蓝色点，既恶性肿瘤，那么可判定绿色点代表的肿瘤性质有很高的概率也是恶性。</p>
<h3 id="u6B27_u62C9_u8DDD_u79BB"><a href="#u6B27_u62C9_u8DDD_u79BB" class="headerlink" title="欧拉距离"></a>欧拉距离</h3><p>kNN算法中唯一用到的数学知识就是如何求点与点之间的距离，在这里我们先使用最普遍的欧拉距离来进行计算，欧拉距离的公式如下：</p>
<p>二维：<br>$$ \sqrt {(x^{(a)}-x^{(b)})^2+(y^{(a)}-y^{(b)})^2} $$</p>
<p>三维：<br>$$ \sqrt {(x^{(a)}-x^{(b)})^2+(y^{(a)}-y^{(b)})^2+(z^{(a)}-z^{(b)})^2} $$</p>
<p>N维(N个特征)：</p>
<p>$$ \sqrt {(x_1^{(a)}-x_1^{(b)})^2+(x_2^{(a)}-x_2^{(b)})^2+…+(x_n^{(a)}-x_n^{(b)})^2} =\sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} $$</p>
<p>用大白话解释就是两个点的所有相同维度之差求平方，然后全部相加再开方。有兴趣的话大家可以再深入研究一下点与点间距离的计算。</p>
<h3 id="u7F16_u7801_u5B9E_u73B0kNN_u7B97_u6CD5"><a href="#u7F16_u7801_u5B9E_u73B0kNN_u7B97_u6CD5" class="headerlink" title="编码实现kNN算法"></a>编码实现kNN算法</h3><p>首先我们来准备一下数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">raw_data_X = [[<span class="number">3.393533211</span>, <span class="number">2.331273381</span>],</span><br><span class="line">			  [<span class="number">3.110073483</span>, <span class="number">1.781539638</span>],</span><br><span class="line">			  [<span class="number">1.343808831</span>, <span class="number">3.368360954</span>],</span><br><span class="line">			  [<span class="number">3.582294042</span>, <span class="number">4.679179110</span>],</span><br><span class="line">			  [<span class="number">2.280362439</span>, <span class="number">2.866990263</span>],</span><br><span class="line">			  [<span class="number">7.423436942</span>, <span class="number">4.696522875</span>],</span><br><span class="line">			  [<span class="number">5.745051997</span>, <span class="number">3.533989803</span>],</span><br><span class="line">			  [<span class="number">9.172168622</span>, <span class="number">2.511101045</span>],</span><br><span class="line">			  [<span class="number">7.792783481</span>, <span class="number">3.424088941</span>],</span><br><span class="line">			  [<span class="number">7.939820817</span>, <span class="number">0.791637231</span>]</span><br><span class="line">			 ]</span><br><span class="line">raw_data_y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>我们将这些样本数据绘制出来看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 和绘制鸢尾花的方式一样</span></span><br><span class="line">X_train = np.array(raw_data_X)</span><br><span class="line">y_train = np.array(raw_data_y)</span><br><span class="line">plt.scatter(X_train[y_train == <span class="number">0</span>, <span class="number">0</span>], X_train[y_train == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">"red"</span>)</span><br><span class="line">plt.scatter(X_train[y_train == <span class="number">1</span>, <span class="number">0</span>], X_train[y_train == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">"blue"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/da457209ab37d3c015afeb7ab4b3796f.jpg" alt=""></p>
<p>下面再创建一组数据，来模拟需要被分类的数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.array([<span class="number">8.093607318</span>, <span class="number">3.365731514</span>])</span><br><span class="line">plt.scatter(X_train[y_train == <span class="number">0</span>, <span class="number">0</span>], X_train[y_train == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">"red"</span>)</span><br><span class="line">plt.scatter(X_train[y_train == <span class="number">1</span>, <span class="number">0</span>], X_train[y_train == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">"blue"</span>)</span><br><span class="line">plt.scatter(x[<span class="number">0</span>], x[<span class="number">1</span>], color=<span class="string">"green"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://paxigrdp0.bkt.clouddn.com/4dfa48281a765f4c476d1e7a16a91119.jpg" alt=""></p>
<p>我们现在就要通过kNN算法来分析这个绿点属于哪个类别，虽然从图上我们已经可以看得出。</p>
<p>我们先来通过欧拉距离公式求出所有点与绿点的距离：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入开方的类库</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line">distances = []</span><br><span class="line"><span class="comment"># 通过for循环求出每一个点与绿点的距离</span></span><br><span class="line"><span class="keyword">for</span> x_train <span class="keyword">in</span> X_train:</span><br><span class="line">	d = sqrt(np.sum((x_train - x) ** <span class="number">2</span>))</span><br><span class="line">	distances.append(d)</span><br><span class="line">	</span><br><span class="line">distances</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">4.812566907609877</span>,</span><br><span class="line"> <span class="number">5.229270827235305</span>,</span><br><span class="line"> <span class="number">6.749798999160064</span>,</span><br><span class="line"> <span class="number">4.6986266144110695</span>,</span><br><span class="line"> <span class="number">5.83460014556857</span>,</span><br><span class="line"> <span class="number">1.4900114024329525</span>,</span><br><span class="line"> <span class="number">2.354574897431513</span>,</span><br><span class="line"> <span class="number">1.3761132675144652</span>,</span><br><span class="line"> <span class="number">0.3064319992975</span>,</span><br><span class="line"> <span class="number">2.5786840957478887</span>]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> <span class="comment"># 其实还可以更简洁的使用行内表达式</span></span><br><span class="line"> distances = [sqrt(np.sum((x_train - x) ** <span class="number">2</span>)) <span class="keyword">for</span> x_train <span class="keyword">in</span> X_train]</span><br><span class="line"> distances</span><br><span class="line"> <span class="comment"># 结果是一样的</span></span><br><span class="line"> [<span class="number">4.812566907609877</span>,</span><br><span class="line"> <span class="number">5.229270827235305</span>,</span><br><span class="line"> <span class="number">6.749798999160064</span>,</span><br><span class="line"> <span class="number">4.6986266144110695</span>,</span><br><span class="line"> <span class="number">5.83460014556857</span>,</span><br><span class="line"> <span class="number">1.4900114024329525</span>,</span><br><span class="line"> <span class="number">2.354574897431513</span>,</span><br><span class="line"> <span class="number">1.3761132675144652</span>,</span><br><span class="line"> <span class="number">0.3064319992975</span>,</span><br><span class="line"> <span class="number">2.5786840957478887</span>]</span><br></pre></td></tr></table></figure>
<p>现在我们就求出了所有点与绿点的距离，但是求出距离并不能表示出每个距离对应点的类别，所以还需要知道这每个距离对应的是哪个点：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此时上文中说过的argsort方法就派上用场了</span></span><br><span class="line"><span class="comment"># argsort方法可以将数组排序，但是返回数组中元素的索引</span></span><br><span class="line">nearest = np.argsort(distances)</span><br><span class="line">nearest</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">8</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">In [ ]:</span><br></pre></td></tr></table></figure>
<p>从上面的结果可以看到，距离绿点最近的点是<code>X_train</code>中的第8行样本。那么接下来我们看看如何通过<code>k</code>值获取这些点的类别：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先定义k的值为6</span></span><br><span class="line">k = <span class="number">6</span></span><br><span class="line"><span class="comment"># 离绿点距离最近的前6个点的类别</span></span><br><span class="line">topK_y = [y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:k]]</span><br><span class="line">topK_y</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入collections的Counter类库用于计数计算</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">votes = Counter(topK_y)</span><br><span class="line">votes</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">Counter(&#123;<span class="number">0</span>: <span class="number">1</span>, <span class="number">1</span>: <span class="number">5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过most_common方法获取最大的若干个结果，传入的参数为想要获取最大的几个结果，这里我们只需要最大的一个结果，既投票票数最多的那个结果</span></span><br><span class="line">votes.most_common(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[(<span class="number">1</span>, <span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更精准的获取投票票数最多的结果</span></span><br><span class="line">predict_y = votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">predict_y</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>到目前位置，我们就判断出了绿点有很大概率类别属于1，这个过程就是kNN算法的核心过程。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>在下一篇笔记中，将会介绍Scikit Learn中是如何封装kNN算法的，以及我们会自己封装一个kNN算法，以及对分类准确度评定，超参数，数据归一化等知识点的讲解。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h2><p>NumPy是Python中的一个类库，它支持高阶维度数组（矩阵）的创建及各种操作、运算，是我们在机器学习中经常会使用的一个类库。这一节再看一些NumPy的矩阵用法。</p>
<h3 id="numpy-random"><a href="#numpy-random" class="headerlink" title="numpy.random"></a>numpy.random</h3><p>NumPy也提供了生成随机数和随机元素数组的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成从0到10之间的随机数</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成元素从0到10，一共4个随机元素的数组</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成元素随机从0到10，3行5列的矩阵</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">6</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<p>如果我们希望每次使用随机方法生成的结果都是一样的，一般调试时候有这个需求，此时NumPy的<code>random()</code>方法也提供了方便简单的方式，既随机种子的概念：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成随机矩阵前给定一个种子</span></span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"><span class="comment"># 然后生成随机矩阵</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 再次生成随机矩阵时，只要传入相同的种子，就可以得到相同结果的矩阵</span></span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 默认范围是从0.0到1.0，返回值为float型</span></span><br><span class="line">np.random.random()</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.18249173045349998</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 传入的参数是数组的大小</span></span><br><span class="line">np.random.random(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.17545176</span>,  <span class="number">0.53155137</span>,  <span class="number">0.53182759</span>,  <span class="number">0.63440096</span>,  <span class="number">0.84943179</span>,</span><br><span class="line">		<span class="number">0.72445532</span>,  <span class="number">0.61102351</span>,  <span class="number">0.72244338</span>,  <span class="number">0.32295891</span>,  <span class="number">0.36178866</span>])</span><br><span class="line">		</span><br><span class="line"><span class="comment"># 创建4行5列，元素值的范围从0.0到1.0的矩阵</span></span><br><span class="line">np.random.random((<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.22826323</span>,  <span class="number">0.29371405</span>,  <span class="number">0.63097612</span>,  <span class="number">0.09210494</span>,  <span class="number">0.43370117</span>],</span><br><span class="line">	   [ <span class="number">0.43086276</span>,  <span class="number">0.4936851</span> ,  <span class="number">0.42583029</span>,  <span class="number">0.31226122</span>,  <span class="number">0.42635131</span>],</span><br><span class="line">	   [ <span class="number">0.89338916</span>,  <span class="number">0.94416002</span>,  <span class="number">0.50183668</span>,  <span class="number">0.62395295</span>,  <span class="number">0.1156184</span> ],</span><br><span class="line">	   [ <span class="number">0.31728548</span>,  <span class="number">0.41482621</span>,  <span class="number">0.86630916</span>,  <span class="number">0.25045537</span>,  <span class="number">0.48303426</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="u6307_u5B9A_u5747_u503C_u548C_u6807_u51C6_u5DEE_u751F_u6210_u968F_u673A_u6570_u6570_u7EC4_u6216_u77E9_u9635"><a href="#u6307_u5B9A_u5747_u503C_u548C_u6807_u51C6_u5DEE_u751F_u6210_u968F_u673A_u6570_u6570_u7EC4_u6216_u77E9_u9635" class="headerlink" title="指定均值和标准差生成随机数数组或矩阵"></a>指定均值和标准差生成随机数数组或矩阵</h3><p>我们先来看看均值、方差、标准差的概念。均值很好理解，就是所有样本数据的平均值，描述了样本集合的中间点：</p>
<p>$$ \overline X=\frac{\sum_{i=1}^nX_i}n $$</p>
<p>方差是衡量样本点和样本期望值相差的度量值：</p>
<p>$$ S^2 = \frac{\sum_{i=1}^n(X_i-\overline X)^2} n $$</p>
<p>标准差描述的是样本集合的各个样本点到均值的距离之平均：</p>
<p>$$ S = \sqrt {\frac{\sum_{i=1}^n(X_i-\overline X)^2} n } $$</p>
<p>标准差也就是对方差开根号。举个例子，<code>[0, 8, 12, 20]</code>和<code>[8, 9, 11, 12]</code>，两个集合的均值都是10，但显然两个集合的差别是很大的，计算两者的标准差，前者是8.3后者是1.8，显然后者较为集中，标准差描述的就是这种散布度或者叫做波动大小。综上，方差的意义在于描述随机变量稳定与波动、集中与分散的状况。标准差则体现随机变量取值与其期望值的偏差。</p>]]>
    
    </summary>
    
      <category term="Matplotlib" scheme="http://www.devtalking.com/tags/Matplotlib/"/>
    
      <category term="kNN" scheme="http://www.devtalking.com/tags/kNN/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记二之矩阵、环境搭建、NumPy]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-2/"/>
    <id>http://www.devtalking.com//articles/machine-learning-2/</id>
    <published>2018-01-12T16:00:00.000Z</published>
    <updated>2018-08-27T02:28:42.631Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u77E9_u9635"><a href="#u77E9_u9635" class="headerlink" title="矩阵"></a>矩阵</h2><p>因为在机器学习中，一些概念、算法都涉及到矩阵的知识，比如训练数据集通常都是以矩阵的方式存在，所以在这里首先介绍一下矩阵的概念。</p>
<h3 id="u57FA_u672C_u6982_u5FF5"><a href="#u57FA_u672C_u6982_u5FF5" class="headerlink" title="基本概念"></a>基本概念</h3><p>在数学概念中，一个$m \times n$的矩阵指的是一个由$m$行$n$列元素排列而成的矩形阵列。用大白话解释就是将一些元素排列成若干行，每行放上相同数量的元素，就是一个矩阵。矩阵里的元素可以是数字、符号或数学式，比如下面这个$A$矩阵就是一个四行两列的矩阵：<br>$$ A=\begin{bmatrix}<br> 1&amp;2\\<br> 3&amp;4\\<br> 5&amp;6\\<br> 7&amp;8\\<br>\end{bmatrix}$$</p>
<p>一个矩阵$A$从左上角数起的第$i$行第$j$列上的元素称为第$i$，$j$项，通常记为$A_{i,j}$、$A_{ij}$或者$A_{[i,j]}$，那么上面的$A$矩阵中$A_{3,2}=6$。如果我们不知道矩阵$A$的具体元素，通常将其表示为$A=[a_{ij}]_{m \times n}$，如果$A$的元素可以写成与行$i$和列$j$有关的统一函数$f$，那么也可以用$A=[f(i,j)]_{m \times n}$来表示。</p>
<h3 id="u77E9_u9635_u7684_u57FA_u672C_u8FD0_u7B97"><a href="#u77E9_u9635_u7684_u57FA_u672C_u8FD0_u7B97" class="headerlink" title="矩阵的基本运算"></a>矩阵的基本运算</h3><p>矩阵的最基本运算包括矩阵加（减）法、数乘、转置、矩阵乘法运算。</p>
<h4 id="u52A0_u51CF_u6CD5_u8FD0_u7B97"><a href="#u52A0_u51CF_u6CD5_u8FD0_u7B97" class="headerlink" title="加减法运算"></a>加减法运算</h4><p>$m \times n$矩阵$A$和$B$的和（差）$A \pm B$也是一个$m \times n$矩阵，其中每个元素是$A$和$B$相应元素的和（差），即$ (A \pm B)_{ij}=A_{ij} \pm B_{ij} $，其中$1 \le i \le m$，$1 \le j \le n$。举个例子：</p>
<p>$$ \begin{bmatrix}<br>1&amp;2&amp;3\\<br>4&amp;5&amp;6\\<br>\end{bmatrix} + \begin{bmatrix}<br>6&amp;5&amp;4\\<br>3&amp;2&amp;1\\<br>\end{bmatrix} = \begin{bmatrix}<br>1+6 &amp; 2+5 &amp; 3+4 \\<br>4+3 &amp; 5+2 &amp; 6+1 \\<br>\end{bmatrix}=\begin{bmatrix}<br>7&amp;7&amp;7\\<br>7&amp;7&amp;7\\<br>\end{bmatrix}<br> $$</p>
<h4 id="u6570_u4E58_u8FD0_u7B97"><a href="#u6570_u4E58_u8FD0_u7B97" class="headerlink" title="数乘运算"></a>数乘运算</h4><p> 标量$c$与$m \times n$的矩阵$A$的数乘也是一个$m \times n$的矩阵$cA$，它的每个元素是矩阵$A$的相应元素与$c$的乘积$(cA)_{ij}=c \cdot A_{ij}$，举个例子：</p>
<p> $$<br> 2 \cdot \begin{bmatrix}<br> 1&amp;2&amp;3\\<br> 4&amp;5&amp;6\\<br> \end{bmatrix}=\begin{bmatrix}<br> 2 \cdot 1 &amp; 2 \cdot 2 &amp; 2 \cdot 3 \\<br> 2 \cdot 4 &amp; 2 \cdot 5 &amp; 2 \cdot 6 \\<br> \end{bmatrix}=\begin{bmatrix}<br> 2&amp;4&amp;6\\<br> 8&amp;10&amp;12\\<br> \end{bmatrix}<br>  $$</p>
  <a id="more"></a>
<h4 id="u8F6C_u7F6E"><a href="#u8F6C_u7F6E" class="headerlink" title="转置"></a>转置</h4><p>$m \times n$的矩阵$A$的转置是一个$n \times m$的矩阵，记为$A^T$，其中的第$i$个行向量是原矩阵$A$的第$i$个列向量，或者说转置矩阵$A^T$第$i$行第$j$列的元素是原矩阵$A$的第$j$行第$i$列，即$(A^T)_{ij}=A_{ji}$，举个例子：</p>
<p>$$ \begin{bmatrix}<br>1&amp;2&amp;3\\<br>4&amp;5&amp;6\\<br>\end{bmatrix}^T=\begin{bmatrix}<br>1&amp;4\\<br>2&amp;5\\<br>3&amp;6\\<br>\end{bmatrix}<br> $$</p>
<h4 id="u77E9_u9635_u4E58_u6CD5"><a href="#u77E9_u9635_u4E58_u6CD5" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h4><p>两个矩阵的乘法仅当第一个矩阵$A$的列数和另一个矩阵$B$的行数相等时才能定义。比如$m \times n$的矩阵$A$与$n \times p$的矩阵$B$的乘积$AB$是一个$m \times p$的矩阵，即矩阵$AB$的元素为：</p>
<p>$$ [AB]_{ij}=A_{i1}B_{1j}+A_{i2}B_{2j}+…+A_{in}B_{nj}=\sum_{r=1}^n{A_{ir}B_{rj}} $$</p>
<p>举个例子：</p>
<p>$$ \begin{bmatrix}<br>1&amp;2&amp;3\\<br>4&amp;5&amp;6\\<br>\end{bmatrix} \times \begin{bmatrix}<br>6&amp;5\\<br>4&amp;3\\<br>2&amp;1\\<br>\end{bmatrix}=\\<br>\begin{bmatrix}<br>(1 \times 6 + 2 \times 4 + 3 \times 2) &amp; (1 \times 5 + 2 \times 3 + 3 \times 1)  \\<br>(4 \times 6 + 5 \times 4 + 6 \times 2) &amp; (4 \times 5 + 5 \times 3 + 6 \times 1) \\<br>\end{bmatrix} =\\<br>\begin{bmatrix}<br>20&amp;14\\<br>56&amp;41\\<br>\end{bmatrix}<br>$$</p>
<h2 id="u73AF_u5883_u642D_u5EFA"><a href="#u73AF_u5883_u642D_u5EFA" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>目前，Python语言是处理大数据量运算比较好的语言之一，大数据分析、计算，机器学习的主流语言和技术栈基本都是以Python为主，所以我们在搭建机器学习入门编程环境时同样使用Python3。在Python生态圈中，目前有非常丰富和健壮的机器学习的类库和框架，如果我们手动一点点配置，其实是非常繁琐的，而且容易出错。所幸，现在有很多集成工具能傻瓜式的帮我们搭建好机器学习的环境，其中就包括各种需要的类库、框架，以及他们之间的依赖关系等。那么在这里推荐著名的Anaconda。</p>
<h3 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h3><p>Anaconda是一个Python包和Python相关工具安装的管理器。它的安装过程非常简单，只需要访问其<a href="https://anaconda.org" target="_blank" rel="external">官方网站</a>下载对应操作系统的安装包既可，这里要注意的是我们需要下载Python 3.6版本的Anaconda。安装好后，运行Anaconda，会显示如图所示的主界面：<br><img src="http://p07npm071.bkt.clouddn.com/2018-01-23-anaconda%E4%B8%BB%E7%95%8C%E9%9D%A2.png" alt=""></p>
<p>在界面左侧就会看到有Home、Environments、Projects等导航。Home中显示的就是由Anaconda提供好的一些工具，可以直接运行或者安装。Environments中就是用来管理各种Python包的地方。我们在机器学习入门阶段会使用名为Jupyter Notebook的工具。</p>
<h2 id="Jupyter_Notebook"><a href="#Jupyter_Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h2><p>我们在Anaconda的Home界面就可以看到Jupyter Notebook工具，该工具其实就是一个基于Web的Python编辑器，可以编写Python代码，然后实时编译运行得出结果，相比PyCharm这种完善的IDE工具，Jupyter Notebook显得非常轻量级，但是在进行算法演练和学习Python各种类库时非常方便，另外Jupyter Notebook在编辑器中除了支持Python语言外，还支持基于Markdown语法的文本编写。我们点击Jupyter Notebook工具界面中的Launch，就会由系统默认浏览器自动打开Jupyter Notebook界面：<br><img src="http://p07npm071.bkt.clouddn.com/2018-01-23-jupyternotebook%E4%B8%BB%E7%95%8C%E9%9D%A2.png" alt=""></p>
<p>大家运行Jupyter Notebook，把玩一会后就可以发现，它其实就是使用浏览器通过Web技术，将当前用户下的目录结构展示出来，并且可以新建目录、文件，然后在文件中进行编码的工具，整个运行环境依托于Anaconda。</p>
<h3 id="Jupyter_Notebook_u57FA_u672C_u6982_u5FF5_u548C_u64CD_u4F5C_u65B9_u5F0F"><a href="#Jupyter_Notebook_u57FA_u672C_u6982_u5FF5_u548C_u64CD_u4F5C_u65B9_u5F0F" class="headerlink" title="Jupyter Notebook基本概念和操作方式"></a>Jupyter Notebook基本概念和操作方式</h3><p>Jupyter Notebook中的核心概念是Cell，可以理解为输入最小单元行，每个Cell支持两种模式的输入，一种是Python代码，另一种的Markdown语法的文本。每行Cell编辑完成后按下ctrl+回车或者command+回车，就可以运行Cell里的内容，如果是Python代码就会运行代码，如果是基于Markdown语法的文本，那么就会按照一定格式渲染文本：<br><img src="http://p07npm071.bkt.clouddn.com/jupyternotebook%E7%A4%BA%E4%BE%8B.png" alt=""></p>
<p>Jupyter Notebook还有很多的快捷键操作，这里就不再一一赘述。后续有相当一部分代码我都会在Jupyter Notebook中进行编码，一些使用技巧和知识点在讲述其他内容时一并讲解。所以不论是在学习机器学习的过程中还是学习Python的过程中，Jupyter Notebook都会是一个非常帮的工具。</p>
<h2 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h2><p>NumPy是Python中的一个类库，它支持高阶维度数组（矩阵）的创建及各种操作、运算，是我们在机器学习中经常会使用的一个类库。这一节主要讲一下如何使用NumPy。</p>
<h3 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array"></a>numpy.array</h3><p>NumPy中的核心数据结构是数组，可以非常方便的创建、操作数组，并支持多维数组，多维数组就可以看作是矩阵。<code>numpy.array</code>与Python中的<code>List</code>的不同之处在于，前者的元素类型是有限定的，而后者的元素类型没有限定。我们先来看看Python的<code>List</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Python的List</span></span><br><span class="line">l = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">l</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 取值</span></span><br><span class="line">l[<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值</span></span><br><span class="line">l[<span class="number">5</span>] = <span class="number">100</span></span><br><span class="line">l</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">100</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值其他类型</span></span><br><span class="line">l[<span class="number">6</span>] = <span class="string">"hello world!"</span></span><br><span class="line">l</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">100</span>, <span class="string">'hello world!'</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<p>Python中的<code>List</code>虽然灵活，但是由于每个元素的类型是不限定的，所以性能和效率会比较差。其实Python中也有元素类型限定的数组那就是<code>array</code>，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Python中的array</span></span><br><span class="line"><span class="keyword">import</span> array</span><br><span class="line">arr = array.array(<span class="string">'i'</span>, [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line">arr</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array(<span class="string">'i'</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取值</span></span><br><span class="line">arr[<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值</span></span><br><span class="line">arr[<span class="number">6</span>] = <span class="string">"hello world!"</span></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">TypeError: an integer <span class="keyword">is</span> required (got type str)</span><br></pre></td></tr></table></figure>
<p>但是Python中的<code>array</code>并没有将一维数组当做向量、多维数组当做矩阵来看，自然也没有提供任何对多维数组的矩阵操作。所以NumPy类库就应运而生了。我们再来看看NumPy中的<code>array</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NumPy中的array</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">nparr = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line">nparr</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取值</span></span><br><span class="line">nparr[<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值</span></span><br><span class="line">nparr[<span class="number">6</span>] = <span class="number">100</span></span><br><span class="line">nparr</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([  <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">2</span>,   <span class="number">3</span>,   <span class="number">4</span>,   <span class="number">5</span>, <span class="number">100</span>,   <span class="number">7</span>,   <span class="number">8</span>,   <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 赋值其他类型</span></span><br><span class="line">nparr[<span class="number">7</span>] = <span class="string">"hello world!"</span></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">ValueError: invalid literal <span class="keyword">for</span> int() <span class="keyword">with</span> base <span class="number">10</span>: <span class="string">'hello world!'</span></span><br></pre></td></tr></table></figure>
<h3 id="numpy-array_u4E2D_u7684_u5143_u7D20_u7C7B_u578B"><a href="#numpy-array_u4E2D_u7684_u5143_u7D20_u7C7B_u578B" class="headerlink" title="numpy.array中的元素类型"></a>numpy.array中的元素类型</h3><p>因为<code>numpy.array</code>中的元素类型是限定的，所以这一小节我们来看看和元素类型相关的方法和概念：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数组中的元素类型</span></span><br><span class="line">nparr.dtype</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">dtype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给元素类型是int的数组，赋值一个float类型的值，会强制转换为int</span></span><br><span class="line">nparr[<span class="number">2</span>] = <span class="number">3.1</span></span><br><span class="line">nparr</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([  <span class="number">0</span>,   <span class="number">1</span>,   <span class="number">3</span>,   <span class="number">3</span>,   <span class="number">4</span>,   <span class="number">5</span>, <span class="number">100</span>,   <span class="number">7</span>,   <span class="number">8</span>,   <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果在创建数组时，有一个元素类型为float，那么该数组的所以元素类型会是float</span></span><br><span class="line">nparr2 = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3.1</span>])</span><br><span class="line">nparr2</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.</span> ,  <span class="number">1.</span> ,  <span class="number">2.</span> ,  <span class="number">3.1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看nparr2的元素类型</span></span><br><span class="line">nparr2.dtype</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">dtype(<span class="string">'float64'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="u521B_u5EFAnumpy-array_u7684_u5176_u4ED6_u65B9_u6CD5"><a href="#u521B_u5EFAnumpy-array_u7684_u5176_u4ED6_u65B9_u6CD5" class="headerlink" title="创建numpy.array的其他方法"></a>创建numpy.array的其他方法</h3><p>NumPy还提供了丰富的能快捷创建数组的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建元素全部为0的数组，参数为数组元素的个数</span></span><br><span class="line">nparr3 = np.zeros(<span class="number">10</span>)</span><br><span class="line">nparr3</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用np.zeros创建的数组，元素类型默认为float</span></span><br><span class="line">nparr3.dtype</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">dtype(<span class="string">'float64'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想指定元素的类型，可以设置第二个参数</span></span><br><span class="line">nparr4 = np.zeros(<span class="number">10</span>, dtype=int)</span><br><span class="line">nparr4.dtype</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">dtype(<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建元素全部为1的数组</span></span><br><span class="line">nparr5 = np.ones(<span class="number">10</span>)</span><br><span class="line">nparr5</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<h3 id="u4F7F_u7528numpy_u521B_u5EFA_u77E9_u9635"><a href="#u4F7F_u7528numpy_u521B_u5EFA_u77E9_u9635" class="headerlink" title="使用numpy创建矩阵"></a>使用numpy创建矩阵</h3><p>NumPy会把二维数组看作一个矩阵来处理，我们来看看如何创建二维数组：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 传入二元元组参数，给定行数和列数</span></span><br><span class="line">nparr6 = np.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">nparr6</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">	   [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 如果要指定类型的话，需要显示使用shape这个参数，传入二元元组，给定行数和列数</span></span><br><span class="line">nparr6 = np.zeros(shape=(<span class="number">3</span>, <span class="number">4</span>), dtype=int)</span><br><span class="line">nparr6</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">	   [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 创建两行四列，元素全部为1的矩阵    </span></span><br><span class="line">nparr7 = np.ones((<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">nparr7</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">	   [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 创建三行五列，元素为指定值的矩阵</span></span><br><span class="line">nparr8 = np.full(shape=(<span class="number">3</span>, <span class="number">5</span>), fill_value=<span class="number">100</span>)</span><br><span class="line">nparr8</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">	   [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">	   [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange"></a>numpy.arange</h3><p>在上文中，我们使用了<code>[i for i in range(0, 20, 2)]</code>这种方式创建了Python的<code>List</code>，其中用到了<code>range()</code>这个方法，该方法有三个参数，用大白话解释就是通过<code>range()</code>创建一个池子，这个池子里的第一个元素大于等于第一个参数的值，最后一个元素小于等于第二个参数的值，元素之间的关系由第三个参数的值决定，第三个参数也称为步长：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个数组，其中的元素大于等于0，小于等于20，每个元素相差为2</span></span><br><span class="line">[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">20</span>, <span class="number">2</span>)]</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>]</span><br></pre></td></tr></table></figure>
<p>NumPy也提供了类似的方法<code>arange()</code>，它的优势是步长支持浮点型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建范围从0到1，步长为0.2的数组</span></span><br><span class="line">np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.</span> ,  <span class="number">0.2</span>,  <span class="number">0.4</span>,  <span class="number">0.6</span>,  <span class="number">0.8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果只传入一个参数，那么表示默认元素从0开始，传入的参数为数组的大小</span></span><br><span class="line">np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<h3 id="numpy-linspace"><a href="#numpy-linspace" class="headerlink" title="numpy.linspace"></a>numpy.linspace</h3><p>NumPy中的<code>linspace()</code>方法可以让我们不用考虑步长，只考虑希望从起始值到终止值的范围内，创建多少个元素，步长会自动进行计算，这里要注意的是linspace方法是包含起始值和终止值的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一个参数是起始值，第二个参数是终止值，第三参数是数组大小</span></span><br><span class="line">np.linspace(<span class="number">0</span>, <span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([  <span class="number">0.</span>        ,   <span class="number">2.22222222</span>,   <span class="number">4.44444444</span>,   <span class="number">6.66666667</span>,</span><br><span class="line">		 <span class="number">8.88888889</span>,  <span class="number">11.11111111</span>,  <span class="number">13.33333333</span>,  <span class="number">15.55555556</span>,</span><br><span class="line">		<span class="number">17.77777778</span>,  <span class="number">20.</span>        ])</span><br></pre></td></tr></table></figure>
<h3 id="numpy-random"><a href="#numpy-random" class="headerlink" title="numpy.random"></a>numpy.random</h3><p>NumPy也提供了生成随机数和随机元素数组的方法，我们来看一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成从0到10之间的随机数</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成元素从0到10，一共4个随机元素的数组</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成元素随机从0到10，3行5列的矩阵</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">6</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">9</span>],</span><br><span class="line">	   [<span class="number">7</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<p>如果我们希望每次使用随机方法生成的结果都是一样的，一般调试时候有这个需求，此时NumPy的<code>random()</code>方法也提供了方便简单的方式，既随机种子的概念：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成随机矩阵前给定一个种子</span></span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"><span class="comment"># 然后生成随机矩阵</span></span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 再次生成随机矩阵时，只要传入相同的种子，就可以得到相同结果的矩阵</span></span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line">np.random.randint(<span class="number">0</span>, <span class="number">10</span>, size=(<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">	   [<span class="number">9</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">3</span>],</span><br><span class="line">	   [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>]])</span><br><span class="line">	   </span><br><span class="line"><span class="comment"># 默认范围是从0.0到1.0，返回值为float型</span></span><br><span class="line">np.random.random()</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.18249173045349998</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 传入的参数是数组的大小</span></span><br><span class="line">np.random.random(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([ <span class="number">0.17545176</span>,  <span class="number">0.53155137</span>,  <span class="number">0.53182759</span>,  <span class="number">0.63440096</span>,  <span class="number">0.84943179</span>,</span><br><span class="line">		<span class="number">0.72445532</span>,  <span class="number">0.61102351</span>,  <span class="number">0.72244338</span>,  <span class="number">0.32295891</span>,  <span class="number">0.36178866</span>])</span><br><span class="line">		</span><br><span class="line"><span class="comment"># 创建4行5列，元素值的范围从0.0到1.0的矩阵</span></span><br><span class="line">np.random.random((<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([[ <span class="number">0.22826323</span>,  <span class="number">0.29371405</span>,  <span class="number">0.63097612</span>,  <span class="number">0.09210494</span>,  <span class="number">0.43370117</span>],</span><br><span class="line">	   [ <span class="number">0.43086276</span>,  <span class="number">0.4936851</span> ,  <span class="number">0.42583029</span>,  <span class="number">0.31226122</span>,  <span class="number">0.42635131</span>],</span><br><span class="line">	   [ <span class="number">0.89338916</span>,  <span class="number">0.94416002</span>,  <span class="number">0.50183668</span>,  <span class="number">0.62395295</span>,  <span class="number">0.1156184</span> ],</span><br><span class="line">	   [ <span class="number">0.31728548</span>,  <span class="number">0.41482621</span>,  <span class="number">0.86630916</span>,  <span class="number">0.25045537</span>,  <span class="number">0.48303426</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>这篇笔记回顾了机器学习中会大量使用的矩阵的知识，以及在本地学习机器学习算法的最佳环境配置，最后介绍了Python中对矩阵封装最好的库NumPy的用法。下篇笔记会继续学习NumPy的用法以及绘图库Matplotlib库，以及机器学习的第一个算法KNN。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经过允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u77E9_u9635"><a href="#u77E9_u9635" class="headerlink" title="矩阵"></a>矩阵</h2><p>因为在机器学习中，一些概念、算法都涉及到矩阵的知识，比如训练数据集通常都是以矩阵的方式存在，所以在这里首先介绍一下矩阵的概念。</p>
<h3 id="u57FA_u672C_u6982_u5FF5"><a href="#u57FA_u672C_u6982_u5FF5" class="headerlink" title="基本概念"></a>基本概念</h3><p>在数学概念中，一个$m \times n$的矩阵指的是一个由$m$行$n$列元素排列而成的矩形阵列。用大白话解释就是将一些元素排列成若干行，每行放上相同数量的元素，就是一个矩阵。矩阵里的元素可以是数字、符号或数学式，比如下面这个$A$矩阵就是一个四行两列的矩阵：<br>$$ A=\begin{bmatrix}<br> 1&amp;2\\<br> 3&amp;4\\<br> 5&amp;6\\<br> 7&amp;8\\<br>\end{bmatrix}$$</p>
<p>一个矩阵$A$从左上角数起的第$i$行第$j$列上的元素称为第$i$，$j$项，通常记为$A_{i,j}$、$A_{ij}$或者$A_{[i,j]}$，那么上面的$A$矩阵中$A_{3,2}=6$。如果我们不知道矩阵$A$的具体元素，通常将其表示为$A=[a_{ij}]_{m \times n}$，如果$A$的元素可以写成与行$i$和列$j$有关的统一函数$f$，那么也可以用$A=[f(i,j)]_{m \times n}$来表示。</p>
<h3 id="u77E9_u9635_u7684_u57FA_u672C_u8FD0_u7B97"><a href="#u77E9_u9635_u7684_u57FA_u672C_u8FD0_u7B97" class="headerlink" title="矩阵的基本运算"></a>矩阵的基本运算</h3><p>矩阵的最基本运算包括矩阵加（减）法、数乘、转置、矩阵乘法运算。</p>
<h4 id="u52A0_u51CF_u6CD5_u8FD0_u7B97"><a href="#u52A0_u51CF_u6CD5_u8FD0_u7B97" class="headerlink" title="加减法运算"></a>加减法运算</h4><p>$m \times n$矩阵$A$和$B$的和（差）$A \pm B$也是一个$m \times n$矩阵，其中每个元素是$A$和$B$相应元素的和（差），即$ (A \pm B)_{ij}=A_{ij} \pm B_{ij} $，其中$1 \le i \le m$，$1 \le j \le n$。举个例子：</p>
<p>$$ \begin{bmatrix}<br>1&amp;2&amp;3\\<br>4&amp;5&amp;6\\<br>\end{bmatrix} + \begin{bmatrix}<br>6&amp;5&amp;4\\<br>3&amp;2&amp;1\\<br>\end{bmatrix} = \begin{bmatrix}<br>1+6 &amp; 2+5 &amp; 3+4 \\<br>4+3 &amp; 5+2 &amp; 6+1 \\<br>\end{bmatrix}=\begin{bmatrix}<br>7&amp;7&amp;7\\<br>7&amp;7&amp;7\\<br>\end{bmatrix}<br> $$</p>
<h4 id="u6570_u4E58_u8FD0_u7B97"><a href="#u6570_u4E58_u8FD0_u7B97" class="headerlink" title="数乘运算"></a>数乘运算</h4><p> 标量$c$与$m \times n$的矩阵$A$的数乘也是一个$m \times n$的矩阵$cA$，它的每个元素是矩阵$A$的相应元素与$c$的乘积$(cA)_{ij}=c \cdot A_{ij}$，举个例子：</p>
<p> $$<br> 2 \cdot \begin{bmatrix}<br> 1&amp;2&amp;3\\<br> 4&amp;5&amp;6\\<br> \end{bmatrix}=\begin{bmatrix}<br> 2 \cdot 1 &amp; 2 \cdot 2 &amp; 2 \cdot 3 \\<br> 2 \cdot 4 &amp; 2 \cdot 5 &amp; 2 \cdot 6 \\<br> \end{bmatrix}=\begin{bmatrix}<br> 2&amp;4&amp;6\\<br> 8&amp;10&amp;12\\<br> \end{bmatrix}<br>  $$</p>]]>
    
    </summary>
    
      <category term="Jupyter NoteBook" scheme="http://www.devtalking.com/tags/Jupyter-NoteBook/"/>
    
      <category term="NumPy" scheme="http://www.devtalking.com/tags/NumPy/"/>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="矩阵" scheme="http://www.devtalking.com/tags/%E7%9F%A9%E9%98%B5/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习笔记一之机器学习定义、导数、最小二乘]]></title>
    <link href="http://www.devtalking.com//articles/machine-learning-1/"/>
    <id>http://www.devtalking.com//articles/machine-learning-1/</id>
    <published>2018-01-05T16:00:00.000Z</published>
    <updated>2018-08-27T02:28:39.921Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u673A_u5668_u5B66_u4E60_u7684_u5B9A_u4E49"><a href="#u673A_u5668_u5B66_u4E60_u7684_u5B9A_u4E49" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><ul>
<li>非正式定义：在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。</li>
<li>正式定义：对于一个计算机程序来讲，给他一个任务T和一个性能测量方法P，如果在经验E的影响下P对T的测量结果得到了改进，那么就说该程序从E中得到了学习。</li>
</ul>
<p>举个机器下棋的例子，经验E对应着程序不断和自己下棋的经历，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率。如果说机器与人类棋手对应的胜率不断提高，那说明机器从自己和自己下棋的过程中得到了学习。</p>
<h2 id="u673A_u5668_u5B66_u4E60_u56DB_u5927_u5185_u5BB9_u7B80_u8FF0"><a href="#u673A_u5668_u5B66_u4E60_u56DB_u5927_u5185_u5BB9_u7B80_u8FF0" class="headerlink" title="机器学习四大内容简述"></a>机器学习四大内容简述</h2><ul>
<li>监督学习（Supervised Learning）</li>
<li>学习理论（Learning Theory）</li>
<li>无监督学习（Unsupervised Learning）</li>
<li>强化学习（Reinforcement Learning）</li>
</ul>
<h3 id="u76D1_u7763_u5B66_u4E60_uFF08Supervised_Learning_uFF09"><a href="#u76D1_u7763_u5B66_u4E60_uFF08Supervised_Learning_uFF09" class="headerlink" title="监督学习（Supervised Learning）"></a>监督学习（Supervised Learning）</h3><p><img src="http://paxigrdp0.bkt.clouddn.com/34576cd0bbd2b3d3905c4c9f5a4db346.jpg" alt=""><br>如上图所示，描述了假定城市区域的房屋面积与售价的关系。横坐标是房屋面积，纵坐标是售价。如果我想在横轴随意取一个面积，就希望知道它的售价，那么就需要一个方法通过面积确定售价。图中的方法是通过一条标准线来找到对应的售价。那么这条线要如何得来呢？</p>
<p>在这个示例中，我们已经给出了若干房屋面积和售价的数据集，即已经告知了机器若干问题和答案。机器通过这些数据集中面积和售价的关系，自我学习从而得出这条线，这种学习类型就属于监督学习类型。</p>
<p>因为这种场景中的数据集取值都是连续的，所以这类问题都可以归为线性回归的问题。<br><img src="http://paxigrdp0.bkt.clouddn.com/da9d63e45fa11a781128bc64bb78ae92.jpg" alt=""></p>
<p>如上图所示，描述了肿瘤大小与恶性良性的关系。横坐标是肿瘤的大小，纵坐标表示恶性或良性，与连续的房屋售价不同的是，这里的纵坐标只有两个值0或1。</p>
<p>这个示例中，我们同样给出了一组肿瘤大小与恶性良性的数据集，我们希望机器通过这组答案数据集自我学习然后有能力通过肿瘤大小判断出恶性或良性。当然这里只是示例，实际中会有很多其他的横轴指标值用于判断学习。</p>
<p>该示例中这种离散的数值问题可以归为分类的问题。</p>
<h3 id="u5B66_u4E60_u7406_u8BBA_uFF08Learning_Theory_uFF09"><a href="#u5B66_u4E60_u7406_u8BBA_uFF08Learning_Theory_uFF09" class="headerlink" title="学习理论（Learning Theory）"></a>学习理论（Learning Theory）</h3><p>任何具体的方式方法背后都有一个或多个理论进行支撑，机器学习也不例外。</p>
<p>这一大块的内容贯穿整个机器学习，包括人工智能和机器学习正式诞生之前的定理证明，理解为什么学习型算法是有效的。每种学习型算法需要多少训练数据，比如上面的房屋售价示例到底需要多少房屋的样本。以及机器学习渗透在我们生活中的真实应用场景等等。</p>
<h3 id="u65E0_u76D1_u7763_u5B66_u4E60_uFF08Unsupervised_Learning_uFF09"><a href="#u65E0_u76D1_u7763_u5B66_u4E60_uFF08Unsupervised_Learning_uFF09" class="headerlink" title="无监督学习（Unsupervised Learning）"></a>无监督学习（Unsupervised Learning）</h3><p>如果我现在有一组不知道任何信息的数据集，然后需要机器进行分析然后给出这组数据集中的几种共性或者相似的结构，将其聚类，这个方式就称之为无监督学习。即我们不会提供机器问题和答案，只提供数据，需要机器自我学习和分析找出共性进行聚类。</p>
<p>无监督学习的应用场景很多，当下流行的各种P图软件都或多或少的用到了这类算法，比如图的修复功能，抠图功能，像素化等一些滤镜都是对像素的聚类。还有声音驳杂功能，从嘈杂的声音中提取出有用的声音等。</p>
<h3 id="u5F3A_u5316_u5B66_u4E60_uFF08Reinforcement_Learning_uFF09"><a href="#u5F3A_u5316_u5B66_u4E60_uFF08Reinforcement_Learning_uFF09" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h3><p>强化学习是机器学习的精髓，更贴近机器的自我学习，用在不能通过一次决策下定论的情形中，比如分析病人肿瘤病情的例子，通过监督学习，我们给定了确切的答案，不是良性就是恶性，即我们做的决策要么对，要么错。但在强化学习中，在一段时间内会机器做出一系列决策。比如我们编写一个程序让一架无人机做出一系列特技表演动作，这种场景就不是一次决策能实现的了的。</p>
<p>那么强化学习究竟是什么？在它背后有一个称为回报函数的概念。比如我们训练宠物狗坐下、握手等行为，每当小狗做出的正确的行为，我们都会给小狗奖励，比如摸摸头进行鼓励或者给一块小骨头。渐渐的小狗就知道做出怎样的行为有奖励，怎样的行为没有奖励，即学会了坐下和握手的行为。这就很类似强化学习的理论，我们需要找到一种方式，来定义我们想要什么，如何定义一个好的行为和一个坏的行为，然后就需要一个合适的学习型算法来获得更多的回报和更少的惩罚。</p>
<a id="more"></a>
<h2 id="u76D1_u7763_u5B66_u4E60"><a href="#u76D1_u7763_u5B66_u4E60" class="headerlink" title="监督学习"></a>监督学习</h2><h3 id="u7EBF_u6027_u56DE_u5F52"><a href="#u7EBF_u6027_u56DE_u5F52" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归是监督学习中相对简单也但很实用的一个数学方法，它是用于确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。我们先从一元线性回归看起。</p>
<p>一元线性回归顾名思义只包括一个自变量和一个因变量，且二者的关系可用一条直线来近似表示，比如上文中的卖房示例，假设我们的分析数据如下表所示：<br><img src="http://paxigrdp0.bkt.clouddn.com/f2e2dafd1795cd2c9560c996e10b4dc2.jpg" alt=""></p>
<p>这组数据是链家网站望京二手房的数据，如果把这些数据画在二维坐标内，就可以得到一个散点图，就像上文图一所示，如果我们想得知房屋面积和售价的关系，就可以利用一元线性回归来画出一条拟合直线。</p>
<h4 id="u5982_u4F55_u753B_u51FA_u62DF_u5408_u76F4_u7EBF"><a href="#u5982_u4F55_u753B_u51FA_u62DF_u5408_u76F4_u7EBF" class="headerlink" title="如何画出拟合直线"></a>如何画出拟合直线</h4><p>首先我们知道一元线性函数的表达式为$ f(y) = ax + b $，该函数可以在二维坐标系内画出一条直线，并且$y$的值随$x$的值变化而变化，既$x$是自变量，$y$是因变量。假设上面给出的数据房屋面积为$x$，售价为$y$，那么每一个实际的$x$值都会有一个实际的$y$值。那么对于我们要画出的拟合直线来讲，每一个实际的$x$值都有会有一个通过拟合直线预测而来的$y$值，我们期望的结果是每个$y$的实际值与预测值的平方和最小，那么我们画出的这条拟合直线称为回归线。用公式表达为：</p>
<p>$$ (y _ {1实际} - y_{1预测})^2 + (y_{2实际}- y_{2预测})^2 + … + (y_{n实际} - y_{n预测})^2 $$</p>
<p>因为$y$的预测值可以由函数$ ax + b $求得，所以代入上面的公式可得：</p>
<p>$$ F(a,b) = \sum_{i=1}^n(y_i-(ax_i + b))^2 $$</p>
<p>所以监督学习中的线性回归方法就是通过大量的数据样本由机器求得$a$和$b$的值，使得函数$F$的值最小，那么回归线也就求出来了，既称为算法收敛。</p>
<h4 id="u5982_u4F55_u6C42_u51FD_u6570_u7684_u6700_u5C0F_u503C"><a href="#u5982_u4F55_u6C42_u51FD_u6570_u7684_u6700_u5C0F_u503C" class="headerlink" title="如何求函数的最小值"></a>如何求函数的最小值</h4><p>说到函数的最小值，那就要引入导数这个数学工具了，下面我从几何角度来解释什么是导数，它和函数的最小值有什么关系。</p>
<h5 id="u5BFC_u6570_u4E0E_u659C_u7387"><a href="#u5BFC_u6570_u4E0E_u659C_u7387" class="headerlink" title="导数与斜率"></a>导数与斜率</h5><p><img src="http://paxigrdp0.bkt.clouddn.com/7dfb137e5166d380c5194ef704b5af2e.jpg" alt=""></p>
<p>如上图所示，在图中的二维坐标系中，有一条曲线，一条直线，该直线为曲线的切线，相交于点$P$，该点的$x$轴坐标值为$x_0$。<br><img src="http://paxigrdp0.bkt.clouddn.com/2c37a842a7fd93c8a3afdc84999c0b05.jpg" alt=""></p>
<p>如上图所示，我们再画一条直线$B$，使其经过曲线上的点$P$和点$Q$，直线$B$称为曲线的割线。当点$Q$逐渐接近点$P$时，直线$B$就会逐渐接近直线$A$，所以说切线其实是割线的极值。<br><img src="http://paxigrdp0.bkt.clouddn.com/193d3633a71b48d7b9fc262296fab1b7.jpg" alt=""></p>
<p>如上图所示，$\Delta x$表示$x$的变化量，并且我们假设直线$B$的函数为$f(x)$，那么$\Delta f$表示函数$f$的变化量。在几何学中，一条直线关于横坐标轴倾斜程度的量，称为斜率，并且定义对于两个已知点$(x_1,y_1)$和$(x_2,y_2)$，如果$x_1$不等于$x_2$，则经过这两点直线的斜率为$k=(y_1-y_2)/(x_1-x_2)$。</p>
<p>所以直线$B$的斜率为$k=\Delta f / \Delta x$。因为点$P$和$Q$的坐标分别为$(x_0,f(x_0))$和$(x_0+\Delta x,f(x_0+\Delta x))$，所以我们代入可得：</p>
<p>$$ k=\frac {f(x_0+\Delta x) - f(x_0)} {(x_0+\Delta x)-x_0}=\frac {f(x_0+\Delta x) - f(x_0)} {\Delta x}$$ </p>
<p>上文中说到切线其实是割线的极值，那么切线$A$的斜率就是当$\Delta x$趋向于0时的值：</p>
<p>$$ k=\lim_{\Delta x-&gt;0}\frac {f(x_0+\Delta x) - f(x_0)} {\Delta x}$$</p>
<p>接下来我们再来看导数，导数是微积分中重要的基础概念，它描述了一个函数在某一点附近的变化率，如果函数的自变量和因变量都是实数的话，那么函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率：</p>
<p>$$ k=df=\lim_{\Delta x-&gt;0} \frac {f(x_0+\Delta x) - f(x_0)} {\Delta x} $$</p>
<h5 id="u6781_u503C"><a href="#u6781_u503C" class="headerlink" title="极值"></a>极值</h5><p>在数学中，极大值与极小值是指在一个域上函数取得最大值或最小值的点的函数值。这个域可以是整个函数域，也可以是整个函数域中的局部域。所以一个函数有局部极大极小值，也有全局极大极小值。</p>
<p>说到极值，又不得不说到微积分中的另外一个概念，驻点。<br><img src="http://paxigrdp0.bkt.clouddn.com/0cdd7f555855ce50d285e0a1b1377b7f.jpg" alt=""></p>
<p>如上图所示，在二维坐标中展示了一个一元函数的曲线，点$A$、$B$、$C$、$D$、$E$都是该曲线上的局部极大值或局部极小值，这些点就称为驻点。在每个驻点，函数在该域的输出值停止增加或减少，即变化率为零。每个驻点其实就是局部域曲线的切点。结合上文中提到的导数是描述函数在某一点附近的变化率，所以可得一元函数的局部极值点的导数为零。</p>
<p>假设有一个一元函数$f(x)=x^2$，该函数的导数通过上面的导数公式可得$f’(x)=2x$，并且该函数代表的曲线为下凹曲线，所以该函数的最小值为$f’(x)=0$，即当$x$等于0时$f(x)$的值最小。</p>
<h4 id="u4E8C_u5143_u51FD_u6570_u7684_u6700_u5C0F_u503C"><a href="#u4E8C_u5143_u51FD_u6570_u7684_u6700_u5C0F_u503C" class="headerlink" title="二元函数的最小值"></a>二元函数的最小值</h4><p>上文中$F(a,b)$是一个二元函数，实质上道理一样，我们可以把二元函数图像设想成一个像碗一样的曲面，那么最小值就是碗底，即曲面的最凹陷部，那么从360度任意方向上看，偏导数都是0。</p>
<h5 id="u504F_u5BFC_u6570"><a href="#u504F_u5BFC_u6570" class="headerlink" title="偏导数"></a>偏导数</h5><p>在数学中，一个多元函数的偏导数是它关于其中一个变量的导数，而保持其他变量恒定。全导数自然就是所有变量的导数。函数$f$关于变量$x$的偏导数写为$f’_x$或$\frac {\partial f}{\partial x}$，偏导数符号$\partial$是全导数符号$d$的变体。</p>
<p>因为曲面上的每一个点都有无穷多条切线，所以描述这种函数的导数非常困难，所以通常选择其中一条切线，并求出它的斜率，将其他变量视为常量，这就是偏导数。</p>
<p>所以$ F(a,b) = \sum_{i=1}^n(y_i-(ax_i + b))^2 $的最小值就是分别求$a$和$b$的偏导数，然后可以得到一个关于$a$和$b$的二元方程组，就可以求出$a$和$b$了，这个方法被称为最小二乘法。</p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h4 id="24F_28a_2Cb_29_24_u7684_u6700_u5C0F_u503C"><a href="#24F_28a_2Cb_29_24_u7684_u6700_u5C0F_u503C" class="headerlink" title="$F(a,b)$的最小值"></a>$F(a,b)$的最小值</h4><p>首先我们把$F(a,b)$展开：</p>
<p>$$ F(a,b) = \sum_{i=1}^n(y_i-(ax_i + b))^2 \\<br>=(y_1-(ax_1 + b))^2+(y_2-(ax_2 + b))^2+…+(y_n-(ax_n + b))^2 \\<br>=(y_1^2-2y_1(ax_1+b)+(ax_1+b)^2)+(y_2^2-2y_2(ax_2+b)+(ax_2+b)^2) \\<br>+…+(y_n^2-2y_n(ax_n+b)+(ax_n+b)^2) \\<br>=y_1^2-2ay_1x_1-2by_1+a^2x_1^2+2abx_1+b^2+y_2^2-2ay_2x_2-2by_2+a^2x_2^2+2abx_2+b^2 \\<br>+…+y_n^2-2ay_nx_n-2by_n+a^2x_n^2+2abx_n+b^2 \\<br>=(y_1^2+..+y_n^2)-2a(x_1y_1+..+x_ny_n)-2b(y_1+..+y_n) \\<br>+a^2(x_1^2+..+x_n^2)+2ab(x_1+..+x_n)+nb^2 $$</p>
<p>然后利用平均数，把上面式子中每个括号里的内容进一步简化，比如：</p>
<p>$$\frac {(y_1^2+..+y_n^2)} n=\overline {y^2}$$</p>
<p>则：</p>
<p>$$(y_1^2+..+y_n^2)=n\overline{y^2}$$</p>
<p>所以$F(a,b)$可以继续简化为：</p>
<p>$$F(a,b)=n\overline{y^2}-2an\overline{xy}-2bn\overline y+a^2n\overline{x^2}+2abn\overline x+nb^2$$</p>
<p>然后我们分别对$a$和$b$求偏导数，并且令偏导等于0：</p>
<p>$$\frac {\partial F}{\partial a}=-2n\overline xy+2an\overline {x^2}+2bn\overline x=0$$</p>
<p>$$\frac {\partial F}{\partial b}=-2n\overline y+2an\overline x+2nb=0$$</p>
<p>消掉$2n$最后得到关于$a$，$b$的二元方程组为：</p>
<p>$$-\overline {xy}+a\overline {x^2}+b\overline x=0$$</p>
<p>$$-\overline y+a\overline x+b=0$$</p>
<p>最后得出a和b的求解公式：</p>
<p>$$a=\frac {\overline x \overline y-\overline {xy}} {(\overline x)^2-\overline {x^2}}$$</p>
<p>$$b=\overline y-a\overline x$$</p>
<p>将上文中的那组数据带入后可算的$a$和$b$的值：<br><img src="http://paxigrdp0.bkt.clouddn.com/4431a672dc9198ae16fc7a9b223f9f22.jpg" alt=""></p>
<p>从带入结果来看，这条回归线的函数为$f(x)=14.93x-360.88$，现在我们就可以通过回归线来做一些预测，比如上图中计算了面积为35.5平方米和55.26平方米房子的售价，分别为169.29万和464.4万。</p>
<h4 id="u8BC4_u4EF7_u56DE_u5F52_u7EBF_u7684_u62DF_u5408_u7A0B_u5EA6"><a href="#u8BC4_u4EF7_u56DE_u5F52_u7EBF_u7684_u62DF_u5408_u7A0B_u5EA6" class="headerlink" title="评价回归线的拟合程度"></a>评价回归线的拟合程度</h4><p>因为我们画出的拟合直线只是一个近似值，所以需要有一个标准来评判回归线拟合程度的好坏，即算法收敛情况。我们来看几个概念：</p>
<h5 id="u603B_u504F_u5DEE_u5E73_u65B9_u548C"><a href="#u603B_u504F_u5DEE_u5E73_u65B9_u548C" class="headerlink" title="总偏差平方和"></a>总偏差平方和</h5><p>总偏差平方和又称总平方和（SST，Sum of Squares for Total），是每个因变量的实际值与因变量的平均值的差的平方和，即反映了因变量取值的总体波动情况：</p>
<p>$$SST=\sum_{i=1}^n(y_i-\overline y)^2$$</p>
<h5 id="u56DE_u5F52_u5E73_u65B9_u548C"><a href="#u56DE_u5F52_u5E73_u65B9_u548C" class="headerlink" title="回归平方和"></a>回归平方和</h5><p>回归平方和（SSR，Sum of Squares for Regression）是因变量的回归值（回归线上的$y$值）与其平均值的差的平方和。它是由于自变量$x$的变化引起的$y$的变化，反映了$y$的总偏差中由于$x$与$y$之间的线性关系引起的$y$的变化部分：</p>
<p>$$SSR=\sum_{i=1}^n(\hat y_i-\overline y)^2$$                                                                                                         </p>
<h5 id="u8BEF_u5DEE_u5E73_u65B9_u548C"><a href="#u8BEF_u5DEE_u5E73_u65B9_u548C" class="headerlink" title="误差平方和"></a>误差平方和</h5><p>误差平方和（SSE，Sum of Squares for Error）是因变量的各实际值与回归值的差的平方和，反映了除$x$对$y$的线性影响之外的其他因素对$y$变化的作用：</p>
<p>$$SSE=SST-SSR$$</p>
<p>因为总偏差是包含了所有可能对$y$值有影响的因素，比如房子的售价不可能只有面积来决定，还包括地域、户型、楼层、楼龄等等因素。回归平方和只包含$x$对$y$变化的影响，即只是面积对售价的影响。</p>
<p>所以拟合优度$R^2=SSR/SST$或者$R^2=1-SSE/SST$，$R^2$的取值范围在0到1之间，越接近1说明拟合程度越好，说明$x$对$y$值的影响权重越大。</p>
<p>比如如果所有的点都在回归线上，那说明$SSE$等于0，则$R^2=1$，说明$y$的变化100%由$x$的变化引起，没有其他因素会影响$y$，则回归线能够完全解释$y$的变化，如果$R^2$的值很低，则说明$x$和$y$之间可能根本就不存在线性关系。<br><img src="http://paxigrdp0.bkt.clouddn.com/cc38ffff2a4ff78ca4772dcddbe4262b.jpg" alt=""></p>
<p>将数据带入计算得出最拟合优度为0.92，拟合程度还不错，说明在特定的望京区域内房屋面积是主要影响价格的因素。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>我认为从线性回归开始学习机器学习是最好的切入点，而且线性回归也是监督学习最根本的支撑，比如梯度下降等算法都需要基于线性回归的知识来理解。另外在我看来人工智能（AI）并不是要造一个机器人脑，这是伪科学。人工智能只是利用机器强大的计算能力，用数学工具，通过对大量数据的统计分析从而得出近似最优答案。所以说，人类和AlphaGO下棋，不是输在智力，而是输在身体机能。</p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u673A_u5668_u5B66_u4E60_u7684_u5B9A_u4E49"><a href="#u673A_u5668_u5B66_u4E60_u7684_u5B9A_u4E49" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><ul>
<li>非正式定义：在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。</li>
<li>正式定义：对于一个计算机程序来讲，给他一个任务T和一个性能测量方法P，如果在经验E的影响下P对T的测量结果得到了改进，那么就说该程序从E中得到了学习。</li>
</ul>
<p>举个机器下棋的例子，经验E对应着程序不断和自己下棋的经历，任务T是下棋，性能测量方法P可以是它在和人类棋手对弈的胜率。如果说机器与人类棋手对应的胜率不断提高，那说明机器从自己和自己下棋的过程中得到了学习。</p>
<h2 id="u673A_u5668_u5B66_u4E60_u56DB_u5927_u5185_u5BB9_u7B80_u8FF0"><a href="#u673A_u5668_u5B66_u4E60_u56DB_u5927_u5185_u5BB9_u7B80_u8FF0" class="headerlink" title="机器学习四大内容简述"></a>机器学习四大内容简述</h2><ul>
<li>监督学习（Supervised Learning）</li>
<li>学习理论（Learning Theory）</li>
<li>无监督学习（Unsupervised Learning）</li>
<li>强化学习（Reinforcement Learning）</li>
</ul>
<h3 id="u76D1_u7763_u5B66_u4E60_uFF08Supervised_Learning_uFF09"><a href="#u76D1_u7763_u5B66_u4E60_uFF08Supervised_Learning_uFF09" class="headerlink" title="监督学习（Supervised Learning）"></a>监督学习（Supervised Learning）</h3><p><img src="http://paxigrdp0.bkt.clouddn.com/34576cd0bbd2b3d3905c4c9f5a4db346.jpg" alt=""><br>如上图所示，描述了假定城市区域的房屋面积与售价的关系。横坐标是房屋面积，纵坐标是售价。如果我想在横轴随意取一个面积，就希望知道它的售价，那么就需要一个方法通过面积确定售价。图中的方法是通过一条标准线来找到对应的售价。那么这条线要如何得来呢？</p>
<p>在这个示例中，我们已经给出了若干房屋面积和售价的数据集，即已经告知了机器若干问题和答案。机器通过这些数据集中面积和售价的关系，自我学习从而得出这条线，这种学习类型就属于监督学习类型。</p>
<p>因为这种场景中的数据集取值都是连续的，所以这类问题都可以归为线性回归的问题。<br><img src="http://paxigrdp0.bkt.clouddn.com/da9d63e45fa11a781128bc64bb78ae92.jpg" alt=""></p>
<p>如上图所示，描述了肿瘤大小与恶性良性的关系。横坐标是肿瘤的大小，纵坐标表示恶性或良性，与连续的房屋售价不同的是，这里的纵坐标只有两个值0或1。</p>
<p>这个示例中，我们同样给出了一组肿瘤大小与恶性良性的数据集，我们希望机器通过这组答案数据集自我学习然后有能力通过肿瘤大小判断出恶性或良性。当然这里只是示例，实际中会有很多其他的横轴指标值用于判断学习。</p>
<p>该示例中这种离散的数值问题可以归为分类的问题。</p>
<h3 id="u5B66_u4E60_u7406_u8BBA_uFF08Learning_Theory_uFF09"><a href="#u5B66_u4E60_u7406_u8BBA_uFF08Learning_Theory_uFF09" class="headerlink" title="学习理论（Learning Theory）"></a>学习理论（Learning Theory）</h3><p>任何具体的方式方法背后都有一个或多个理论进行支撑，机器学习也不例外。</p>
<p>这一大块的内容贯穿整个机器学习，包括人工智能和机器学习正式诞生之前的定理证明，理解为什么学习型算法是有效的。每种学习型算法需要多少训练数据，比如上面的房屋售价示例到底需要多少房屋的样本。以及机器学习渗透在我们生活中的真实应用场景等等。</p>
<h3 id="u65E0_u76D1_u7763_u5B66_u4E60_uFF08Unsupervised_Learning_uFF09"><a href="#u65E0_u76D1_u7763_u5B66_u4E60_uFF08Unsupervised_Learning_uFF09" class="headerlink" title="无监督学习（Unsupervised Learning）"></a>无监督学习（Unsupervised Learning）</h3><p>如果我现在有一组不知道任何信息的数据集，然后需要机器进行分析然后给出这组数据集中的几种共性或者相似的结构，将其聚类，这个方式就称之为无监督学习。即我们不会提供机器问题和答案，只提供数据，需要机器自我学习和分析找出共性进行聚类。</p>
<p>无监督学习的应用场景很多，当下流行的各种P图软件都或多或少的用到了这类算法，比如图的修复功能，抠图功能，像素化等一些滤镜都是对像素的聚类。还有声音驳杂功能，从嘈杂的声音中提取出有用的声音等。</p>
<h3 id="u5F3A_u5316_u5B66_u4E60_uFF08Reinforcement_Learning_uFF09"><a href="#u5F3A_u5316_u5B66_u4E60_uFF08Reinforcement_Learning_uFF09" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h3><p>强化学习是机器学习的精髓，更贴近机器的自我学习，用在不能通过一次决策下定论的情形中，比如分析病人肿瘤病情的例子，通过监督学习，我们给定了确切的答案，不是良性就是恶性，即我们做的决策要么对，要么错。但在强化学习中，在一段时间内会机器做出一系列决策。比如我们编写一个程序让一架无人机做出一系列特技表演动作，这种场景就不是一次决策能实现的了的。</p>
<p>那么强化学习究竟是什么？在它背后有一个称为回报函数的概念。比如我们训练宠物狗坐下、握手等行为，每当小狗做出的正确的行为，我们都会给小狗奖励，比如摸摸头进行鼓励或者给一块小骨头。渐渐的小狗就知道做出怎样的行为有奖励，怎样的行为没有奖励，即学会了坐下和握手的行为。这就很类似强化学习的理论，我们需要找到一种方式，来定义我们想要什么，如何定义一个好的行为和一个坏的行为，然后就需要一个合适的学习型算法来获得更多的回报和更少的惩罚。</p>]]>
    
    </summary>
    
      <category term="机器学习" scheme="http://www.devtalking.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://www.devtalking.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="机器学习算法" scheme="http://www.devtalking.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于TestNG和PowerMock的单元测试指南]]></title>
    <link href="http://www.devtalking.com//articles/testng-powermock/"/>
    <id>http://www.devtalking.com//articles/testng-powermock/</id>
    <published>2017-01-18T16:00:00.000Z</published>
    <updated>2018-06-11T14:48:42.835Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u4EC0_u4E48_u662FTestNG"><a href="#u4EC0_u4E48_u662FTestNG" class="headerlink" title="什么是TestNG"></a>什么是TestNG</h2><p>TestNG是一套开源测试框架，是从JUnit继承而来，TestNG意为test next generation。它的优势如下：</p>
<ul>
<li>支持注解。</li>
<li>可以在任意的大线程池中，使用各种策略运行测试（所有方法都可以拥有自己的线程或者每个测试类拥有一个线程等等）。</li>
<li>代码多线程安全测试。</li>
<li>灵活的测试配置。</li>
<li>支持数据驱动测试(@DataProvider)。</li>
<li>支持参数。</li>
<li>强大的执行模型（不再用TestSuite）。</li>
<li>支持各种工具和插件（Eclipse、IDEA、Maven等）。</li>
<li>可以更灵活地嵌入BeanShell。</li>
<li>默认JDK运行时功能和日志记录（无依赖关系）。</li>
<li>依赖应用服务测试的方式。</li>
</ul>
<h2 id="TestNG_u7684_u6700_u7B80_u5355_u793A_u4F8B"><a href="#TestNG_u7684_u6700_u7B80_u5355_u793A_u4F8B" class="headerlink" title="TestNG的最简单示例"></a>TestNG的最简单示例</h2><p>我们先来看一个TestNG最简单的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.junit.AfterClass;</span><br><span class="line"><span class="keyword">import</span> org.junit.BeforeClass;</span><br><span class="line"><span class="keyword">import</span> org.testng.annotations.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@BeforeClass</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beforeClass</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"this is before class"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">TestHelloWorld</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"this is HelloWorld test case"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@AfterClass</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterClass</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"this is after class"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从面上的简单示例中可以看出TestNG的生命周期，并且可以看到TestNG是使用注解来控制生命周期的。下面我们来看看TestNG支持的各种注解。</p>
<h2 id="TestNG_u7684_u6CE8_u89E3"><a href="#TestNG_u7684_u6CE8_u89E3" class="headerlink" title="TestNG的注解"></a>TestNG的注解</h2><p>TestNG的注解大部分用在方法级别上，一共有六大类注解。</p>
<h3 id="Before_u7C7B_u522B_u548CAfter_u7C7B_u522B_u6CE8_u89E3"><a href="#Before_u7C7B_u522B_u548CAfter_u7C7B_u522B_u6CE8_u89E3" class="headerlink" title="Before类别和After类别注解"></a>Before类别和After类别注解</h3><ul>
<li>@BeforeSuite：被注解的方法将会在所有测试类执行之前运行。</li>
<li>@AfterSuite：被注解的方法将会在所有测试类执行之后运行。</li>
<li>@BeforeTest：被注解的方法将会在当前测试类中的所有测试方法执行之前运行。</li>
<li>@AfterTest：被注解的方法将会在当前测试类中的所有测试方法执行之后运行。</li>
<li>@BeforeClass：被注解的方法将会在当前测试类中的第一个测试方法执行之前运行。</li>
<li>@AfterClass：被注解的方法将会在当前测试类中的最后一个测试方法执行之后运行。</li>
<li>@BeforeMethod：被注解的方法将会在当前测试类中的每个测试方法执行之前运行。</li>
<li>@AfterMethod：被注解的方法将会在当前测试类中的每个测试方法执行之后运行。<br>我们可以根据不同的场景来使用不同的注解。</li>
</ul>
<a id="more"></a>
<h3 id="@Test_u6CE8_u89E3"><a href="#@Test_u6CE8_u89E3" class="headerlink" title="@Test注解"></a>@Test注解</h3><p>@Test注解是TestNG的核心注解，被打上该注解的方法，表示为一个测试方法，这个注解有多个配置属性，用法如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="annotation">@Test</span>(param1 = ..., param2 = ...)</span><br></pre></td></tr></table></figure>
<ul>
<li>alwaysRun：如果=true，表示即使该测试方法所依赖的前置测试有失败的情况，也要执行。</li>
<li>dataProvider：选定传入参数的构造器。（后面会讲到@DataProvider注解）</li>
<li>dataProviderClass：确定参数构造器的Class类。(参数构造器首先会在当前测试类里面查找，如果参数构造器不在当前测试类定义，那么必须使用该属性来执行它所在的Class类)</li>
<li>dependsOnGroups：确定依赖的前置测试组别。</li>
<li>dependsOnMethods：确定依赖的前置测试方法。</li>
<li>description：测试方法描述信息。(建议为每个测试方法添加有意义的描述信息，这将会在最后的报告中展示出来)</li>
<li>enabled：默认为true，如果指定为false，表示不执行该测试方法。</li>
<li>expectedExceptions：指定期待测试方法抛出的异常，多个异常以逗号隔开。</li>
<li>groups：指定该测试方法所属的组，可以指定多个组，以逗号隔开。</li>
<li>invocationCount：指定测试方法需要被调用的次数。</li>
<li>invocationTimeOut：每一次调用的超时时间，如果invocationCount没有指定，该参数会被忽略。应用场景可以为测试获取数据库连接，超时就认定为失败。单位是毫秒。</li>
<li>priority：指定测试方法的优先级，数值越低，优先级越高，将会优先与其他数值高的测试方法被调用。(注意是针对一个测试类的优先级)</li>
<li>timeout：指定整个测试方法的超时时间。单位是毫秒。</li>
</ul>
<p>下面我们写一个简单的测试类，说明@Test注解的使用以及属性的配置方式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestAnnotationPropertiesTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Test</span>(priority = <span class="number">1</span>, invocationCount = <span class="number">3</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test1</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"invoke test1"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Test</span>(priority = <span class="number">2</span>, invocationCount = <span class="number">2</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test2</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"invoke test2"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>testng.xml：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="doctype">&lt;!DOCTYPE suite SYSTEM "http://testng.org/testng-1.0.dtd" &gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">suite</span> <span class="attribute">name</span>=<span class="value">"Suite1"</span> <span class="attribute">verbose</span>=<span class="value">"1"</span> &gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"test1"</span> &gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"com.testngdemo.TestAnnotationPropertiesTest"</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">suite</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">invoke test1&#10;invoke test1&#10;invoke test1&#10;invoke test2&#10;invoke test2</span><br></pre></td></tr></table></figure>
<h3 id="@Parameters_u6CE8_u89E3"><a href="#@Parameters_u6CE8_u89E3" class="headerlink" title="@Parameters注解"></a>@Parameters注解</h3><p>@Parameters 注解用于为测试方法传递参数， 用法如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AnnotationParametersTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Parameters</span>(value = &#123;<span class="string">"param1"</span>, <span class="string">"param2"</span>&#125;)</span><br><span class="line"> <span class="annotation">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">(String arg1, String arg2)</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"use @Parameters to fill method arguments : arg 1 = "</span> + arg1 + <span class="string">", arg2 = "</span> + arg2);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>testng.xml配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"testAnnotationParameters"</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">parameter</span> <span class="attribute">name</span>=<span class="value">"param1"</span> <span class="attribute">value</span>=<span class="value">"value1"</span>&gt;</span><span class="tag">&lt;/<span class="title">parameter</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">parameter</span> <span class="attribute">name</span>=<span class="value">"param2"</span> <span class="attribute">value</span>=<span class="value">"value2"</span>&gt;</span><span class="tag">&lt;/<span class="title">parameter</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"com.testngdemo.AnnotationParametersTest"</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">use @Parameters to fill method arguments : arg 1 = value1, arg2 = value2</span><br></pre></td></tr></table></figure>
<h3 id="@DataProvider_u6CE8_u89E3"><a href="#@DataProvider_u6CE8_u89E3" class="headerlink" title="@DataProvider注解"></a>@DataProvider注解</h3><p>上面提到@Parameters注解可以为测试方法传递参数，但是这种方式参数值需要配置在testng.xml里面，灵活性不高。而@DataProvider注解同样可以为测试方法传递参数值，并且，它是真正意义上的参数构造器，可以传入多组测试数据对测试方法进行测试。被@DataProvider注解的方法，方法返回值必须为Object[][]或者Iterator<object[]>，用法如下所示：</object[]></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AnnotationDataProviderTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@DataProvider</span>(name=<span class="string">"testMethodDataProvider"</span>)</span><br><span class="line"> <span class="keyword">public</span> Object[][] testMethodDataProvider() &#123;</span><br><span class="line"> 	<span class="keyword">return</span> <span class="keyword">new</span> Object[][]&#123;&#123;<span class="string">"value1-1"</span>, <span class="string">"value2-1"</span>&#125;, &#123;<span class="string">"value1-2"</span>, <span class="string">"value2-2"</span>&#125;, &#123;<span class="string">"value1-3"</span>, <span class="string">"value2-3"</span>&#125;&#125;;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Test</span>(dataProvider=<span class="string">"testMethodDataProvider"</span>)</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">(String arg1, String arg2)</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"use @DataProvider to fill method argument : arg1 = "</span> + arg1 + <span class="string">" , arg2 = "</span> + arg2);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>testng.xml：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"testDataProvider"</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"com.testngdemo.AnnotationDataProviderTest"</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">use @DataProvider to fill method argument : arg1 = value1-1 , arg2 = value2-1</span><br><span class="line">use @DataProvider to fill method argument : arg1 = value1-2 , arg2 = value2-2</span><br><span class="line">use @DataProvider to fill method argument : arg1 = value1-3 , arg2 = value2-3</span><br></pre></td></tr></table></figure>
<h3 id="@Factory__u6CE8_u89E3"><a href="#@Factory__u6CE8_u89E3" class="headerlink" title="@Factory 注解"></a>@Factory 注解</h3><p>在一个方法上面打上@Factory注解，表示该方法将返回能够被TestNG测试的测试类。利用了设计模式中的工厂模式，用法如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AnnotationFactoryTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Factory</span></span><br><span class="line"> <span class="keyword">public</span> Object[] getSimpleTest() &#123;</span><br><span class="line"> 	<span class="keyword">return</span> <span class="keyword">new</span> Object[]&#123; <span class="keyword">new</span> SimpleTest(<span class="string">"one"</span>), <span class="keyword">new</span> SimpleTest(<span class="string">"two"</span>)&#125;;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span> String param;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="title">SimpleTest</span><span class="params">(String param)</span> </span>&#123;</span><br><span class="line"> 	<span class="keyword">this</span>.param = param;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"SimpleTest.param = "</span> + param);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>testng.xml配置：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"testFactory"</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"com.crazypig.testngdemo.AnnotationFactoryTest"</span> /&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>测试结果：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">SimpleTest.param = one</span><br><span class="line">SimpleTest.param = two</span><br></pre></td></tr></table></figure>
<h3 id="@Listeners__u6CE8_u89E3"><a href="#@Listeners__u6CE8_u89E3" class="headerlink" title="@Listeners 注解"></a>@Listeners 注解</h3><p>一般我们写测试类不会涉及到这种类型的注解，这个注解必须定义在类、接口或者枚举类级别。实用的Listener包括ISuiteListener、ITestListener和IInvokedMethodListener，他们可以在suite级别、test级别和test method一些执行点执行一些自定义操作，如打印日志等。</p>
<h2 id="TestNG_u7684_u914D_u7F6E_u6587_u4EF6"><a href="#TestNG_u7684_u914D_u7F6E_u6587_u4EF6" class="headerlink" title="TestNG的配置文件"></a>TestNG的配置文件</h2><p>在上文的示例中，出现了testng.xml这种东西，这一节就来看看TestNG的配置文件。和JUnit一样，TestNG也可以直接在测试类中针对某个测试方法运行，或者运行整个测试类。此外TestNG还提供了更强大和灵活的配置文件方式，比如在配置文件中控制测试类的执行策略，是并行还是串行，在配置文件里进行传参以及对测试类进行分组测试等。这样能让运行测试方法和测试类更有逻辑性，能灵活的应对不同的测试场景，最大化的覆盖业务场景。</p>
<p>当下，Java开发基本都使用Maven来进行依赖管理，TestNG也同样支持Maven，这样我们只需要在POM文件中将TestNG的配置文件配置进去，就可以使用mvn test来跑单元测试了。示例如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-surefire-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.16<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">suiteXmlFiles</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">suiteXmlFile</span>&gt;</span>src/test/resources/testng.xml<span class="tag">&lt;/<span class="title">suiteXmlFile</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">suiteXmlFiles</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="testng-xml_u7684_u57FA_u672C_u683C_u5F0F"><a href="#testng-xml_u7684_u57FA_u672C_u683C_u5F0F" class="headerlink" title="testng.xml的基本格式"></a>testng.xml的基本格式</h3><h4 id="u6807_u7B7E"><a href="#u6807_u7B7E" class="headerlink" title="标签"></a><suite>标签</suite></h4><p><suite>元素是testng.xml文件的根元素。从DTD文件（如下所示）可以看出，<suite>可以包含一个<groups>元素，用以定义全局的组，该组对所有的测试可见。<suite>可以包含多个<test>元素，一个<test>就定义了一个测试用例（其中可能包含多个测试方法）。</test></test></suite></groups></suite></suite></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">!ELEMENT</span> <span class="attribute">suite</span> (<span class="attribute">groups</span>?,(<span class="attribute">listeners</span>|<span class="attribute">packages</span>|<span class="attribute">test</span>|<span class="attribute">parameter</span>|<span class="attribute">method-selectors</span>|<span class="attribute">suite-files</span>)*) &gt;</span></span><br></pre></td></tr></table></figure>
<p>示例如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="doctype">&lt;!DOCTYPE suite SYSTEM "http://testng.org/testng-1.0.dtd" &gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">suite</span> <span class="attribute">name</span>=<span class="value">"Suite1"</span> <span class="attribute">verbose</span>=<span class="value">"1"</span> &gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">groups</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">run</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"..."</span> /&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="title">exclude</span> <span class="attribute">name</span>=<span class="value">"..."</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">run</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">groups</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"HelloWorld1"</span>&gt;</span></span><br><span class="line">       ...</span><br><span class="line">  <span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">suite</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><suite>标签中有一个重要的属性是parallel，通过该属性可以配置测试用例的线程运行策略，该属性的具体值如下：</suite></p>
<ul>
<li>methods：针对每个测试方法启独立线程运行。</li>
<li>classes：针对每个测试类启独立线程运行，该测试类中的所有测试方法均在一个线程中运行。</li>
<li>instances：针对测试类实例启独立线程运行，不同实例的相同测试方法在不同的线程中运行。</li>
</ul>
<h4 id="u6807_u7B7E-1"><a href="#u6807_u7B7E-1" class="headerlink" title="标签"></a><test>标签</test></h4><p><test>元素是<suite>的子元素，用以定义一个测试用例。定义测试用例可以通过<classes>或<packages>。</packages></classes></suite></test></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">!ELEMENT</span> <span class="attribute">test</span> (<span class="attribute">method-selectors</span>?,<span class="attribute">parameter</span>*,<span class="attribute">groups</span>?,<span class="attribute">packages</span>?,<span class="attribute">classes</span>?) &gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><classes>表示以测试类的方式定义测试用例，粒度较小。示例如下：</classes></li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="doctype">&lt;!DOCTYPE suite SYSTEM "http://testng.org/testng-1.0.dtd" &gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">suite</span> <span class="attribute">name</span>=<span class="value">"Suite1"</span> <span class="attribute">verbose</span>=<span class="value">"1"</span> &gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"HelloWorld1"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"test.sample.ParameterSample"</span>/&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"test.sample.ParameterTest"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">suite</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><packages>表示以测试类所在的包的方式定义测试用例，包中的所有测试类都被涉及，粒度较大。示例如下：</packages></li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="doctype">&lt;!DOCTYPE suite SYSTEM "http://testng.org/testng-1.0.dtd" &gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">suite</span> <span class="attribute">name</span>=<span class="value">"Suite1"</span> <span class="attribute">verbose</span>=<span class="value">"1"</span> &gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"HelloWorld1"</span>   &gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">packages</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">package</span> <span class="attribute">name</span>=<span class="value">"test.sample"</span> /&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="title">packages</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">suite</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><test>元素中也有<groups>元素，上文中提到，<suite>中可以定义一个全局的<groups>。而这里<test>元素中也可以定义一个自己的<groups>，其中定义的组仅对当前所在的测试用例可见。示例如下：</groups></test></groups></suite></groups></test></li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"HelloWorld1"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">groups</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">run</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">exclude</span> <span class="attribute">name</span>=<span class="value">"brokenTests"</span>  /&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"checkinTests"</span>  /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">run</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">groups</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line">     ...</span><br><span class="line">  <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><parameters>元素可以用于在配置文件中给测试方法传递参数，在上文中的@Parameters注解一节中有示例。</parameters></li>
</ul>
<blockquote>
<p>注意：在testng.xml配置文件中，<suite>中可以定义多个<test>，那么这些<test>的执行顺序默认按照其在<suite>中出现的先后顺序。当然，也可以提供<test>的preserve-order=’false’改变默认顺序。  </test></suite></test></test></suite></p>
</blockquote>
<h4 id="u6807_u7B7E-2"><a href="#u6807_u7B7E-2" class="headerlink" title="标签"></a><classes>标签</classes></h4><p><test>可以通过<classes>或<packages>定义测试用例，但只是在测试类或类包的层次上，那么能不能具体到测试类的某个方法呢？<br>对于<classes>中的一个<class>，可以提供<methods>设置测试方法。示例如下：</methods></class></classes></packages></classes></test></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"HelloWorld1"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"test.Test1"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">methods</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"m1"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"m2"</span> /&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="title">methods</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">class</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"test.Test2"</span> /&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="u4EC0_u4E48_u662FPowerMock"><a href="#u4EC0_u4E48_u662FPowerMock" class="headerlink" title="什么是PowerMock"></a>什么是PowerMock</h2><p>mock是模拟对象，用于模拟真实对象的行为。PowerMock可以支持EasyMock和Mockito，作为Mockito的扩展，使用PowerMock可以mock private方法，mock static方法，mock final方法，mock construction方法。PowerMock封装了部分Mockito的API，可以使用Mockito的语法来进行测试代码的编写。</p>
<h3 id="PowerMock_u6CE8_u89E3@PrepareForTest"><a href="#PowerMock_u6CE8_u89E3@PrepareForTest" class="headerlink" title="PowerMock注解@PrepareForTest"></a>PowerMock注解@PrepareForTest</h3><p>@PrepareForTest(Employee.class)语句告诉PowerMock准备Employee类进行测试。适用于模拟final类或有final, private, static, native方法的类 @PrepareForTest是当使用PowerMock强大的Mock静态、final、private方法时，需要添加的注解。 如果测试用例里没有使用注解@PrepareForTest，可以不加注解@RunWith(PowerMockRunner.class),反之亦然。</p>
<h3 id="PowerMockTestCase_u7236_u7C7B"><a href="#PowerMockTestCase_u7236_u7C7B" class="headerlink" title="PowerMockTestCase父类"></a>PowerMockTestCase父类</h3><p>如果想在TestNG框架下使用PowerMock，那么需要测试类继承PowerMockTestCase类，作用是告诉TestNG框架，在测试方法中会用到PowerMock的功能，比如mock static、final、private方法等。</p>
<h3 id="u6D4B_u8BD5static_u65B9_u6CD5"><a href="#u6D4B_u8BD5static_u65B9_u6CD5" class="headerlink" title="测试static方法"></a>测试static方法</h3><p>示例中的待测试类为Employee：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">count</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getEmployeeCount</span><span class="params">()</span> </span>&#123;    </span><br><span class="line">        <span class="keyword">return</span> Employee.count();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>对应的测试类为EmployeeServiceTest：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="annotation">@PrepareForTest</span>(Employee.class)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldReturnTheCountOfEmployeesUsingTheDomainClass</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        PowerMockito.mockStatic(Employee.class);</span><br><span class="line">        PowerMockito.when(Employee.count()).thenReturn(<span class="number">900</span>);</span><br><span class="line"></span><br><span class="line">        EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">        assertEquals(<span class="number">900</span>, employeeService.getEmployeeCount());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面的示例中可以看到，如果要mock静态方法，需要使用<code>mockStatic()</code>方法来mock该静态方法所属的类，然后通过PowerMock的链式语法对方法进行mock，即当调用某个方法时，我期望返回什么值，所以<code>PowerMockito.when(Employee.count()).thenReturn(900);</code>这句用大白话来翻译的话就是当调用Employee的<code>count()</code>方法时期望返回900。然后再调用要测试的方法，使用断言对结果进行验证。</p>
<h3 id="u6D4B_u8BD5_u8FD4_u56DEvoid_u7684_u9759_u6001_u65B9_u6CD5"><a href="#u6D4B_u8BD5_u8FD4_u56DEvoid_u7684_u9759_u6001_u65B9_u6CD5" class="headerlink" title="测试返回void的静态方法"></a>测试返回void的静态方法</h3><p>示例中的待测试类为EmployeeService：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">giveIncrementOf</span><span class="params">(<span class="keyword">int</span> percentage)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">giveIncrementToAllEmployeesOf</span><span class="params">(<span class="keyword">int</span> percentage)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            Employee.giveIncrementOf(percentage);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的测试类为EmployeeServiceTest：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="annotation">@PrepareForTest</span>(Employee.class)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldReturnTrueWhenIncrementOf10PercentageIsGivenSuccessfully</span><span class="params">()</span> </span>&#123;        </span><br><span class="line">        PowerMockito.mockStatic(Employee.class);</span><br><span class="line">        PowerMockito.doNothing().when(Employee.class);</span><br><span class="line">        Employee.giveIncrementOf(<span class="number">10</span>);</span><br><span class="line">        EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">        assertTrue(employeeService.giveIncrementToAllEmployeesOf(<span class="number">10</span>));    </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldReturnFalseWhenIncrementOf10PercentageIsNotGivenSuccessfully</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">        PowerMockito.mockStatic(Employee.class);</span><br><span class="line">        PowerMockito.doThrow(<span class="keyword">new</span> IllegalStateException()).when(Employee.class);</span><br><span class="line">        Employee.giveIncrementOf(<span class="number">10</span>);</span><br><span class="line">        EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">        assertFalse(employeeService.giveIncrementToAllEmployeesOf(<span class="number">10</span>));</span><br><span class="line">    &#125;    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个示例中展示了调用方法不触发实现逻辑及抛异常的PowerMock方法<code>doNothing()</code>和<code>doThrow()</code>。</p>
<ul>
<li><code>doNothing()</code>：该方法告诉PowerMock下一个方法调用时什么也不做。</li>
<li><code>doThrow()</code>：该方法告诉PowerMock下一个方法调用时产生给定异常。</li>
</ul>
<h3 id="u9A8C_u8BC1_u65B9_u6CD5_u662F_u5426_u8C03_u7528"><a href="#u9A8C_u8BC1_u65B9_u6CD5_u662F_u5426_u8C03_u7528" class="headerlink" title="验证方法是否调用"></a>验证方法是否调用</h3><p>示例中的待测试类为EmployeeService：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saveEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(employee.isNew()) &#123;</span><br><span class="line">            employee.create();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        employee.update();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isNew</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">giveIncrementOf</span><span class="params">(<span class="keyword">int</span> percentage)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>对应的测试类为EmployeeServiceTest：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldCreateNewEmployeeIfEmployeeIsNew</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Employee mock = PowerMockito.mock(Employee.class);</span><br><span class="line">        PowerMockito.when(mock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">        EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">        employeeService.saveEmployee(mock);</span><br><span class="line">        Mockito.verify(mock).create();</span><br><span class="line">        Mockito.verify(mock, Mockito.never()).update();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>从上面的示例中看到，首先使用了<code>PowerMockito.mock()</code>而不是<code>PowerMockito.mockStaic()</code>，因为我们要测试的不是静态方法。<code>Mockito.verify(mock).create()</code>验证调用了<code>create()</code>方法。 <code>Mockito.verify(mock, Mockito.never()).update()</code>验证没有调用<code>update()</code>方法。</p>
<p>我们再来看看如何验证静态方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">giveIncrementOf</span><span class="params">(<span class="keyword">int</span> percentage)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">giveIncrementToAllEmployeesOf</span><span class="params">(<span class="keyword">int</span> percentage)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            Employee.giveIncrementOf(percentage);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span>(Exception e) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldInvoke_giveIncrementOfMethodOnEmployeeWhileGivingIncrement</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">        PowerMockito.mockStatic(Employee.class);</span><br><span class="line">        PowerMockito.doNothing().when(Employee.class);</span><br><span class="line">        Employee.giveIncrementOf(<span class="number">9</span>);</span><br><span class="line">        EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">        employeeService.giveIncrementToAllEmployeesOf(<span class="number">9</span>);</span><br><span class="line">        PowerMockito.verifyStatic();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先还是使用<code>PowerMockito.mockStatic()</code>方法进行mock，然后使用<code>PowerMockito.verifyStatic()</code>验证静态方法是否调用。</p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

<h3 id="u9A8C_u8BC1_u65B9_u6CD5_u7684_u8C03_u7528_u6B21_u6570_u53CA_u8C03_u7528_u987A_u5E8F"><a href="#u9A8C_u8BC1_u65B9_u6CD5_u7684_u8C03_u7528_u6B21_u6570_u53CA_u8C03_u7528_u987A_u5E8F" class="headerlink" title="验证方法的调用次数及调用顺序"></a>验证方法的调用次数及调用顺序</h3><p>示例中的待测试类为EmployeeService：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saveEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(employee.isNew()) &#123;</span><br><span class="line">            employee.create();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        employee.update();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isNew</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">giveIncrementOf</span><span class="params">(<span class="keyword">int</span> percentage)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>对应的测试类为EmployeeServiceTest：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="annotation">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldInvokeIsNewBeforeInvokingCreate</span><span class="params">()</span> </span>&#123;    </span><br><span class="line">       Employee mock = PowerMockito.mock(Employee.class);</span><br><span class="line">       PowerMockito.when(mock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">       EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">       employeeService.saveEmployee(mock);</span><br><span class="line">       InOrder inOrder = Mockito.inOrder(mock);</span><br><span class="line">       inOrder.verify(mock).isNew();</span><br><span class="line">       inOrder.verify(mock).create();</span><br><span class="line">       Mockito.verify(mock, Mockito.never()).update();</span><br><span class="line">	  Mockito.verify(mock, Mockito.times(<span class="number">1</span>)).isNew();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的示例中，通过<code>Mockito.inOrder()</code>方法通过mock的对象构造了<code>InOrder</code>对象，然后按期望的顺序验证方法的调用顺序。<code>Mockito.verify(mock, Mockito.times(1)).isNew()</code>这句的意思是验证<code>Employee</code>类的<code>isNew()</code>方法调用了一次，除此之外，还能验证一些其他的调用次数策略：</p>
<ul>
<li><code>Mockito.times(int n)</code>： 准确的验证方法调用的次数。</li>
<li><code>Mockito.atLeastOnce()</code>： 验证方法至少调用一次 。</li>
<li><code>Mockito.atLeast(int n)</code>： 验证方法最少调用次数 。</li>
<li><code>Mockito.atMost(int n)</code>： 验证方法最多调用次数。</li>
</ul>
<h3 id="u6D4B_u8BD5final_u7C7B_u6216_u65B9_u6CD5"><a href="#u6D4B_u8BD5final_u7C7B_u6216_u65B9_u6CD5" class="headerlink" title="测试final类或方法"></a>测试final类或方法</h3><p>示例中的待测试类为<code>EmployeeGenerator</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeIdGenerator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">getNextId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setEmployeeId</span><span class="params">(<span class="keyword">int</span> nextId)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saveEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(employee.isNew()) &#123;</span><br><span class="line">            employee.setEmployeeId(EmployeeIdGenerator.getNextId());</span><br><span class="line">            employee.create();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        employee.update();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试类为<code>EmployeeServiceTest</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="annotation">@PrepareForTest</span>(EmployeeIdGenerator.class)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldGenerateEmployeeIdIfEmployeeIsNew</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Employee mock = PowerMockito.mock(Employee.class);</span><br><span class="line">        PowerMockito.when(mock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">        PowerMockito.mockStatic(EmployeeIdGenerator.class);</span><br><span class="line">        PowerMockito.when(EmployeeIdGenerator.getNextId()).thenReturn(<span class="number">90</span>);</span><br><span class="line">        EmployeeService employeeService = <span class="keyword">new</span></span><br><span class="line">        EmployeeService();</span><br><span class="line">        employeeService.saveEmployee(mock);</span><br><span class="line">        PowerMockito.verifyStatic();</span><br><span class="line">        EmployeeIdGenerator.getNextId();</span><br><span class="line">        Mockito.verify(mock).setEmployeeId(<span class="number">90</span>);</span><br><span class="line">        Mockito.verify(mock).create();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从示例中看到，测试方法体中并没有什么特殊的存在，只是在<code>EmployeeServiceTest</code>类上使用了<code>@PrepareForTest</code>注解来申明将要用到PowerMock的能力。</p>
<h3 id="u6D4B_u8BD5_u6784_u9020_u65B9_u6CD5"><a href="#u6D4B_u8BD5_u6784_u9020_u65B9_u6CD5" class="headerlink" title="测试构造方法"></a>测试构造方法</h3><p>示例中的待测试类为<code>WelcomeEmail</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WelcomeEmail</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WelcomeEmail</span><span class="params">(<span class="keyword">final</span> Employee employee, <span class="keyword">final</span> String message)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saveEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(employee.isNew()) &#123;</span><br><span class="line">            employee.setEmployeeId(EmployeeIdGenerator.getNextId());</span><br><span class="line">            employee.create();</span><br><span class="line">            WelcomeEmail emailSender = <span class="keyword">new</span> WelcomeEmail(employee,</span><br><span class="line">            <span class="string">"Welcome to Mocking with PowerMock How-to!"</span>);</span><br><span class="line">            emailSender.send();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        employee.update();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试类为<code>WelcomeEmailTest</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span>  <span class="title">WelcomeEmailTest</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@PrepareForTest</span>(&#123;EmployeeIdGenerator.class, EmployeeService.class&#125;)</span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="annotation">@Test</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldSendWelcomeEmailToNewEmployees</span><span class="params">()</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Employee employeeMock = PowerMockito.mock(Employee.class);</span><br><span class="line">            PowerMockito.when(employeeMock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">            PowerMockito.mockStatic(EmployeeIdGenerator.class);</span><br><span class="line">            WelcomeEmail welcomeEmailMock = PowerMockito.mock(WelcomeEmail.class);</span><br><span class="line">            PowerMockito.whenNew(WelcomeEmail.class).withArguments(employeeMock, <span class="string">"Welcome"</span>).thenReturn(welcomeEmailMock);</span><br><span class="line">            EmployeeService employeeService = <span class="keyword">new</span> EmployeeService();</span><br><span class="line">            employeeService.saveEmployee(employeeMock);</span><br><span class="line"></span><br><span class="line">            PowerMockito.verifyNew(WelcomeEmail.class).withArguments(employeeMock, <span class="string">"Welcome"</span>);</span><br><span class="line">            Mockito.verify(welcomeEmailMock).send();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面示例中可以看到，通过PowerMock的链式语法<code>PowerMockito.whenNew().withArguments().thenReturn()</code><br>可以mock类的构造函数，通过<code>PowerMockito.verifyNew().withArguments()</code>验证类的构造函数。</p>
<h3 id="Answer_u6A21_u5F0F"><a href="#Answer_u6A21_u5F0F" class="headerlink" title="Answer模式"></a>Answer模式</h3><p>在某些边缘的情况下不可能通过简单地通过<code>PowerMockito.when().thenReturn()</code>对方法进行模拟，这时可以使用Answer接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldReturnCountOfEmployeesFromTheServiceWithDefaultAnswer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">			Employee employeeMock = PowerMockito.mock(Employee.class);</span><br><span class="line">         	PowerMockito.when(employeeMock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">       	EmployeeService mock = PowerMockito.mock(EmployeeService.class, <span class="keyword">new</span> Answer() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Object <span class="title">answer</span><span class="params">(InvocationOnMock invocation)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">10</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        	&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的示例中，被mock的<code>EmployeeService</code>类调用任何方法都会返回10，因为都会响应Answer接口的<code>answewr</code>方法。此时我们就需要<code>InbocationOnMock</code>类提供的方法进行响应分流，它支持的方法如下：</p>
<ul>
<li><code>callRealMethod()</code>：调用真正的方法 。</li>
<li><code>getArguments()</code>：获取所有参数。</li>
<li><code>getMethod()</code>：返回mock实例调用的方法。</li>
<li><code>getMock()</code>：获取mock实例。</li>
</ul>
<h3 id="u4F7F_u7528spy_u8FDB_u884C_u90E8_u5206_u6A21_u62DF"><a href="#u4F7F_u7528spy_u8FDB_u884C_u90E8_u5206_u6A21_u62DF" class="headerlink" title="使用spy进行部分模拟"></a>使用spy进行部分模拟</h3><p>示例中的待测试类<code>EmployeeService</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saveEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(employee.isNew()) &#123;</span><br><span class="line">            createEmployee(employee);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        employee.update();</span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">createEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        employee.setEmployeeId(EmployeeIdGenerator.getNextId());</span><br><span class="line">        employee.create();</span><br><span class="line">        WelcomeEmail emailSender = <span class="keyword">new</span> WelcomeEmail(employee,</span><br><span class="line">        <span class="string">"Welcome"</span>);</span><br><span class="line">        emailSender.send();</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>测试类<code>EmployeeServiceTest</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldInvokeTheCreateEmployeeMethodWhileSavingANewEmployee</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> EmployeeService spy = PowerMockito.spy(<span class="keyword">new</span> EmployeeService());</span><br><span class="line">        <span class="keyword">final</span> Employee employeeMock = PowerMockito.mock(Employee.class);</span><br><span class="line">        PowerMockito.when(employeeMock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">        PowerMockito.doNothing().when(spy).createEmployee(employeeMock);</span><br><span class="line">        spy.saveEmployee(employeeMock);</span><br><span class="line">        Mockito.verify(spy).createEmployee(employeeMock);      </span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>被<code>PowerMockito.spy()</code>的类，它的方法只能使用<code>PowerMockito.doNothing()</code>、<code>PowerMockito.doReturn()</code>、<code>PowerMockito.doThrow()</code>来模拟，否则调用方法时都是真实调用。也就是说被mock的类，里面的方法全都是被mock的，如果想真实调用某个方法，需要用callRealMethod方法。被spy的类，里面的方法是没有被mock的，调用时候是真实调用，除非单独mock里面的某个方法。</p>
<h3 id="u6A21_u62DF_u79C1_u6709_u65B9_u6CD5"><a href="#u6A21_u62DF_u79C1_u6709_u65B9_u6CD5" class="headerlink" title="模拟私有方法"></a>模拟私有方法</h3><p>示例中待测试的类为<code>EmployeeService</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeService</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createEmployee</span><span class="params">(Employee employee)</span> </span>&#123;</span><br><span class="line">        employee.setEmployeeId(EmployeeIdGenerator.getNextId());</span><br><span class="line">        employee.create();</span><br><span class="line">        WelcomeEmail emailSender = <span class="keyword">new</span> WelcomeEmail(employee, <span class="string">"Welcome"</span>);</span><br><span class="line">        emailSender.send();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试类为<code>EmployeeServiceTest</code>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span>   <span class="title">EmployeeServiceTest</span></span>&#123;</span><br><span class="line"><span class="annotation">@PrepareForTest</span>(&#123;EmployeeIdGenerator.class, EmployeeService.class&#125;)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EmployeeServiceTest</span> <span class="keyword">extends</span> <span class="title">PowerMockTestCase</span> </span>&#123;</span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">shouldInvokeTheCreateEmployeeMethodWhileSavingANewEmployee</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> EmployeeService spy = PowerMockito.spy(<span class="keyword">new</span> EmployeeService());</span><br><span class="line">        <span class="keyword">final</span> Employee employeeMock = PowerMockito.mock(Employee.class);</span><br><span class="line">        PowerMockito.when(employeeMock.isNew()).thenReturn(<span class="keyword">true</span>);</span><br><span class="line">        PowerMockito.doNothing().when(spy, <span class="string">"createEmployee"</span>, employeeMock);</span><br><span class="line">        spy.saveEmployee(employeeMock);</span><br><span class="line">        PowerMockito.verifyPrivate(spy).invoke(<span class="string">"createEmployee"</span>, employeeMock);</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从示例中看到，测试方法体中同样没有什么特殊的存在，只是在<code>EmployeeServiceTest</code>类上使用了两个注解来申明将要用到PowerMock的能力。</p>
<h3 id="TestNG_u548CPowerMock_u7684Maven_u914D_u7F6E"><a href="#TestNG_u548CPowerMock_u7684Maven_u914D_u7F6E" class="headerlink" title="TestNG和PowerMock的Maven配置"></a>TestNG和PowerMock的Maven配置</h3><p>在Maven的pom文件中需要配置如下信息：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.powermock<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>powermock-api-mockito<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.powermock<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>powermock-api-support<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.powermock<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>powermock-module-testng<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.powermock<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>powermock-module-testng-common<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.testng<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>testng<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">version</span>&gt;</span>6.10<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="title">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="u5355_u5143_u6D4B_u8BD5_u89C4_u8303"><a href="#u5355_u5143_u6D4B_u8BD5_u89C4_u8303" class="headerlink" title="单元测试规范"></a>单元测试规范</h2><p>我们后端Java开发需要使用上述的TestNG和PowerMock两个工具进行单元测试编写，以下是我们应该遵循的一些规范：</p>
<ul>
<li>Team Leader在将PRD拆分为开发任务时，对任务的估时要包含单元测试的编写时间。</li>
<li>Team Leader在Review时，单元测试是否编写和功能性是否开发完成同等重要，即一个任务的交付不只是功能按需开发完毕，同时要包含完整的单元测试。</li>
<li>我们现在开发实体时都是基于标准自定义实体的流程进行开发，即每个实体都会有一个XXXBusinessService，那么对应的测试类应该是XXXBusinessServiceTest。</li>
<li>在SaaS层做单元测试，只对业务逻辑进行验证，所有用到的Paas层的服务及SaaS层的其他服务都需要进行mock，即我们这层的单元测试不会真正的对DAO进行操作。</li>
<li>测试中需要用的mock类和参数应该在前置方法中统一处理，测试方法中按需使用即可，如有特别需要的，可以在测试方法中单独处理。</li>
<li>每个XXXBusinessService中的方法应该对应多个测试方法，即方法体里的每个分支对应一个测试方法，这里的分支指<code>if else</code>、<code>try catch</code>、<code>switch case</code>。</li>
<li>每个待测方法至少应该有对应的三个测试方法：<ul>
<li>对入参健壮性校验的测试方法。</li>
<li>正向主业务逻辑的测试方法。</li>
<li>反向主业务逻辑的测试方法。 </li>
</ul>
</li>
<li>测试方法中要进行方法是否调用的验证、方法调用次数的验证、返回结果的预期验证。</li>
<li>如果被测试的public方法中用了private方法，那么需要对private方法进行mock。然后单独对该private方法写单元测试进行验证。</li>
<li>单元测试写完后，之后只要对方法进行修改，在提交前必须要全局跑一遍单元测试，保证没有问题后再提交代码。</li>
<li>测试方法的命名使用驼峰形式，并且要尽可能表达测试目的，比如<code>public void shouldInvokeTheCreateEmployeeMethodWhileSavingANewEmployee()</code>。</li>
<li>测试方法除了命名以外，重要逻辑需要有注释进行补充说明。</li>
</ul>
<h2 id="u5355_u5143_u6D4B_u8BD5_u793A_u4F8B"><a href="#u5355_u5143_u6D4B_u8BD5_u793A_u4F8B" class="headerlink" title="单元测试示例"></a>单元测试示例</h2><p>下面以派工的一个方法为例，待测试类为<code>FieldJobBusinessServiceImpl</code>，待测试的方法为<code>querySupportStaff</code>，测试类为<code>FieldJobBusinessServiceImplTest</code>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 待测方法</span></span><br><span class="line"><span class="annotation">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProcessorResult <span class="title">querySupportStaff</span><span class="params">(String operation, Long referEntityId, BusinessWebContext businessWebContext)</span> </span>&#123;</span><br><span class="line">        ProcessorResult processorResult = <span class="keyword">new</span> ProcessorResult();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(StringUtils.isBlank(operation))&#123;</span><br><span class="line">            processorResult.setStatusCode(CodeMessage.PARAM_NULL_ERR.getCode());</span><br><span class="line">            processorResult.setMessage(CodeMessage.PARAM_NULL_ERR.getMsg());</span><br><span class="line">            <span class="keyword">return</span> processorResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(!operation.equals(create) &amp;&amp; !operation.equals(assign) &amp;&amp; !operation.equals(transfer))&#123;</span><br><span class="line">            processorResult.setStatusCode(CodeMessage.PARAM_FORMAT_ERR.getCode());</span><br><span class="line">            processorResult.setMessage(CodeMessage.PARAM_FORMAT_ERR.getMsg());</span><br><span class="line">            <span class="keyword">return</span> processorResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(operation.equals(create))&#123;</span><br><span class="line">            processorResult = queryStaff4Create(referEntityId, businessWebContext);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(operation.equals(assign) || operation.equals(transfer))&#123;</span><br><span class="line">            processorResult = queryStaff4AssignOrTransfer(businessWebContext);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> processorResult;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>上面的待测方法一共有四个分支，方法实现时用到了两个私有方法<code>queryStaff4Create</code>和<code>queryStaff4AssignOrTransfer</code>。测试类中对该方法的测试方法有五个，分别为：</p>
<ul>
<li>参数为空时的测试：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">     * 测试参数为空时的逻辑</span><br><span class="line">     * <span class="doctag">@throws</span> Exception</span><br><span class="line">     */</span></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">querySupportStaffWithNullParameters</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String operation = <span class="keyword">null</span>;</span><br><span class="line">        Long referEntityId = <span class="keyword">null</span>;</span><br><span class="line">        ProcessorResult processorResult = fieldJobBusinessServiceImpl.querySupportStaff(operation, referEntityId, businessWebContext);</span><br><span class="line">        Assert.assertTrue(processorResult.getStatusCode() == CodeMessage.PARAM_NULL_ERR.getCode());</span><br><span class="line">        Assert.assertTrue(processorResult.getMessage().equals(CodeMessage.PARAM_NULL_ERR.getMsg()));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>参数格式不正确时的测试：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">     * 测试参数格式不正确时的逻辑</span><br><span class="line">     * <span class="doctag">@throws</span> Exception</span><br><span class="line">     */</span></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">querySupportStaffWithWrongParameters</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String operation = <span class="string">"wrong operation"</span>;</span><br><span class="line">        Long referEntityId = <span class="keyword">null</span>;</span><br><span class="line">        ProcessorResult processorResult = fieldJobBusinessServiceImpl.querySupportStaff(operation, referEntityId, businessWebContext);</span><br><span class="line">        Assert.assertTrue(processorResult.getStatusCode() == CodeMessage.PARAM_FORMAT_ERR.getCode());</span><br><span class="line">        Assert.assertTrue(processorResult.getMessage().equals(CodeMessage.PARAM_FORMAT_ERR.getMsg()));</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>创建派工单时，获取所有人场景的测试：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">     * 测试操作参数为create时的逻辑，即创建派工单时，获取所有人调用</span><br><span class="line">     * <span class="doctag">@throws</span> Exception</span><br><span class="line">     */</span></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">querySupportStaffWithCreateOperation</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String operation = <span class="string">"create"</span>;</span><br><span class="line">        Long referEntityId = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">        ProcessorResult mockSuccessProcessorResult = <span class="keyword">new</span> ProcessorResult();</span><br><span class="line">        mockSuccessProcessorResult.setStatusCode(CodeMessage.SUCCESS.getCode());</span><br><span class="line">        mockSuccessProcessorResult.setMessage(CodeMessage.SUCCESS.getMsg());</span><br><span class="line"></span><br><span class="line">        FieldJobBusinessServiceImpl spyFieldJobBusinessServiceImpl = PowerMockito.spy(<span class="keyword">new</span> FieldJobBusinessServiceImpl());</span><br><span class="line">        <span class="comment">// 在public方法中mock掉private方法，后面会对private方法写单独的单元测试，这里当调用queryStaff4Create方法时什么也不做，并直接返回上面构造好的成功的ProcessorResult</span></span><br><span class="line">        PowerMockito.doReturn(mockSuccessProcessorResult).when(spyFieldJobBusinessServiceImpl,<span class="string">"queryStaff4Create"</span>, Mockito.any(Long.class), Mockito.any(BusinessWebContext.class));</span><br><span class="line"></span><br><span class="line">        ProcessorResult processorResult = spyFieldJobBusinessServiceImpl.querySupportStaff(operation, referEntityId, businessWebContext);</span><br><span class="line">        <span class="comment">// 验证queryStaff4Create方法被调用，并且只被调用过一次</span></span><br><span class="line">        PowerMockito.verifyPrivate(spyFieldJobBusinessServiceImpl, Mockito.times(<span class="number">1</span>)).invoke(<span class="string">"queryStaff4Create"</span>, referEntityId, businessWebContext);</span><br><span class="line">        <span class="comment">// 验证queryStaff4AssignOrTransfer方法没有被调用过</span></span><br><span class="line">        PowerMockito.verifyPrivate(spyFieldJobBusinessServiceImpl, Mockito.never()).invoke(<span class="string">"queryStaff4AssignOrTransfer"</span>, businessWebContext);</span><br><span class="line">        Assert.assertTrue(processorResult.getStatusCode() == CodeMessage.SUCCESS.getCode());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>分配派工单时，获取所有人场景的测试：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">     * 测试操作参数为assign时的逻辑，即分配派工单时，获取所有人调用</span><br><span class="line">     * <span class="doctag">@throws</span> Exception</span><br><span class="line">     */</span></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">querySupportStaffWithAssignOperation</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String operation = <span class="string">"assign"</span>;</span><br><span class="line">        Long referEntityId = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">        ProcessorResult mockSuccessProcessorResult = <span class="keyword">new</span> ProcessorResult();</span><br><span class="line">        mockSuccessProcessorResult.setStatusCode(CodeMessage.SUCCESS.getCode());</span><br><span class="line">        mockSuccessProcessorResult.setMessage(CodeMessage.SUCCESS.getMsg());</span><br><span class="line"></span><br><span class="line">        FieldJobBusinessServiceImpl spyFieldJobBusinessServiceImpl = PowerMockito.spy(<span class="keyword">new</span> FieldJobBusinessServiceImpl());</span><br><span class="line">        <span class="comment">// 在public方法中mock掉private方法，后面会对private方法写单独的单元测试，这里当调用queryStaff4AssignOrTransfer方法时什么也不做，并直接返回上面构造好的成功的ProcessorResult</span></span><br><span class="line">        PowerMockito.doReturn(mockSuccessProcessorResult).when(spyFieldJobBusinessServiceImpl,<span class="string">"queryStaff4AssignOrTransfer"</span>, Mockito.any(BusinessWebContext.class));</span><br><span class="line"></span><br><span class="line">        ProcessorResult processorResult = spyFieldJobBusinessServiceImpl.querySupportStaff(operation, referEntityId, businessWebContext);</span><br><span class="line">        <span class="comment">// 验证queryStaff4AssignOrTransfer方法被调用，并且只被调用过一次</span></span><br><span class="line">        PowerMockito.verifyPrivate(spyFieldJobBusinessServiceImpl, Mockito.times(<span class="number">1</span>)).invoke(<span class="string">"queryStaff4AssignOrTransfer"</span>, businessWebContext);</span><br><span class="line">        <span class="comment">// 验证queryStaff4Create方法没有被调用过</span></span><br><span class="line">        PowerMockito.verifyPrivate(spyFieldJobBusinessServiceImpl, Mockito.never()).invoke(<span class="string">"queryStaff4Create"</span>, referEntityId, businessWebContext);</span><br><span class="line">        Assert.assertTrue(processorResult.getStatusCode() == CodeMessage.SUCCESS.getCode());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>转移派工单时，获取所有人场景的测试：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line">     * 测试操作参数为transfer时的逻辑，即转移派工单时，获取所有人调用</span><br><span class="line">     * <span class="doctag">@throws</span> Exception</span><br><span class="line">     */</span></span><br><span class="line">    <span class="annotation">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">querySupportStaffWithTransferOperation</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String operation = <span class="string">"transfer"</span>;</span><br><span class="line">        Long referEntityId = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">        ProcessorResult mockSuccessProcessorResult = <span class="keyword">new</span> ProcessorResult();</span><br><span class="line">        mockSuccessProcessorResult.setStatusCode(CodeMessage.SUCCESS.getCode());</span><br><span class="line">        mockSuccessProcessorResult.setMessage(CodeMessage.SUCCESS.getMsg());</span><br><span class="line"></span><br><span class="line">        FieldJobBusinessServiceImpl spyFieldJobBusinessServiceImpl = PowerMockito.spy(<span class="keyword">new</span> FieldJobBusinessServiceImpl());</span><br><span class="line">        <span class="comment">// 在public方法中mock掉private方法，后面会对private方法写单独的单元测试，这里当调用queryStaff4AssignOrTransfer方法时什么也不做，并直接返回上面构造好的成功的ProcessorResult</span></span><br><span class="line">        PowerMockito.doReturn(mockSuccessProcessorResult).when(spyFieldJobBusinessServiceImpl,<span class="string">"queryStaff4AssignOrTransfer"</span>, Mockito.any(BusinessWebContext.class));</span><br><span class="line"></span><br><span class="line">        ProcessorResult processorResult = spyFieldJobBusinessServiceImpl.querySupportStaff(operation, referEntityId, businessWebContext);</span><br><span class="line">        <span class="comment">// 验证queryStaff4AssignOrTransfer方法被调用，并且只被调用过一次</span></span><br><span class="line">        PowerMockito.verifyPrivate(spyFieldJobBusinessServiceImpl, Mockito.times(<span class="number">1</span>)).invoke(<span class="string">"queryStaff4AssignOrTransfer"</span>, businessWebContext);</span><br><span class="line">        <span class="comment">// 验证queryStaff4Create方法没有被调用过</span></span><br><span class="line">        PowerMockito.verifyPrivate(spyFieldJobBusinessServiceImpl, Mockito.never()).invoke(<span class="string">"queryStaff4Create"</span>, referEntityId, businessWebContext);</span><br><span class="line">        Assert.assertTrue(processorResult.getStatusCode() == CodeMessage.SUCCESS.getCode());</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>从上面的测试方法中可以看到，对用到的私有方法进行了mock，并且对方法的调用进行了验证。代码中都有注释，这里不再累赘。测试方法写完后，将其配置在TestNG的配置文件中<code>/scr/test/resources/testng.xml</code>：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="doctype">&lt;!DOCTYPE suite SYSTEM "http://testng.org/testng-1.0.dtd" &gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">suite</span> <span class="attribute">name</span>=<span class="value">"Field Service Cloud"</span> <span class="attribute">verbose</span>=<span class="value">"1"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">test</span> <span class="attribute">name</span>=<span class="value">"FieldJobBusinessService"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">classes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">class</span> <span class="attribute">name</span>=<span class="value">"com.rkhd.business.fsc.fieldjob.FieldJobBusinessServiceTest"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">methods</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"querySupportStaffWithNullParameters"</span>/&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"querySupportStaffWithWrongParameters"</span>/&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"querySupportStaffWithCreateOperation"</span>/&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"querySupportStaffWithAssignOperation"</span>/&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span> <span class="attribute">name</span>=<span class="value">"querySupportStaffWithTransferOperation"</span>/&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">methods</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">class</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">classes</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">test</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">suite</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>以后有可能整个manager-service会共用一个配置文件，所以使用<code>&lt;suite&gt;</code>作为产品线的区分，使用<code>&lt;test&gt;</code>作为实体的区分，鼠标右键点击该配置文件，即可看到执行测试的选项。</p>
<p>针对<code>querySupportStaff</code>方法，以上这个五个测试方法可以完全覆盖该方法承载的业务逻辑和场景，并且达到了该方法内百分百的代码覆盖率。使用IDEA的Coverage插件，可以计算测试方法对代码的覆盖率，并可以生成HTML文档。</p>
<p>虽然<code>querySupportStaff</code>测试完了，但是其中的两个私有方法是被mock掉的，并且是都是待测类中的方法，所以接下来就需要对这两个私有方法单独再写单元测试进行验证，写单元测试的规则与规范和<code>querySupportStaff</code>方法的单元测试一致。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>希望该文档能作为后端Java开发保证代码质量的指导手册，养成良好的单元测试编写习惯，最终让我们具备认为完成功能开发只是完成了一半任务的优秀素养。</p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h2 id="u4EC0_u4E48_u662FTestNG"><a href="#u4EC0_u4E48_u662FTestNG" class="headerlink" title="什么是TestNG"></a>什么是TestNG</h2><p>TestNG是一套开源测试框架，是从JUnit继承而来，TestNG意为test next generation。它的优势如下：</p>
<ul>
<li>支持注解。</li>
<li>可以在任意的大线程池中，使用各种策略运行测试（所有方法都可以拥有自己的线程或者每个测试类拥有一个线程等等）。</li>
<li>代码多线程安全测试。</li>
<li>灵活的测试配置。</li>
<li>支持数据驱动测试(@DataProvider)。</li>
<li>支持参数。</li>
<li>强大的执行模型（不再用TestSuite）。</li>
<li>支持各种工具和插件（Eclipse、IDEA、Maven等）。</li>
<li>可以更灵活地嵌入BeanShell。</li>
<li>默认JDK运行时功能和日志记录（无依赖关系）。</li>
<li>依赖应用服务测试的方式。</li>
</ul>
<h2 id="TestNG_u7684_u6700_u7B80_u5355_u793A_u4F8B"><a href="#TestNG_u7684_u6700_u7B80_u5355_u793A_u4F8B" class="headerlink" title="TestNG的最简单示例"></a>TestNG的最简单示例</h2><p>我们先来看一个TestNG最简单的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.junit.AfterClass;</span><br><span class="line"><span class="keyword">import</span> org.junit.BeforeClass;</span><br><span class="line"><span class="keyword">import</span> org.testng.annotations.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@BeforeClass</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beforeClass</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"this is before class"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@Test</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">TestHelloWorld</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"this is HelloWorld test case"</span>);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="annotation">@AfterClass</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterClass</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"this is after class"</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从面上的简单示例中可以看出TestNG的生命周期，并且可以看到TestNG是使用注解来控制生命周期的。下面我们来看看TestNG支持的各种注解。</p>
<h2 id="TestNG_u7684_u6CE8_u89E3"><a href="#TestNG_u7684_u6CE8_u89E3" class="headerlink" title="TestNG的注解"></a>TestNG的注解</h2><p>TestNG的注解大部分用在方法级别上，一共有六大类注解。</p>
<h3 id="Before_u7C7B_u522B_u548CAfter_u7C7B_u522B_u6CE8_u89E3"><a href="#Before_u7C7B_u522B_u548CAfter_u7C7B_u522B_u6CE8_u89E3" class="headerlink" title="Before类别和After类别注解"></a>Before类别和After类别注解</h3><ul>
<li>@BeforeSuite：被注解的方法将会在所有测试类执行之前运行。</li>
<li>@AfterSuite：被注解的方法将会在所有测试类执行之后运行。</li>
<li>@BeforeTest：被注解的方法将会在当前测试类中的所有测试方法执行之前运行。</li>
<li>@AfterTest：被注解的方法将会在当前测试类中的所有测试方法执行之后运行。</li>
<li>@BeforeClass：被注解的方法将会在当前测试类中的第一个测试方法执行之前运行。</li>
<li>@AfterClass：被注解的方法将会在当前测试类中的最后一个测试方法执行之后运行。</li>
<li>@BeforeMethod：被注解的方法将会在当前测试类中的每个测试方法执行之前运行。</li>
<li>@AfterMethod：被注解的方法将会在当前测试类中的每个测试方法执行之后运行。<br>我们可以根据不同的场景来使用不同的注解。</li>
</ul>]]>
    
    </summary>
    
      <category term="TestNG" scheme="http://www.devtalking.com/tags/TestNG/"/>
    
      <category term="Unit Test" scheme="http://www.devtalking.com/tags/Unit-Test/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[《Google力推的那些前端技术，最近有何进展？》笔记]]></title>
    <link href="http://www.devtalking.com//articles/google-web-tech/"/>
    <id>http://www.devtalking.com//articles/google-web-tech/</id>
    <published>2017-01-08T16:00:00.000Z</published>
    <updated>2018-06-01T17:22:58.513Z</updated>
    <content type="html"><![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>Google是一个伟大的公司，有很多伟大的产品。从Google一直不做桌面系统和对Chrome的大力发展来看，Google对前端事业是非常热衷的。</p>
<h2 id="Angular"><a href="#Angular" class="headerlink" title="Angular"></a>Angular</h2><p>Angular、React、Vue是目前前端框架的三驾马车。我在16年9月写过一篇文章<a href="http://www.devtalking.com/articles/angularjs1x-guide/">《温故而知新-AngularJS 1.x 小记》</a>，当时还叫AngularJS，版本是1.X，那时已经很惊艳了，把前端解释性的脚本语言封装成了面向对象的编程模式，另外对HTML的元素维护提供了便利的方式方法。当时我负责的BPM产品，Process Builder的前端架构果断更新为了AngularJS。时隔两年多，Angular 6都已经发布了，如果我还有机会做前端产品的话，我应该还是会果断选择Angular。</p>
<blockquote>
<p>Angular 6 添加了ng update和ng add这样的新功能，让你的应用程序保持最新的状态，帮助Angular开发者加快创新的步伐。渲染器Project Ivy也有很大的改进，它能使Angular调试更容易，以更快地速度编译和运行，它还可以与现有的应用程序一起使用，Angular团队还在小型Hello World应用程序做了演示，不使用的Angular功能将自动从应用的JavaScript bundle自动删除。</p>
</blockquote>
<h2 id="PWA"><a href="#PWA" class="headerlink" title="PWA"></a>PWA</h2><p>在移动的世界里，Application是重中之重，是构建移动生态的核心。那么构建Mobile App的方式的问题就来了。</p>
<ul>
<li>Native App：既用iOS的原生语言OC、Swift或Android原生语言Java构建的App。</li>
<li>Web App：既使用前端框架和技术（Angular、React、Vue，HTML 5，CSS 3等）构建的，在移动设备上运行展现的App。</li>
<li>Native &amp; Web App：原生语言和Web技术混搭构建的App。这类App还孕育出了像React Native这种使用前端语言解析为原生语言的框架。</li>
</ul>
<p>以上三种类型各有利弊：</p>
<ul>
<li>Native App给用户的体验是最好的，交互动画效果平滑，可以基于硬件的性能做优化等。但是研发周期比较长，并且因为iOS和Android应用的审核机制，导致版本更新周期比较长，修复完问题或有新需求增加后无法做到及时更系用户设备上的App。</li>
<li>Web App可以做到版本的热更新，研发周期短，Web端和移动端可共享研发资源。但是用户体验比较差，一些需要使用到硬件功能不好实现。</li>
<li>Native &amp; Web App则致力于综合他们的优点，规避他们的缺点。</li>
</ul>
<p>Progressive Web App（<a href="https://lavas.baidu.com/pwa" target="_blank" rel="external">PWA</a>）就是Google推出的提升Web App体验，给用户原生应用体验的框架。主要特点包括：</p>
<ul>
<li>可靠 - 即使在不稳定的网络环境下，也能瞬间加载并展现。</li>
<li>体验 - 快速响应，并且有平滑的动画响应用户的操作</li>
<li>粘性 - 像设备上的原生应用，具有沉浸式的用户体验，用户可以添加到桌面<br>目前已支持Android，iOS 11.3以后版本，Safari，Chrome，MS Edge。</li>
</ul>
<a id="more"></a>
<h2 id="Service_Worker"><a href="#Service_Worker" class="headerlink" title="Service Worker"></a>Service Worker</h2><p><a href="https://developers.google.com/web/fundamentals/primers/service-workers/?hl=zh-cn" target="_blank" rel="external">Service Worker</a>是Chrome团队力推的一个Web API，它将开发人员从页面的生命周期中解放出来，运行于浏览器后台，可以控制打开作用范围下的所有页面请求，使Web应用程序能够脱机工作，打开了通向不需要网页或用户交互的功能的大门。目前，它们已包括如推送通知和后台同步等功能。将来，还会支持如定期同步或地理围栏等其他功能。</p>
<p>服务工作线程相关注意事项：</p>
<ul>
<li>它是一种 JavaScript 工作线程，无法直接访问 DOM。 服务工作线程通过响应 postMessage 接口发送的消息来与其控制的页面通信，页面可在必要时对 DOM 执行操作。</li>
<li>服务工作线程是一种可编程网络代理，让您能够控制页面所发送网络请求的处理方式。</li>
<li>它在不用时会被中止，并在下次有需要时重启，因此，您不能依赖于服务工作线程的 onfetch 和 onmessage 处理程序中的全局状态。如果存在您需要持续保存并在重启后加以重用的信息，服务工作线程可以访问 IndexedDB API。</li>
<li>服务工作线程广泛地利用了 promise，因此如果您不熟悉 promise，则应停下阅读此内容，看一看 Promise 简介。</li>
</ul>
<h2 id="WebAssembly"><a href="#WebAssembly" class="headerlink" title="WebAssembly"></a>WebAssembly</h2><p><a href="http://webassembly.org.cn/" target="_blank" rel="external">WebAssembly</a>是由主浏览器厂商组成的W3C社区团体制定的一个新的规范。是一个字节码技术的底层编程语言，是一种编译性语言，它能使网站能够运行用C或C ++等语言编写的高性能低级代码，为Web打开了新世界，今年3月，来自Autodesk的AutoCAD就采用了35年前的代码库，并用WebAssembly编译让其直接在浏览器中运行，这意味着，无论你的设备或操作系统如何，你都可以直接在浏览器中用CAD绘图。</p>
<p>同样是增强Web交互和承载内容的工具。</p>
<h2 id="Polymer"><a href="#Polymer" class="headerlink" title="Polymer"></a>Polymer</h2><p><a href="https://www.ibm.com/developerworks/cn/web/wa-polymer/index.html" target="_blank" rel="external">Polymer</a> 是Google主推的一个 JavaScript 库，它可帮助你创建自定义的可重用 HTML 元素，并使用它们来构建高性能、可维护的 App。它的目的就是使用可重用的、可组合的、可视的功能组件组装现代的移动 Web 应用程序。</p>
<h2 id="AMP"><a href="#AMP" class="headerlink" title="AMP"></a>AMP</h2><p><a href="https://www.ampproject.org/zh_cn/learn/overview/" target="_blank" rel="external">Accelerated Mobile Pages（AMP）</a>是Google带领开发的开源项目，目的是为提升移动设备对网站的访问速度。主要分为AMP HTML、AMP JS、AMP Cache三部分：</p>
<ul>
<li>AMP HTML是描述网页所用的标记语言，相当于普通网页使用的HTML之亚种。AMP HTML在图像显示等方面使用与HTML不同的专用标签，另外还限制了HTML部分功能的使用。</li>
<li>AMP JS是一套JavaScript库，保证AMP HTML的正确和快速显示。除此之外，AMP JS还负责在只支持普通HTML的浏览器中担任桥梁，使其能正确支持AMP HTML的专用功能。AMP HTML中可以调用该函数库。</li>
<li>AMP Cache是缓存并传输AMP页面的CDN，进一步提高AMP网页的性能。用户在搜索引擎中点击AMP网页时，实际上访问的是优化后的缓存页面。Google的AMP Cache名为Google AMP Cache。</li>
</ul>
<h2 id="Lighthouse"><a href="#Lighthouse" class="headerlink" title="Lighthouse"></a>Lighthouse</h2><p>Lighthouse是一个分析网络质量的工具，为你提供网站性能衡量指标和指导，它可以直接从Chrome DevTools内部进行访问，从命令行运行或与其他开发产品集成，仅在2018年，就有50万开发人员在他们的网站上运行Lighthouse。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>以上技术几乎是一套构建高性能高质量Web App的组合套装。Angular作为构建Web App的总框架，使用Polymer和AMP保证性能，使用PWA和Service Worker使Web App达到原生的体验，使用WebAssembly进一步增加Web App的交互和展示内容，最后使用Lighthouse对性能质量监控护航。</p>
<p>阅读文章：<a href="http://www.infoq.com/cn/news/2018/05/Google-arch-development" target="_blank" rel="external">《Google力推的那些前端技术，最近有何进展？》</a></p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

]]></content>
    <summary type="html">
    <![CDATA[<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>Google是一个伟大的公司，有很多伟大的产品。从Google一直不做桌面系统和对Chrome的大力发展来看，Google对前端事业是非常热衷的。</p>
<h2 id="Angular"><a href="#Angular" class="headerlink" title="Angular"></a>Angular</h2><p>Angular、React、Vue是目前前端框架的三驾马车。我在16年9月写过一篇文章<a href="http://www.devtalking.com/articles/angularjs1x-guide/">《温故而知新-AngularJS 1.x 小记》</a>，当时还叫AngularJS，版本是1.X，那时已经很惊艳了，把前端解释性的脚本语言封装成了面向对象的编程模式，另外对HTML的元素维护提供了便利的方式方法。当时我负责的BPM产品，Process Builder的前端架构果断更新为了AngularJS。时隔两年多，Angular 6都已经发布了，如果我还有机会做前端产品的话，我应该还是会果断选择Angular。</p>
<blockquote>
<p>Angular 6 添加了ng update和ng add这样的新功能，让你的应用程序保持最新的状态，帮助Angular开发者加快创新的步伐。渲染器Project Ivy也有很大的改进，它能使Angular调试更容易，以更快地速度编译和运行，它还可以与现有的应用程序一起使用，Angular团队还在小型Hello World应用程序做了演示，不使用的Angular功能将自动从应用的JavaScript bundle自动删除。</p>
</blockquote>
<h2 id="PWA"><a href="#PWA" class="headerlink" title="PWA"></a>PWA</h2><p>在移动的世界里，Application是重中之重，是构建移动生态的核心。那么构建Mobile App的方式的问题就来了。</p>
<ul>
<li>Native App：既用iOS的原生语言OC、Swift或Android原生语言Java构建的App。</li>
<li>Web App：既使用前端框架和技术（Angular、React、Vue，HTML 5，CSS 3等）构建的，在移动设备上运行展现的App。</li>
<li>Native &amp; Web App：原生语言和Web技术混搭构建的App。这类App还孕育出了像React Native这种使用前端语言解析为原生语言的框架。</li>
</ul>
<p>以上三种类型各有利弊：</p>
<ul>
<li>Native App给用户的体验是最好的，交互动画效果平滑，可以基于硬件的性能做优化等。但是研发周期比较长，并且因为iOS和Android应用的审核机制，导致版本更新周期比较长，修复完问题或有新需求增加后无法做到及时更系用户设备上的App。</li>
<li>Web App可以做到版本的热更新，研发周期短，Web端和移动端可共享研发资源。但是用户体验比较差，一些需要使用到硬件功能不好实现。</li>
<li>Native &amp; Web App则致力于综合他们的优点，规避他们的缺点。</li>
</ul>
<p>Progressive Web App（<a href="https://lavas.baidu.com/pwa">PWA</a>）就是Google推出的提升Web App体验，给用户原生应用体验的框架。主要特点包括：</p>
<ul>
<li>可靠 - 即使在不稳定的网络环境下，也能瞬间加载并展现。</li>
<li>体验 - 快速响应，并且有平滑的动画响应用户的操作</li>
<li>粘性 - 像设备上的原生应用，具有沉浸式的用户体验，用户可以添加到桌面<br>目前已支持Android，iOS 11.3以后版本，Safari，Chrome，MS Edge。</li>
</ul>]]>
    
    </summary>
    
      <category term="Technology" scheme="http://www.devtalking.com/tags/Technology/"/>
    
      <category term="技术" scheme="http://www.devtalking.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>
