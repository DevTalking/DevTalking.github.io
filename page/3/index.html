<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121973094-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121973094-1');
</script>


<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
 <script type="text/x-mathjax-config">
 MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});
 </script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #272822; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #272822, 0 0 5px #272822; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #272822;    /*上边框颜色*/
        border-left-color: #272822;    /*左边框颜色*/
    }
</style>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  
    <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  


<link rel="stylesheet" type="text/css" href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" />

<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.2"/>




  <meta name="keywords" content="Hexo,next" />



  <link rel="alternate" href="/atom.xml" title="程序员说" type="application/atom+xml" />



  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />


<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="程序员说">
<meta property="og:url" content="http://www.devtalking.com/page/3/index.html">
<meta property="og:site_name" content="程序员说">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="程序员说">
<meta name="twitter:description">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>



  <title> 程序员说 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?980738dc41a50d91861a17ad4b768a1f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  
<script type="text/javascript">
    //微信二维码点击背景关闭
    $('body').delegate('.-mob-share-weixin-qrcode-bg','click', function(){
         $(".-mob-share-weixin-qrcode-close").trigger("click");
    }); 
</script>


  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">程序员说</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home"></i> <br />
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th"></i> <br />
            
            分類
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive"></i> <br />
            
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags"></i> <br />
            
            標籤
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                <a class="post-title-link" href="/articles/machine-learning-8/" itemprop="url">
                  机器学习笔记八之多项式回归、拟合程度、模型泛化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2018-03-15T00:00:00+08:00" content="2018-03-15">
              2018-03-15
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习算法/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/articles/machine-learning-8/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/articles/machine-learning-8/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在前面的笔记中，我们使用的样本数据都是具有一定线性关系的，我们使用简单线性回归或者多元线性回归来拟合这些数据。但是这种具有强假设的数据集在实际应用中是比较少的，大多数的样本数据都没有明显的线性关系，那么这一节主要来看看如何对非线性关系的样本数据进行处理并预测。</p>
<h2 id="u591A_u9879_u5F0F_u56DE_u5F52"><a href="#u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>多项式回归就是我们要使用的方法，其本质上还是基于线性回归的数学原理，只不过是对损失函数进行巧妙的改变使得原本我们用线性回归求得一条直线，变为求得一条曲线。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/ba54b737f3717cf2fec01814463c4a4c.jpg" alt=""><br>在线性回归的问题中，对于这些数据（蓝色点），我们是要寻找一条直线，让这条直线尽可能的拟合这些数据，如果数据只有一个特征的话，我们的模型就是：</p>
<p>$$ y=ax + b $$</p>
<p>$x$就是数据的已知特征，$a$和$b$是我们的模型需要求出的参数。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/f6a83a4e5b65ca84c17ff6d5468c63e2.jpg" alt=""></p>
<p>那如果我们的数据是像上图这种分布形态呢？显然红色那条直线无法拟合这些数据，而黄色那条曲线能更好的拟合这些数据。在样本数据特征数量不变的情况下，这条黄色曲线的模型其实就是一个一元二次方程：</p>
<p>$$y = ax^2+bx+c$$</p>
<p>我们抛开这个方程表示的坐标系形态来看，假设将$x^2$看作$x_1$，将$x$看作$x_2$，那么方式就可以写为：</p>
<p>$$y=ax_1+bx_2+c$$</p>
<p>这就变成了具有两个特征的样本数据的线性回归问题。现在大家就可以理解在本节开头所说的对损失函数进行巧妙改变的含义了吧。</p>
<p>所以对与多项式回归而言，就是需要我们为样本数据增加一个特征，使之可以用线性回归的原理更好的拟合样本数据，但是求出的是对于原始样本而言的非线性的曲线。</p>
<h3 id="u5B9E_u73B0_u591A_u9879_u5F0F_u56DE_u5F52"><a href="#u5B9E_u73B0_u591A_u9879_u5F0F_u56DE_u5F52" class="headerlink" title="实现多项式回归"></a>实现多项式回归</h3><p>这一节我们在Jupyter Notebook中看看如何实现多项式回归：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建样本数据，从-3到3的100个随机数</span></span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 转换为100行1列的矩阵，既样本数据只有一个特征</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求y的方程</span></span><br><span class="line">y = <span class="number">0.5</span> * x**<span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将样本数据绘制出来</span></span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/ccfd43e57d544b2be620023d01b3cf2a.jpg" alt=""></p>
          <div class="post-more-link text-center">
            <a class="btn" href="/articles/machine-learning-8/#more" rel="contents">
              閱讀全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                <a class="post-title-link" href="/articles/machine-learning-7/" itemprop="url">
                  机器学习笔记七之主成分分析法（PCA）、人脸识别应用
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2018-02-27T00:00:00+08:00" content="2018-02-27">
              2018-02-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习算法/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/articles/machine-learning-7/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/articles/machine-learning-7/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>在机器学习的实际使用中，我们都希望有足够多的样本数据，并且有足够的特征来训练我们的模型，所以高维特征数据是经常会用到的，但是高维特征数据同样会带来一些问题：</p>
<ul>
<li>机器学习算法收敛速度下降。</li>
<li>特征难于分辨，很难第一时间认识某个特征代表的意义。</li>
<li>会产生冗余特征，增加模型训练难度，比如说某一品牌型号汽车的特征数据，有从中国采集的，也有从国外采集的，那么就会产生公里/小时和英里/小时这种特征，但其实这两个特征代表的意义是一样的。</li>
<li>无法通过可视化对训练数据进行综合分析。</li>
</ul>
<p>以上问题都是高维特征数据带来的普遍问题，所以将高维特征数据降为低维特征数据就很重要了。这篇笔记主要讲解机器学习中经常用到的降维算法PCA。</p>
<p>PCA是英文Principle Component Analysis的缩写，既主成分分析法。该算法能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。</p>
<h2 id="u7406_u89E3PCA_u7B97_u6CD5_u964D_u7EF4_u7684_u539F_u7406"><a href="#u7406_u89E3PCA_u7B97_u6CD5_u964D_u7EF4_u7684_u539F_u7406" class="headerlink" title="理解PCA算法降维的原理"></a>理解PCA算法降维的原理</h2><p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/96db7d87e7af97e05b630366cc02bcf1.jpg" alt=""></p>
<p>我们从二维降一维的场景来理解PCA降维的原理。上面的图示显示了一个二维的特征坐标，横坐标是特征1，纵座标是特征2。图中的五个点就表示了五条特征数据。我们先来想一下最简单粗暴的降维方式就是丢弃掉其中一个特征。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/b51d7ec017736d4f47fb7e836c33f074.jpg" alt=""></p>
<p>如上图中显示，将特征2抛弃，这里大家先注意一下这五个点落在特征1轴上的间距。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/aa709dfbb796ada2b6b6dae9d0d43d64.jpg" alt=""></p>
<p>或者如上图所示抛弃特征1，大家再注意一下这五个点落在特征2轴上的间距。能很明显的发现，抛弃特征2，落在特征1轴上的五个点之间间距比较大，并且分布均匀。而抛弃特征1，落在特征2轴上的五个点之间间距大多都比较小，并且分布不均匀。</p>
<p>就这两种简单粗暴的降维方式而言，哪种更好一些呢？这里我们先来看看方差的概念，方差描述的是随机数据的离散程度，也就是离期望值（不严谨的说，期望值等同于均值）的距离。所以方差越大，数据的离散程度越高，约分散，离均值的距离越大。方差越小，数据的离散程度越小，约聚合，离均值的距离约小。那么大家可以想想作为机器学习算法训练的样本数据，每组特征应该尽可能的全，在该特征的合理范围内尽可能的广，这样才能更高的代表真实性，也就是每组特征数据的方差应该尽可能的大才好。所以就上面两种情况来看，抛弃特征2的降维方式更好一些。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/articles/machine-learning-7/#more" rel="contents">
              閱讀全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                <a class="post-title-link" href="/articles/machine-learning-6/" itemprop="url">
                  机器学习笔记六之梯度下降、优化梯度公式、随机梯度下降
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2018-02-15T00:00:00+08:00" content="2018-02-15">
              2018-02-15
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习算法/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/articles/machine-learning-6/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/articles/machine-learning-6/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记主要介绍梯度下降法，梯度下降不是机器学习专属的算法，它是一种基于搜索的最优化方法，也就是通过不断的搜索然后找到损失函数的最小值。像上篇笔记中使用正规方程解实现多元线性回归，基于$X_b\theta$这个模型我们可以推导出$\theta$的数学解，但是很多模型是推导不出数学解的，所以就需要梯度下降法来搜索出最优解。</p>
<h2 id="u68AF_u5EA6_u4E0B_u964D_u6CD5_u6982_u5FF5"><a href="#u68AF_u5EA6_u4E0B_u964D_u6CD5_u6982_u5FF5" class="headerlink" title="梯度下降法概念"></a>梯度下降法概念</h2><p>我们来看看在二维坐标里的一个曲线方程：</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/8b3433626c22d7f31a236492458a58e0.jpg" alt=""></p>
<p>纵坐标表示损失函数L的值，横坐标表示系数$\theta$。每一个$\theta$的值都会对应一个损失函数L的值，我们希望损失函数收敛，既找到一个$\theta$的值，使损失函数L的值最小。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/7d33b5dbd153029338804e42063e7dae.jpg" alt=""></p>
<p>在曲线上定义一点A，对应一个$\theta$值，一个损失函数L的值，要判断点A是否是损失函数L的最小值，既求该点的导数，在第一篇笔记中我解释过，点A的导数就是直线M的斜率，直线M是点A的切线，所以导数描述了一个函数在某一点附近的变化率，并且导数大于零时，函数在区间内单调递增，导数小于零时函数在区间内单调递减。所以$\frac {d L}{d \theta}$表示损失函数L增大的变化率，$-\frac {d L}{d \theta}$表示损失函数L减小的变化率。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/79181d4455f5a40c4a652a41a743bc7b.jpg" alt=""></p>
<p>再在曲线上定义一点B，在点A的下方，B点的$\theta$值就是A点的$\theta$值加上让损失函数L递减的变化率$-\eta \frac {d L}{d \theta}$，$\eta$称为步长，既B点在$-\frac {d L}{d \theta}$变化率的基础下移动了多少距离，在机器学习中$\eta$这个值也称为学习率。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/12d864def3bdafa3f3ecda683f2950e4.jpg" alt=""></p>
<p>同理还可以再求得点C，然后看是否是损失函数的L的最小值。所以梯度下降法就是基于损失函数在某一点的变化率$-\frac {d L}{d \theta}$，以及寻找下一个点的步长$\eta$，不停的找到下一个点，然后判断该点处的损失函数值是否为最小值的过程。$-\eta \frac {d L}{d \theta}$就称为梯度。</p>
<p>在第一篇笔记中将极值的时候提到过，当曲线或者曲面很复杂时，会有多个驻点，既局部极值，所以如果运行一次梯度下降法寻找损失函数极值的话很有可能找到的只是局部极小值点。所以在实际运用中我们需要多次运行算法，随机化初始点，然后进行比较找到真正的全局极小值点，所以初始点的位置是梯度下降法的一个超参数。</p>
<p>不过在线性回归的场景中，我们的损失函数$\sum_{i=1}^m(y^{(i)}-\hat y^{(i)})^2$是有唯一极小值的，所以暂时不需要多次执行算法搜寻全局极值。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/articles/machine-learning-6/#more" rel="contents">
              閱讀全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                <a class="post-title-link" href="/articles/machine-learning-5/" itemprop="url">
                  机器学习笔记五之线性回归、评测标准、多元线性回归
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2018-02-05T00:00:00+08:00" content="2018-02-05">
              2018-02-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习算法/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/articles/machine-learning-5/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/articles/machine-learning-5/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>这篇笔记主要介绍线性回归算法，对在第一篇笔记中介绍过的线性回归算法进行实现。kNN算法主要解决的是分类问题，并且它的结果不具备良好的解释性。线性回归算法主要解决回归问题，它的结果具有良好的可解释性，和kNN算法的介绍过程一样，线性回归算法也蕴含了机器学习中很多重要的思想，并且它是许多强大的非线性模型的基础。</p>
<h2 id="u7B80_u5355_u7EBF_u6027_u56DE_u5F52"><a href="#u7B80_u5355_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="简单线性回归"></a>简单线性回归</h2><p>在第一篇笔记中，我们举过房屋面积大小和价格例子，将其绘制在二维坐标图上，横轴表示房屋面积，纵轴表示房屋价格，这里样本特征数据只有一个，那就是房屋面积，而在kNN算法的分类问题中，二维坐标图上横纵轴表示的都是样本特征数据，这是一个比较明显的区别。如果线性回归问题要在图中表示两种样本特征数据的话就需要三维空间坐标来表示。我们一般将只有一种样本特征数据的线性回归问题称为简单线性回归问题。</p>
<h3 id="u56DE_u987E_u7EBF_u6027_u56DE_u5F52"><a href="#u56DE_u987E_u7EBF_u6027_u56DE_u5F52" class="headerlink" title="回顾线性回归"></a>回顾线性回归</h3><p>线性回归其实就是寻找一条直线，最大程度的拟合样本特征和样本输出标记之间的关系。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/a9e2a5cbcffbf0037e8bf53f49cc414e.jpg" alt=""></p>
<p>我们来看上面这张图，大家知道直线的方程是$y=ax+b$，那么点A肯定是在一条直线上，该条直线方程为$y^{(i)}=ax^{(i)}+b$，那么点A的横轴值为$x^{(i)}$，既是A的样本特征值，纵轴值为$y^{(i)}$，既是A的样本输出值。我们假设图中的红线就是拟合直线，方程为$\hat y^{(i)}=ax^{(i)}+b$，也就是将$x^{(i)}$代入这条红线，会得到一个预测的纵轴值$\hat y^{(i)}$。我们希望真值$y^{(i)}$和预测值$\hat y^{(i)}$的差值越小，说明我们的拟合直线拟合的越好。</p>
<p>因为差值有正有负，为了保证都是正数，所以将差值进行平方，之所以不用绝对值，是为了方便求导数。</p>
<blockquote>
<p>方程求导的知识可参阅<a href="http://www.devtalking.com/articles/machine-learning-1/"> 《机器学习笔记一之机器学习定义、导数、最小二乘》 </a>。</p>
</blockquote>
<p>$$ (y^{(i)} - \hat y^{(i)})^2 $$</p>
<p>将所有样本特征都考虑到，既将所有真值和预测值的差值求和：</p>
<p>$$ \sum_{i=1}^m(y^{(i)} - \hat y^{(i)})^2 $$</p>
<p>将$ax^{(i)}+b$代入上面的公式就得到：</p>
<p>$$\sum_{i=1}^m(y^{(i)} - ax^{(i)}-b)^2$$</p>
<p>上面的公式我们称为损失函数（Loss Function），损失函数值越小，我们的拟合直线越好。在Loss函数中，$a$和$b$是变量，所以我们要做的就是找到使Loss函数值最小的$a$和$b$。这个套路是近乎所有参数学习算法常用的套路，既通过分析问题，确定问题的损失函数，通过最优化损失函数获得机器学习的模型。像线性回归、多项式回归、逻辑回归、SVM、神经网络等都是这个套路。</p>
          <div class="post-more-link text-center">
            <a class="btn" href="/articles/machine-learning-5/#more" rel="contents">
              閱讀全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                <a class="post-title-link" href="/articles/machine-learning-4/" itemprop="url">
                  机器学习笔记四之kNN算法、超参数、数据归一化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2018-01-27T00:00:00+08:00" content="2018-01-27">
              2018-01-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习算法/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/articles/machine-learning-4/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/articles/machine-learning-4/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        
          <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-4115205380866695"
     data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Notebook中调用，以及计算器算法的封装规范，kNN的<code>k</code>值如何计算，如何使用Scikit Learn中的kNN算法，还有机器学习算法中的一些主要概念，比如训练数据集、测试数据集，分类准确度，超参数，数据归一化。另外会具体用代码实现第一篇笔记中介绍过的线性回归算法。</p>
<h2 id="u5C01_u88C5kNN_u7B97_u6CD5"><a href="#u5C01_u88C5kNN_u7B97_u6CD5" class="headerlink" title="封装kNN算法"></a>封装kNN算法</h2><p>上一篇笔记中我们对kNN算法在Jupyter Notebook中进行了实现，但是想要复用这个算法就很不方便，所以我们来看看如何在PyCharm中封装算法，并且在Jupyter Notebook中进行调用。</p>
<p>PyCharm的配置这里我就不再累赘，如图所示，我们创建了一个Python文件<code>kNN.py</code>，然后定义了<code>kNNClassify</code>方法，该方法有4个参数，分别是kNN算法的<code>k</code>值，训练样本特征数据集<code>XTrain</code>，训练样本类别数据集<code>yTrain</code>，预测特征数据集<code>x</code>。该方法中的实现和在Jupyter Notebook中实现的一模一样，只不过加了三个断言，让方法的健壮性更好一点。我们给出<strong>N维欧拉定理</strong>：</p>
<p>$$ \sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} $$</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/574075757e0d0a820e2a8a7ef37f79ec.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kNN.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNNClassify</span><span class="params">(k, XTrain, yTrain, x)</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> <span class="number">1</span> &lt;= k &lt;= XTrain.shape[<span class="number">0</span>], <span class="string">"k 的取值范围不正确"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] == yTrain.shape[<span class="number">0</span>], <span class="string">"训练样本数据行数应该与训练结果集行数相同"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">1</span>] == x.shape[<span class="number">0</span>], <span class="string">"训练样本数据特性个数应该与被预测数据特性个数相同"</span></span><br><span class="line"></span><br><span class="line">	distances = [sqrt(np.sum((xTrain - x) ** <span class="number">2</span>)) <span class="keyword">for</span> xTrain <span class="keyword">in</span> XTrain]</span><br><span class="line">	nearest = np.argsort(distances)</span><br><span class="line"></span><br><span class="line">	topKy = [yTrain[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:k]]</span><br><span class="line">	votes = Counter(topKy)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这样我们就在PyCharm中封装好了kNN算法的方法，我们再来看看如何在Jupyter Notebook中调用封装好的方法呢，这就需要使用<code>%run</code>这个命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">raw_data_X = [[<span class="number">3.393533211</span>, <span class="number">2.331273381</span>],</span><br><span class="line">			  [<span class="number">3.110073483</span>, <span class="number">1.781539638</span>],</span><br><span class="line">			  [<span class="number">1.343808831</span>, <span class="number">3.368360954</span>],</span><br><span class="line">			  [<span class="number">3.582294042</span>, <span class="number">4.679179110</span>],</span><br><span class="line">			  [<span class="number">2.280362439</span>, <span class="number">2.866990263</span>],</span><br><span class="line">			  [<span class="number">7.423436942</span>, <span class="number">4.696522875</span>],</span><br><span class="line">			  [<span class="number">5.745051997</span>, <span class="number">3.533989803</span>],</span><br><span class="line">			  [<span class="number">9.172168622</span>, <span class="number">2.511101045</span>],</span><br><span class="line">			  [<span class="number">7.792783481</span>, <span class="number">3.424088941</span>],</span><br><span class="line">			  [<span class="number">7.939820817</span>, <span class="number">0.791637231</span>]</span><br><span class="line">			 ]</span><br><span class="line">raw_data_y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">XTrain = np.array(raw_data_X)</span><br><span class="line">yTrain = np.array(raw_data_y)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">8.093607318</span>, <span class="number">3.365731514</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用%run命令可以引入Python文件，并可使用该Python文件中定义的属性和方法</span></span><br><span class="line">%run ../pycharm/kNN.py</span><br><span class="line">predicty = kNNClassify(<span class="number">6</span>, XTrain, yTrain, x)</span><br><span class="line">predicty</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
          <div class="post-more-link text-center">
            <a class="btn" href="/articles/machine-learning-4/#more" rel="contents">
              閱讀全文 &raquo;
            </a>
          </div>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>


 </div>

        

        
      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://www.devtalking.com/devtalking.png" alt="DevTalking" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DevTalking</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">96</span>
              <span class="site-state-item-name">文章</span>
            </a>
          </div>

         <!-- <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">9</span>
              <span class="site-state-item-name">分類</span>
              </a>
          </div> -->

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">91</span>
              <span class="site-state-item-name">標籤</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="menu-item-icon icon-next-feed"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/宇轩-付-5aa406a6" target="_blank">
                  
                    <i class="fa fa-linkedin"></i> linkedin
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:jace.fu@icloud.com" target="_blank">
                  
                    <i class="fa fa-envelope"></i> Email
                  
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DevTalking</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  

    <script type="text/javascript">
      var disqus_shortname = 'jacefu';
      var disqus_identifier = 'page/3/index.html';
      var disqus_title = '';
      var disqus_url = '';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
    </script>
  


  
  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  

  <script type="text/javascript" src="/lib/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.2" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }

      motionIntegrator.bootstrap();
    });
  </script>

  
  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
