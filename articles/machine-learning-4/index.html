<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121973094-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121973094-1');
</script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-4115205380866695",
          enable_page_level_ads: true
     });
</script>

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
 <script type="text/x-mathjax-config">
 MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});
 </script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #272822; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #272822, 0 0 5px #272822; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #272822;    /*上边框颜色*/
        border-left-color: #272822;    /*左边框颜色*/
    }
</style>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  
    <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  


<link rel="stylesheet" type="text/css" href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" />

<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.2"/>




  <meta name="keywords" content="数据归一化,机器学习," />



  <link rel="alternate" href="/atom.xml" title="程序员说" type="application/atom+xml" />



  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.2" />


<meta name="description" content="&amp;lt;!-- DevTalking Banner1 --&amp;gt;


(adsbygoogle = window.adsbygoogle || []).push({});



上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Not">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记四之kNN算法、超参数、数据归一化">
<meta property="og:url" content="http://www.devtalking.com/articles/machine-learning-4/index.html">
<meta property="og:site_name" content="程序员说">
<meta property="og:description" content="&amp;lt;!-- DevTalking Banner1 --&amp;gt;


(adsbygoogle = window.adsbygoogle || []).push({});



上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Not">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/574075757e0d0a820e2a8a7ef37f79ec.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/13a8bbc81eef52f649e82539248d29ad.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/ab2b99d15c26edf50171a5dbb8e8cecb.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/30144371e3f3e8726b2f83c5cef76564.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/42bf4dc0e2e8cf0d11afd17037d16b59.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/b29f73d52a86bfcc04a919471f6e730f.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/53a1c3214fa97bb5c905e80898cbad63.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/43b6730765831fcc1dbecb24069ed177.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/96cc0403babcacae118a05b14b5d9730.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/f4dbe1f0c876372c0263013bd3e27043.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/c962c7f93465af5cff9be84f884c97fd.jpg">
<meta property="og:image" content="https://devtalking.oss-cn-beijing.aliyuncs.com/a446676198ff1ff737c340099f48c7cb.jpg">
<meta property="og:updated_time" content="2020-06-21T08:12:29.015Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记四之kNN算法、超参数、数据归一化">
<meta name="twitter:description" content="&amp;lt;!-- DevTalking Banner1 --&amp;gt;


(adsbygoogle = window.adsbygoogle || []).push({});



上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Not">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>



  <title> 机器学习笔记四之kNN算法、超参数、数据归一化 | 程序员说 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?980738dc41a50d91861a17ad4b768a1f";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  
<script type="text/javascript">
    //微信二维码点击背景关闭
    $('body').delegate('.-mob-share-weixin-qrcode-bg','click', function(){
         $(".-mob-share-weixin-qrcode-close").trigger("click");
    }); 
</script>


  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">程序员说</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home"></i> <br />
            
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th"></i> <br />
            
            分類
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive"></i> <br />
            
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags"></i> <br />
            
            標籤
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习笔记四之kNN算法、超参数、数据归一化
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            發表於
            <time itemprop="dateCreated" datetime="2018-01-27T00:00:00+08:00" content="2018-01-27">
              2018-01-27
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分類於
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习算法/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习算法</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/articles/machine-learning-4/#comments" itemprop="discussionUrl">
                <span class="post-comments-count disqus-comment-count" data-disqus-identifier="/articles/machine-learning-4/" itemprop="commentsCount"></span>
              </a>
            </span>
            
          

          

        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>上一篇笔记主要介绍了NumPy，Matplotlib和Scikit Learn中Datasets三个库的用法，以及基于欧拉定理的kNN算法的基本实现。这一篇笔记的主要内容是通过PyCharm封装kNN算法并且在Jupyter Notebook中调用，以及计算器算法的封装规范，kNN的<code>k</code>值如何计算，如何使用Scikit Learn中的kNN算法，还有机器学习算法中的一些主要概念，比如训练数据集、测试数据集，分类准确度，超参数，数据归一化。另外会具体用代码实现第一篇笔记中介绍过的线性回归算法。</p>
<h2 id="u5C01_u88C5kNN_u7B97_u6CD5"><a href="#u5C01_u88C5kNN_u7B97_u6CD5" class="headerlink" title="封装kNN算法"></a>封装kNN算法</h2><p>上一篇笔记中我们对kNN算法在Jupyter Notebook中进行了实现，但是想要复用这个算法就很不方便，所以我们来看看如何在PyCharm中封装算法，并且在Jupyter Notebook中进行调用。</p>
<p>PyCharm的配置这里我就不再累赘，如图所示，我们创建了一个Python文件<code>kNN.py</code>，然后定义了<code>kNNClassify</code>方法，该方法有4个参数，分别是kNN算法的<code>k</code>值，训练样本特征数据集<code>XTrain</code>，训练样本类别数据集<code>yTrain</code>，预测特征数据集<code>x</code>。该方法中的实现和在Jupyter Notebook中实现的一模一样，只不过加了三个断言，让方法的健壮性更好一点。我们给出<strong>N维欧拉定理</strong>：</p>
<p>$$ \sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} $$</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/574075757e0d0a820e2a8a7ef37f79ec.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kNN.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kNNClassify</span><span class="params">(k, XTrain, yTrain, x)</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">assert</span> <span class="number">1</span> &lt;= k &lt;= XTrain.shape[<span class="number">0</span>], <span class="string">"k 的取值范围不正确"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] == yTrain.shape[<span class="number">0</span>], <span class="string">"训练样本数据行数应该与训练结果集行数相同"</span></span><br><span class="line">	<span class="keyword">assert</span> XTrain.shape[<span class="number">1</span>] == x.shape[<span class="number">0</span>], <span class="string">"训练样本数据特性个数应该与被预测数据特性个数相同"</span></span><br><span class="line"></span><br><span class="line">	distances = [sqrt(np.sum((xTrain - x) ** <span class="number">2</span>)) <span class="keyword">for</span> xTrain <span class="keyword">in</span> XTrain]</span><br><span class="line">	nearest = np.argsort(distances)</span><br><span class="line"></span><br><span class="line">	topKy = [yTrain[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:k]]</span><br><span class="line">	votes = Counter(topKy)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这样我们就在PyCharm中封装好了kNN算法的方法，我们再来看看如何在Jupyter Notebook中调用封装好的方法呢，这就需要使用<code>%run</code>这个命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">raw_data_X = [[<span class="number">3.393533211</span>, <span class="number">2.331273381</span>],</span><br><span class="line">			  [<span class="number">3.110073483</span>, <span class="number">1.781539638</span>],</span><br><span class="line">			  [<span class="number">1.343808831</span>, <span class="number">3.368360954</span>],</span><br><span class="line">			  [<span class="number">3.582294042</span>, <span class="number">4.679179110</span>],</span><br><span class="line">			  [<span class="number">2.280362439</span>, <span class="number">2.866990263</span>],</span><br><span class="line">			  [<span class="number">7.423436942</span>, <span class="number">4.696522875</span>],</span><br><span class="line">			  [<span class="number">5.745051997</span>, <span class="number">3.533989803</span>],</span><br><span class="line">			  [<span class="number">9.172168622</span>, <span class="number">2.511101045</span>],</span><br><span class="line">			  [<span class="number">7.792783481</span>, <span class="number">3.424088941</span>],</span><br><span class="line">			  [<span class="number">7.939820817</span>, <span class="number">0.791637231</span>]</span><br><span class="line">			 ]</span><br><span class="line">raw_data_y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">XTrain = np.array(raw_data_X)</span><br><span class="line">yTrain = np.array(raw_data_y)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">8.093607318</span>, <span class="number">3.365731514</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用%run命令可以引入Python文件，并可使用该Python文件中定义的属性和方法</span></span><br><span class="line">%run ../pycharm/kNN.py</span><br><span class="line">predicty = kNNClassify(<span class="number">6</span>, XTrain, yTrain, x)</span><br><span class="line">predicty</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="u673A_u5668_u5B66_u4E60_u6D41_u7A0B"><a href="#u673A_u5668_u5B66_u4E60_u6D41_u7A0B" class="headerlink" title="机器学习流程"></a>机器学习流程</h2><p>这一小节我们来看看机器学习的大概流程是怎样的，如下图所示：<br><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/13a8bbc81eef52f649e82539248d29ad.jpg" alt=""></p>
<p>监督学习算法首先需要的是训练数据集，然后通过一个机器学习算法生成一个模型，最后就可以用这个模型来预测新的数据得到结果。通常，我们将使用机器学习生成模型的过程用fit来表示，使用模型预测新的数据的过程用predict来表示。这就是机器学习最基本的一个流程。</p>
<p>在第一篇笔记中，介绍了线性回归的概念，我们最后得到了一个二元线性回归的公式：$ F(a,b) = \sum_{i=1}^n(y_i-(ax_i + b))^2 $。这个公式其实就是通过线性回归算法得到的模型，通过fit过程，训练模型得到<code>a</code>，<code>b</code>，然后通过predict过程预测新的样例数据得到结果。</p>
<p>但是我们发现kNN算法不存在训练模型的过程，因为新的样例数据其实是需要通过训练数据集来进行预测的，所以换个角度来看，kNN算法的模型就是它的训练数据集，在上图中模型阶段其实就是把训练数据集复制了一份作为模型来使用，那么对于fit和predict过程而言，kNN算法的predict过程其实是核心，而fit过程非常简单。</p>
<h2 id="u4F7F_u7528Scikit_Learn_u4E2D_u7684kNN_u7B97_u6CD5"><a href="#u4F7F_u7528Scikit_Learn_u4E2D_u7684kNN_u7B97_u6CD5" class="headerlink" title="使用Scikit Learn中的kNN算法"></a>使用Scikit Learn中的kNN算法</h2><p>这一节我们来看看如何使用Scikit Learn中封装的kNN算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入Scikit Learn中的kNN算法的类库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment"># 初始化kNN算法分类器的实例，参数n_neighbors就是k值</span></span><br><span class="line">kNNClassifier = KNeighborsClassifier(n_neighbors=<span class="number">6</span>)</span><br><span class="line"><span class="comment"># 训练，拟合模型</span></span><br><span class="line">kNNClassifier.fit(XTrain, yTrain)</span><br><span class="line"><span class="comment"># 预测新的样例数据，该方法接受的参数类型为二维数组，如果只有一行也需要转换为一行的二维数组</span></span><br><span class="line">kNNClassifier.predict(x.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>从示例代码中可以看出，Scikit Learn中封装的kNN算法严格遵从了上一节介绍的机器学习的基本流程，其实不止是kNN算法，Scikit Learn中的所有机器学习算法都遵从这个基本流程。</p>
<h2 id="u91CD_u65B0_u5C01_u88C5kNN_u7B97_u6CD5"><a href="#u91CD_u65B0_u5C01_u88C5kNN_u7B97_u6CD5" class="headerlink" title="重新封装kNN算法"></a>重新封装kNN算法</h2><p>所以我们可以优化一下我们之前封装的kNN算法的方法，将其封装为类似Scikit Learn中的方式：<br><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/ab2b99d15c26edf50171a5dbb8e8cecb.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNNClassifier</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 初始化kNN分类器</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> k &gt;= <span class="number">1</span>, <span class="string">"k 值不能小于1"</span></span><br><span class="line"></span><br><span class="line">		self.k = k</span><br><span class="line">		self._XTrain = <span class="keyword">None</span></span><br><span class="line">		self._yTrain = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 根据训练数据集XTrain和yTrain训练kNN分类器，在kNN中这一步就是复制训练数据集</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, XTrain, yTrain)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] == yTrain.shape[<span class="number">0</span>], \</span><br><span class="line">			<span class="string">"训练样本特征数据集的行数要与训练样本分类结果数据集的行数相同"</span></span><br><span class="line">		<span class="keyword">assert</span> XTrain.shape[<span class="number">0</span>] &gt;= self.k, \</span><br><span class="line">			<span class="string">"训练样本特征数据集的行数，既样本点的数量要大于等于k值"</span></span><br><span class="line"></span><br><span class="line">		self._XTrain = XTrain</span><br><span class="line">		self._yTrain = yTrain</span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 输入样本数据，根据模型进行预测</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, XPredict)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> self._XTrain <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._yTrain <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, \</span><br><span class="line">			<span class="string">"在执行predict方法前必须先执行fit方法"</span></span><br><span class="line">		<span class="keyword">assert</span> XPredict.shape[<span class="number">1</span>] == self._XTrain.shape[<span class="number">1</span>], \</span><br><span class="line">			<span class="string">"被预测数据集的特征数，既列数必须与模型数据集中的特征数相同"</span></span><br><span class="line"></span><br><span class="line">		ypredict = [self._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> XPredict]</span><br><span class="line">		<span class="keyword">return</span> np.array(ypredict)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 实现私有的预测方法，kNN算法的核心代码</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">_predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> x.shape[<span class="number">0</span>] == self._XTrain.shape[<span class="number">1</span>], \</span><br><span class="line">			<span class="string">"输入的样本数据的特征数量必须等于模型数据，既训练样本数据的特征数量"</span></span><br><span class="line"></span><br><span class="line">		distance = [sqrt(np.sum((xTrain - x) ** <span class="number">2</span>)) <span class="keyword">for</span> xTrain <span class="keyword">in</span> self._XTrain]</span><br><span class="line">		nearest = np.argsort(distance)</span><br><span class="line">		topK = [self._yTrain[i] <span class="keyword">for</span> i <span class="keyword">in</span> nearest[:self.k]]</span><br><span class="line">		votes = Counter(topK)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> votes.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">"kNN(k=%d)"</span> % self.k</span><br></pre></td></tr></table></figure>
<p>上面的代码清晰的定义了<code>fit</code>和<code>predict</code>方法，至于<code>_predict</code>这个私有方法可以随意，可以将逻辑直接写在<code>predict</code>方法里，也可以拆分出来。然后我们在Jupyter Notebook中再来使用一下我们封装的kNN算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%run ../pycharm/kNN/kNN.py</span><br><span class="line">myKNNClassifier = KNNClassifier(<span class="number">6</span>)</span><br><span class="line">myKNNClassifier.fit(XTrain, yTrain)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">kNN(k=<span class="number">6</span>)</span><br><span class="line">xTrain = x.reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">myKNNClassifier.predict(xTrain)</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h2 id="u5224_u65AD_u673A_u5668_u5B66_u4E60_u7B97_u6CD5_u7684_u6027_u80FD"><a href="#u5224_u65AD_u673A_u5668_u5B66_u4E60_u7B97_u6CD5_u7684_u6027_u80FD" class="headerlink" title="判断机器学习算法的性能"></a>判断机器学习算法的性能</h2><p>现在大家应该知道机器算法的目的主要是训练出模型，然后输入样本，通过模型来预测结果，可见这个模型是非常关键的，模型的好坏直接影响预测结果的准确性，继而对实际运用会产生巨大的影响。模型的训练除了机器学习算法以外，对它影响比较大的还有训练样本数据，我们在实现kNN算法时，是将所有的样本数据用于训练模型，那么模型训练出来后就已经没有数据供我们验证模型的好坏了，只能直接投入真实环境使用，这样的风险是很大的。</p>
<p>所以为了避免上述这种情况，最简单的做法是将所有训练样本数据进行切分，将大部分数据用于训练模型，而另外一小部分数据用来测试训练出的模型，这样如果我们用测试数据发现这个模型不够好，那么我们就有机会在将模型投入真实环境使用之前改进算法，训练出更好的模型。</p>
<p>我们来看看如何封装拆分训练数据的方法：<br><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/30144371e3f3e8726b2f83c5cef76564.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练样本数据 X 和 y 按照 test_radio 分割成 X_train, y_train, X_test, y_test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span><span class="params">(X, y, test_radio = <span class="number">0.2</span>, seed = None)</span>:</span></span><br><span class="line">	<span class="keyword">assert</span> X.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>], \</span><br><span class="line">		<span class="string">"训练样本特征数据集的行数要与训练样本分类结果数据集的行数相同"</span></span><br><span class="line">	<span class="keyword">assert</span> <span class="number">0.0</span> &lt;= test_radio &lt;= <span class="number">1.0</span>, \</span><br><span class="line">		<span class="string">"test_radio 的值必须在 0 到 1 之间"</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 如果 seed 有值，将其设置进numpy的随机函数中</span></span><br><span class="line">	<span class="keyword">if</span> seed:</span><br><span class="line">		np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">	shuffled_indexes = np.random.permutation(len(X))</span><br><span class="line">	test_size = int(len(X) * test_radio)</span><br><span class="line">	test_indexes = shuffled_indexes[:test_size]</span><br><span class="line">	train_indexes = shuffled_indexes[test_size:]</span><br><span class="line"></span><br><span class="line">	X_train = X[train_indexes]</span><br><span class="line">	y_train = y[train_indexes]</span><br><span class="line"></span><br><span class="line">	X_test = X[test_indexes]</span><br><span class="line">	y_test = y[test_indexes]</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> X_train, y_train, X_test, y_test</span><br></pre></td></tr></table></figure>
<p>我们来解读一下上面的代码：</p>
<ul>
<li>首先<code>train_test_split</code>函数有四个参数，两个必填参数，两个非必填有默认值的参数。<code>X</code>是训练样本特征数据集，<code>y</code>是训练样本分类结果数据集，<code>test_radio</code>是设置训练数据和测试数据的比例，<code>seed</code>就很好理解了，就是NumPy的随机函数提供的随机种子机制。</li>
<li>上面代码中有一个方法大家之前应该没见过，那就是<code>permutation(x)</code>，该方法表示返回一个乱序的一维向量，元素从0到x，所以<code>shuffled_indexes</code>是一个乱序的一维向量数组，它的元素总数为训练样本数据的总数，既训练样本数据矩阵的行数，元素的范围从0到训练样本数据的总数。</li>
<li>根据<code>test_radio</code>计算出需要分割出的测试数据数量<code>test_size</code>。</li>
<li>根据<code>test_size</code>从<code>shuffled_indexes</code>中取出<code>test_indexes</code>和<code>train_indexes</code>，这两个数组中存的元素就是作为索引来用的。</li>
<li>根据<code>test_indexes</code>和<code>train_indexes</code>从<code>X</code>和<code>y</code>中得到<code>X_train</code>、<code>y_train</code>、<code>X_test</code>、<code>y_test</code>。</li>
</ul>
<p>之前在Jupyter Notebook中我们使用<code>%run</code>命令使用我们封装的代码 ，这一节我们来看看如何使用<code>import</code>的方式使用我们自己封装的代码。其实这和Jupyter Notebook没多大关系，我们需要做的只是给Python设置一个搜索包的路径而已，这里这会对MacOS，以及安装了Anaconda的环境作以说明，Windows系统大同小异。</p>
<p>首先找到路径<code>/anaconda3/lib/python3.6/site-packages</code>，在该路径下创建一个文件<code>XXX.pth</code>，该文件的扩展名必须为<code>pth</code>，文件名称可以随意。然后在该文件中输入你希望Python搜索包的绝对路径即可。</p>
<p>设置完搜索路径后，我们需要修改一下PyCharm中的目录结构：<br><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/42bf4dc0e2e8cf0d11afd17037d16b59.jpg" alt=""></p>
<p>我新建了一个目录名为<code>myML</code>，<code>kNN.py</code>是我们之前封装的kNN算法相关的方法，<code>modelSelection.py</code>里就是我们刚才封装好的拆分训练和测试数据的方法，另外还增加了一个<code>__init__.py</code>的文件，因为有了这个文件，<code>myML</code>就变为了一个包。<code>__init__.py</code>的作用这里不做过多解释。</p>
<p>这样我们就可以在Jupyter Notebook中用<code>import</code>的方式导入我们封装的模块了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> myML.modelSelection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, y_train, X_test, y_test = train_test_split(X, y)</span><br><span class="line">X_train.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">120</span>, <span class="number">4</span>)</span><br><span class="line">y_train.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">120</span>,)</span><br><span class="line">X_test.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">30</span>, <span class="number">4</span>)</span><br><span class="line">y_test.shape</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">(<span class="number">30</span>,)</span><br></pre></td></tr></table></figure>
<p>这样就可以很方便的使用我们封装的模块了，下面我们来看看怎么判断我们封装的kNN算法的好坏程度：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 先用训练数据训练模型，然后输入测试样本特征数据，得到预测结果</span></span><br><span class="line"><span class="keyword">from</span> myML.kNN <span class="keyword">import</span> KNNClassifier</span><br><span class="line">my_knn_classifier = KNNClassifier(<span class="number">6</span>)</span><br><span class="line">my_knn_classifier.fit(X_train, y_train)</span><br><span class="line">my_y_test = my_knn_classifier.predict(X_test)</span><br><span class="line">my_y_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">	   <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 用预测出的结果和测试样本分类结果数据做对比，得出准确率</span></span><br><span class="line">y_test</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">	   <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">	   </span><br><span class="line">sum(my_y_test == y_test) / len(y_test)</span><br><span class="line"><span class="comment"># 结果，准确率为96.67%</span></span><br><span class="line"><span class="number">0.96666666666666667</span></span><br></pre></td></tr></table></figure>
<p>这样我们就得出了一个算法的好坏程度。</p>
<h2 id="u8D85_u53C2_u6570"><a href="#u8D85_u53C2_u6570" class="headerlink" title="超参数"></a>超参数</h2><p>目前我们在使用kNN算法时，<code>k</code>的值都是我们给定的值，这个作为算法的参数值称为超参数，也就是在运行机器学习算法之前需要指定的参数。还有一类参数称为模型参数，既在算法过程中学习的参数，但是大家已经知道kNN算法实际是没有模型的，所以也不存在模型参数，但是<code>k</code>值是一个典型的超参数。</p>
<h3 id="u5BFB_u627E_u6700_u597D_u7684k_u503C"><a href="#u5BFB_u627E_u6700_u597D_u7684k_u503C" class="headerlink" title="寻找最好的k值"></a>寻找最好的k值</h3><p>Scikit Learn中kNN算法的<code>k</code>值默认是5，有时候这个值并不是最优的值，那么我们可以通过一个简单的方式来寻找到最优的<code>k</code>值，那就是给定一个<code>k</code>值的范围，然后循环传入算法求训练分数最好的那个<code>k</code>值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先我们使用scikit learn中的手写数字数据集，并将其拆分为训练数据集和测试数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后通过循环的方式寻找最好的k值</span></span><br><span class="line">best_score = <span class="number">0.0</span></span><br><span class="line">best_k = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">	knn_clf = KNeighborsClassifier(n_neighbors = k)</span><br><span class="line">	knn_clf.fit(X_train, y_train)</span><br><span class="line">	score = knn_clf.score(X_test, y_test)</span><br><span class="line">	<span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">		best_k = k</span><br><span class="line">		best_score = score</span><br><span class="line">		</span><br><span class="line">print(<span class="string">"best_k = "</span>, best_k)</span><br><span class="line">print(<span class="string">"best_score = "</span>, best_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">best_k =  <span class="number">4</span></span><br><span class="line">best_score =  <span class="number">0.991666666667</span></span><br></pre></td></tr></table></figure>
<p>从上面的代码示例中可以看到，在1到10这个范围的<code>k</code>值中，4是训练分数最高的<code>k</code>值。不过这里需要注意的是，如果求出<code>k</code>为10，那么我们需要再扩大范围进行寻找，因为有可能10并不是最优的<code>k</code>值，只因为我们给定的范围最大到10，所以这种情况下，我们需要根据实际情况对8至20的<code>k</code>值范围再进行计算，如果结果仍然为10，那么才认定10为最优<code>k</code>值。</p>
<h3 id="u8DDD_u79BB_u7684_u6743_u91CD"><a href="#u8DDD_u79BB_u7684_u6743_u91CD" class="headerlink" title="距离的权重"></a>距离的权重</h3><p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/b29f73d52a86bfcc04a919471f6e730f.jpg" alt=""></p>
<p>上面这张图如果用之前我们了解过的kNN算法来分析的话，绿色的点肯定是属于蓝色点分类的，但是我们之前都一直忽略了一个问题，<strong>那就是当找到<code>k</code>个相邻的点后，在投票时是没有再考虑未知分类点与相邻点之间的距离的</strong>。就比如上图，如果考虑了3个最近相邻点与绿色点之间的距离的话，那么绿色点的分类就会属于红色点的分类，因为在计算距离权重时是取距离的倒数，所以绿色点与红色点的距离权重为1，绿色点与两个蓝色点的距离权重为1/3 + 1/4 = 7/12。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/53a1c3214fa97bb5c905e80898cbad63.jpg" alt=""></p>
<p>上图的情况如果不考虑距离权重的话，就会出现平票的情况，那么只能随机在三个分类中选一个作为绿色点的分类，如果加上距离权重，就能确定得出绿色点的分类了。</p>
<p>所以与相邻点的距离权重是kNN算法的另一个重要的超参数，大家可以看一下Scikit Learn的<a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" target="_blank" rel="external">kNN</a>官网，<code>KNeighborsClassifier</code>的构造函数中有一个参数<code>weights</code>，这就是距离权重参数，默认值为<code>uniform</code>，既不考虑距离权重，如果要考虑距离权重的话，需要设置值为<code>distance</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(n_neighbors = <span class="number">4</span>, weights = <span class="string">'distance'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="u8DDD_u79BB_u7684_u7C7B_u578B"><a href="#u8DDD_u79BB_u7684_u7C7B_u578B" class="headerlink" title="距离的类型"></a>距离的类型</h3><p>到目前为止，我们自己封装的kNN算法使用的距离公式是欧拉距离，其实还有其他的距离公式，比如<strong>曼哈顿距离</strong>:</p>
<p>$$\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|$$</p>
<p>其实曼哈顿距离和欧拉距离在数学公式表现形式上是有一定相似性的，我们可以将欧拉距离做以转换：</p>
<p>$$ \sqrt {\sum_{i=1}^n(x_i^{(a)}-x_i^{(b)})^2} = \sqrt {\sum_{i=1}^n|x_i^{(a)}-x_i^{(b)}|^2} = (\sum_{i=1}^n|x_i^{(a)}-x_i^{(b)}|^2)^\frac 1 2 $$ </p>
<p>对曼哈顿距离也做以转换：</p>
<p>$$ \sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}| = （\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|^1）^\frac 1 1 $$</p>
<p>通过上面两个公式可以得到一个共性的公式：</p>
<p>$$ （\sum_{i=1}^n |X_i^{(a)}-X_i^{(b)}|^p）^\frac 1 p $$</p>
<p>这个公式就称之为<strong>明可夫斯基距离（Minkowski Distance）</strong>。</p>
<p>既当<code>p</code>为1时为曼哈顿距离，当<code>p</code>为2时为欧拉距离，当<code>p</code>大于2时表示其他距离，所以<code>p</code>又是一个kNN算法的超参数，在<code>KNeighborsClassifier</code>的构造函数中同样有一个参数<code>p</code>就是表示使用的距离类型，默认为2，既默认为欧拉距离。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_score = <span class="number">0.0</span></span><br><span class="line">best_k = -<span class="number">1</span></span><br><span class="line">best_p = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">	<span class="keyword">for</span> p <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">		knn_clf = KNeighborsClassifier(n_neighbors = k, weights = <span class="string">"distance"</span>, p = p)</span><br><span class="line">		knn_clf.fit(X_train, y_train)</span><br><span class="line">		score = knn_clf.score(X_test, y_test)</span><br><span class="line">		<span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">			best_k = k</span><br><span class="line">			best_score = score</span><br><span class="line">			best_p = p</span><br><span class="line"></span><br><span class="line">print(<span class="string">"best_p = "</span>, best_p)            </span><br><span class="line">print(<span class="string">"best_k = "</span>, best_k)</span><br><span class="line">print(<span class="string">"best_score = "</span>, best_score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">best_p =  <span class="number">2</span></span><br><span class="line">best_k =  <span class="number">3</span></span><br><span class="line">best_score =  <span class="number">0.988888888889</span></span><br></pre></td></tr></table></figure>
<p>从上面代码运行的结果来看，最优的<code>p</code>值为2，也就是欧拉距离，考虑了距离权重后，最优<code>k</code>值为3。而且一些超参数是组合使用的，比如当使用超参数<code>p</code>时，距离权重的超参数<code>weights</code>的取值就必须是<code>distance</code>。并且<code>k</code>和<code>p</code>这两个超参数双重嵌套循环，就组成了一个类似网格的搜索方式，所幸Scikit Learn提供了封装好的网格搜索的方法供我们使用。</p>
<h2 id="u7F51_u683C_u641C_u7D22_u8D85_u53C2_u6570"><a href="#u7F51_u683C_u641C_u7D22_u8D85_u53C2_u6570" class="headerlink" title="网格搜索超参数"></a>网格搜索超参数</h2><p>在使用网格搜索前，我们需要先将各种超参数的组合定义出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = [</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="string">'weights'</span>: [<span class="string">'uniform'</span>],</span><br><span class="line">		<span class="string">'n_neighbors'</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>)]</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="string">'weights'</span>: [<span class="string">'distance'</span>],</span><br><span class="line">		<span class="string">'n_neighbors'</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>)],</span><br><span class="line">		<span class="string">'p'</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">	&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>我们定义了一个<code>param_grid</code>数组，元素为字典，每个字典描述了一种超参数的组合，下面我们使用Scikit Learn提供的<code>GridSearchCV</code>来使用我们定义好的超参数组合：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line">new_knn_clf = grid_search.best_estimator_</span><br><span class="line">new_knn_clf</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">KNeighborsClassifier(algorithm=<span class="string">'auto'</span>, leaf_size=<span class="number">30</span>, metric=<span class="string">'minkowski'</span>,</span><br><span class="line">		   metric_params=<span class="keyword">None</span>, n_jobs=<span class="number">1</span>, n_neighbors=<span class="number">3</span>, p=<span class="number">3</span>,</span><br><span class="line">		   weights=<span class="string">'distance'</span>)</span><br></pre></td></tr></table></figure>
<p>上面的示例代码不难理解，我们使用构建出的kNN分类器<code>knn_clf</code>和超参数组合<code>param_grid</code>构造出了网格搜索对象<code>grid_search</code>，通过它进行<code>fit</code>操作，这个过程就是根据我们提供的超参数组合进行搜寻，找到最优的超参数组合。通过<code>best_estimator_</code>返回新的，已经设置了最优超参数组合的kNN分类器对象。从输出结果其实已经可以看到首先是选择了考虑距离权重的超参数组合，然后求出了<code>k</code>值，也就是<code>n_neighbors</code>为3，<code>p</code>值为3。</p>
<p><code>GridSearchCV</code>也提供了几个属性，可以让我们方便的查看超参数和模型评分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid_search.best_params_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line">&#123;<span class="string">'n_neighbors'</span>: <span class="number">3</span>, <span class="string">'p'</span>: <span class="number">3</span>, <span class="string">'weights'</span>: <span class="string">'distance'</span>&#125;</span><br><span class="line"></span><br><span class="line">grid_search.best_score_</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.98538622129436326</span></span><br></pre></td></tr></table></figure>
<h3 id="GridSearchCV_u7684_u5176_u4ED6_u53C2_u6570"><a href="#GridSearchCV_u7684_u5176_u4ED6_u53C2_u6570" class="headerlink" title="GridSearchCV的其他参数"></a>GridSearchCV的其他参数</h3><p>在构造<code>GridSearchCV</code>对象时除了kNN分类器和超参数组合外，还有几个比较有用的参数：</p>
<ul>
<li><code>n_jobs</code>：该参数决定了在进行网格搜索时使用当前计算机的CPU核数，1就是使用1个核，2就是使用2个核，如果设置为-1，那么代表使用所有的核进行搜索。</li>
<li><code>verbose</code>：该参数决定了在网格搜索时的日志输出级别。</li>
</ul>
<h2 id="u6570_u636E_u5F52_u4E00_u5316"><a href="#u6570_u636E_u5F52_u4E00_u5316" class="headerlink" title="数据归一化"></a>数据归一化</h2><p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/43b6730765831fcc1dbecb24069ed177.jpg" alt=""></p>
<p>大家先看看上面表格中的样本数据，两个样本的肿瘤大小相差有5倍，从医学角度来讲这个差距已经是非常大了，但从实际数值差距来讲并不是很大。再看看发现时间，两个样本之间相差100天，在数值上的差距远远大于肿瘤大小的差距。所以如果使用kNN算法，用欧拉距离计算的话，两个样本发现时间之差远远大于肿瘤大小之差，所以就会主导样本间的距离，这个显然是有问题的，对预测的结果是有偏差的。</p>
<p>所以我们就需要对样本数据进行数据归一化，将所有的数据映射到同一尺度。比较简便的方式就是<strong>最值归一化</strong>，既用下面的公式把所有数据映射到0-1之间：</p>
<p>$$ x_{scale} = \frac {x - x_{min}} {x_{max} - x_{min}} $$</p>
<p>最值归一化虽然简便，但是是有一定适用范围的，那就是适用于样本数据有明显分布边界的情况，比如学生的考试分数，从0到100分，或者像素值，从0到255等。假如像人的月收入这种没有边界的样本数据集，就不能使用最值归一化了，此时就需要用到另外一个数据归一化的方法<strong>均值方差归一化</strong>，该方法就是把所有数据归一到均值为0方差为1的分布中，公式如下：</p>
<p>$$ x_{scale} = \frac {x - x_{mean}} S $$</p>
<p>就是将每个值减去均值，然后除以方差，通过均值方差归一化后的数据不一定在0-1之间，但是他们的均值为0，方差为1。</p>
<p>下面我们来分别实现一下这两个数据归一化方法。先来看看最值归一化的实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成从0到100，一共100个元素的数组</span></span><br><span class="line">x = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, size = <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 变更数组元素的类型</span></span><br><span class="line">x = np.array(x, dtype = float)</span><br><span class="line">x_scale = (x - np.min(x)) / (np.max(x) - np.min(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成50行，2列的矩阵，元素在0到100之间</span></span><br><span class="line">X = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 对每一列数据进行最值归一化</span></span><br><span class="line">X[:, <span class="number">0</span>] = (X[:, <span class="number">0</span>] - np.min(X[:, <span class="number">0</span>])) / (np.max(X[:, <span class="number">0</span>]) - np.min(X[:, <span class="number">0</span>]))</span><br><span class="line">X[:, <span class="number">1</span>] = (X[:, <span class="number">1</span>] - np.min(X[:, <span class="number">1</span>])) / (np.max(X[:, <span class="number">1</span>]) - np.min(X[:, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用matplotlib将X展示出来</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/96cc0403babcacae118a05b14b5d9730.jpg" alt=""></p>
<p>可以看到最值归一化后数据都在0到1之间。我们再来看看均值方差归一化的实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X2 = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line">X2 = np.array(X2, dtype = float)</span><br><span class="line">X2[:, <span class="number">0</span>] = (X2[:, <span class="number">0</span>] - np.mean(X2[:, <span class="number">0</span>])) / np.std(X2[:, <span class="number">0</span>])</span><br><span class="line">X2[:, <span class="number">1</span>] = (X2[:, <span class="number">1</span>] - np.mean(X2[:, <span class="number">1</span>])) / np.std(X2[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 均值接近0</span></span><br><span class="line">np.mean(X2[:, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">6.2172489379008772e-17</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方差接近1</span></span><br><span class="line">np.std(X2[:, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 结果</span></span><br><span class="line"><span class="number">0.99999999999999989</span></span><br><span class="line"></span><br><span class="line">plt.scatter(X2[:, <span class="number">0</span>], X2[:, <span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/f4dbe1f0c876372c0263013bd3e27043.jpg" alt=""></p>
<h2 id="u5982_u4F55_u5BF9_u6D4B_u8BD5_u6570_u636E_u96C6_u8FDB_u884C_u5F52_u4E00_u5316"><a href="#u5982_u4F55_u5BF9_u6D4B_u8BD5_u6570_u636E_u96C6_u8FDB_u884C_u5F52_u4E00_u5316" class="headerlink" title="如何对测试数据集进行归一化"></a>如何对测试数据集进行归一化</h2><p>之前我们说过会对样本数据进行拆分，拆分为训练数据和测试数据，对于训练数据我们可以直接使用最值归一化或均值方法归一化，但是对测试数据我们就不能直接使用归一化的方法了，因为测试数据其实充当了真实环境中需要预测的数据，很多时候需要预测的数据只有一组，这时候我们是没办法对一组数据进行归一化的，因为无法得到均值和方差，所以我们需要结合归一化后训练数据归一化测试数据：<code>(x_test - mean_train) / std_train</code>。那么我们就需要保存训练数据归一化后的数据，此时我们就可以用到Scikit Learn提供的数据归一化的对象<code>Scalar</code>。</p>
<p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/c962c7f93465af5cff9be84f884c97fd.jpg" alt=""></p>
<p><code>Scalar</code>的使用流程和机器学习算法的使用流程很像，输入训练数据集，进行<code>fit</code>操作，这里的<code>fit</code>操作就不是训练模型了，而是进行数据归一化处理，然后是<code>transform</code>，既对需要预测的数据进行归一化。我们来看看如何使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用鸢尾花数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割出训练数据集和测试数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.2</span>, random_state = <span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入StandardScaler，也就是均值方差归一化的对象</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">standardScaler = StandardScaler()</span><br><span class="line">standardScaler.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将特征训练数据集和特征测试数据集进行归一化处理</span></span><br><span class="line">X_train_standard = standardScaler.transform(X_train)</span><br><span class="line">X_test_standard = standardScaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用kNN</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors = <span class="number">3</span>)</span><br><span class="line">knn_clf.fit(X_train_standard, y_train)</span><br><span class="line">knn_clf.score(X_test_standard, y_test)</span><br></pre></td></tr></table></figure>
<h2 id="u5C01_u88C5_u81EA_u5DF1_u7684_u6570_u636E_u5F52_u4E00_u5316_u65B9_u6CD5"><a href="#u5C01_u88C5_u81EA_u5DF1_u7684_u6570_u636E_u5F52_u4E00_u5316_u65B9_u6CD5" class="headerlink" title="封装自己的数据归一化方法"></a>封装自己的数据归一化方法</h2><p><img src="https://devtalking.oss-cn-beijing.aliyuncs.com/a446676198ff1ff737c340099f48c7cb.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StandardScaler</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		self.mean_ = <span class="keyword">None</span></span><br><span class="line">		self.scaler_ = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取训练数据集的平均值和方差</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X.ndim == <span class="number">2</span>, <span class="string">"X 的维度必须为2，既X是一个矩阵"</span></span><br><span class="line"></span><br><span class="line">		self.mean_ = np.array([np.mean(X[:, i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])])</span><br><span class="line">		self.scaler_ = np.array([np.std(X[:, i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X.shape[<span class="number">1</span>])])</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 进行均值方差归一化处理</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">		<span class="keyword">assert</span> X.ndim == <span class="number">2</span>, <span class="string">"X 的维度必须为2，既X是一个矩阵"</span></span><br><span class="line">		<span class="keyword">assert</span> self.mean_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.scaler_ <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">"均值和方差不能为空"</span></span><br><span class="line">		<span class="keyword">assert</span> X.shape[<span class="number">1</span>] == len(self.mean_), <span class="string">"训练数据集矩阵的列数必须等于均值数组的元素个数"</span></span><br><span class="line">		<span class="keyword">assert</span> X.shape[<span class="number">1</span>] == len(self.scaler_), <span class="string">"训练数据集矩阵的列数必须等于方差数组的元素个数"</span></span><br><span class="line"></span><br><span class="line">		X_transform = np.empty(shape=X.shape, dtype=float)</span><br><span class="line">		<span class="keyword">for</span> col <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">			X_transform[:, col] = (X[:, col] - self.mean_[col]) / self.scaler_[col]</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> X_transform</span><br></pre></td></tr></table></figure>
<p>这样我们就封装好了自己的均值方差归一化的方法，另外，Scikit Learn也提供了最值归一化的对象<code>MinMaxScaler</code>，使用流程都是一样的，大家也可是试试看。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>这一篇笔记主要介绍了kNN算法实现逻辑以外的概念，但也是机器学习中非常重要的一些概念，以后也会经常看到它们的身影。通过两篇笔记的介绍，我们知道kNN算法是一个解决多分类问题的算法，而且算法实现相对比较简单，但效果很强大。下一篇我们来实现第一篇笔记中介绍过的线性回归法。</p>
<blockquote>
<p>申明：本文为慕课网<a href="https://www.imooc.com/t/108955" target="_blank" rel="external">liuyubobobo</a>老师<a href="https://coding.imooc.com/learn/list/169.html" target="_blank" rel="external">《Python3入门机器学习 经典算法与应用》</a>课程的学习笔记，未经允许不得转载。</p>
</blockquote>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- DevTalking Banner1 -->
<ins class="adsbygoogle" style="display:inline-block;width:728px;height:90px" data-ad-client="ca-pub-4115205380866695" data-ad-slot="5844761160"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/数据归一化/" rel="tag">#数据归一化</a>
          
            <a href="/tags/机器学习/" rel="tag">#机器学习</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/articles/machine-learning-3/" rel="next" title="机器学习笔记三之NumPy、Matplotlib、kNN算法">
                <i class="fa fa-chevron-left"></i> 机器学习笔记三之NumPy、Matplotlib、kNN算法
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/articles/machine-learning-5/" rel="prev" title="机器学习笔记五之线性回归、评测标准、多元线性回归">
                机器学习笔记五之线性回归、评测标准、多元线性回归 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!--MOB SHARE BEGIN-->
<div class="-hoofoo-share-title">分享到：</div>
<div class="-hoofoo-share-buttons">
    <div class="-mob-share-weibo -hoofoo-share-weibo -hoofoo-share-ui-button"><i class="fa fa-weibo" aria-hidden="true"></i></div>
    <div class="-mob-share-weixin -hoofoo-share-weixin -hoofoo-share-ui-button"><i class="fa fa-weixin" aria-hidden="true"></i></div>
    <div class="-mob-share-twitter -hoofoo-share-twitter -hoofoo-share-ui-button"><i class="fa fa-twitter" aria-hidden="true"></i></div>
    <div class="-hoofoo-share-more -hoofoo-share-ui-button -mob-share-open"><i class="fa fa-ellipsis-h" aria-hidden="true"></i></div>
</div><div class="-mob-share-ui -mob-share-ui-theme -mob-share-ui-theme-slide-left" style="display: none">
    <ul class="-mob-share-list">
        <li class="-mob-share-weixin"><p>微信</p></li>
        <li class="-mob-share-pocket"><p>Pocket</p></li>
        <li class="-mob-share-instapaper"><p>Instapaper</p></li>
        <li class="-mob-share-linkedin"><p>Linkedin</p></li>
        <li class="-mob-share-twitter"><p>Twitter</p></li>
        <li class="-mob-share-weibo"><p>新浪微博</p></li>
        <li class="-mob-share-douban"><p>豆瓣</p></li>
        <li class="-mob-share-facebook"><p>Facebook</p></li>
        <li class="-mob-share-google"><p>Google+</p></li>
    </ul>
    <div class="-mob-share-close">取消</div>
</div>
<div class="-mob-share-ui-bg"></div>
<script id="-mob-share" src="http://f1.webshare.mob.com/code/mob-share.js?appkey=26252752de4d6"></script>
<!--MOB SHARE END--> 
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
              <div id="disqus_thread">
                <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
              </div>
            
          </div>
        
      </div>

      
        
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目錄
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            本站概覽
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://www.devtalking.com/devtalking.png" alt="DevTalking" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DevTalking</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">122</span>
              <span class="site-state-item-name">文章</span>
            </a>
          </div>

         <!-- <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">11</span>
              <span class="site-state-item-name">分類</span>
              </a>
          </div> -->

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">94</span>
              <span class="site-state-item-name">標籤</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="menu-item-icon icon-next-feed"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/宇轩-付-5aa406a6" target="_blank">
                  
                    <i class="fa fa-linkedin"></i> linkedin
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:jace.fu@icloud.com" target="_blank">
                  
                    <i class="fa fa-envelope"></i> Email
                  
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#u5C01_u88C5kNN_u7B97_u6CD5"><span class="nav-number">1.</span> <span class="nav-text">封装kNN算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u673A_u5668_u5B66_u4E60_u6D41_u7A0B"><span class="nav-number">2.</span> <span class="nav-text">机器学习流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u4F7F_u7528Scikit_Learn_u4E2D_u7684kNN_u7B97_u6CD5"><span class="nav-number">3.</span> <span class="nav-text">使用Scikit Learn中的kNN算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u91CD_u65B0_u5C01_u88C5kNN_u7B97_u6CD5"><span class="nav-number">4.</span> <span class="nav-text">重新封装kNN算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u5224_u65AD_u673A_u5668_u5B66_u4E60_u7B97_u6CD5_u7684_u6027_u80FD"><span class="nav-number">5.</span> <span class="nav-text">判断机器学习算法的性能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u8D85_u53C2_u6570"><span class="nav-number">6.</span> <span class="nav-text">超参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#u5BFB_u627E_u6700_u597D_u7684k_u503C"><span class="nav-number">6.1.</span> <span class="nav-text">寻找最好的k值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#u8DDD_u79BB_u7684_u6743_u91CD"><span class="nav-number">6.2.</span> <span class="nav-text">距离的权重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#u8DDD_u79BB_u7684_u7C7B_u578B"><span class="nav-number">6.3.</span> <span class="nav-text">距离的类型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u7F51_u683C_u641C_u7D22_u8D85_u53C2_u6570"><span class="nav-number">7.</span> <span class="nav-text">网格搜索超参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GridSearchCV_u7684_u5176_u4ED6_u53C2_u6570"><span class="nav-number">7.1.</span> <span class="nav-text">GridSearchCV的其他参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u6570_u636E_u5F52_u4E00_u5316"><span class="nav-number">8.</span> <span class="nav-text">数据归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u5982_u4F55_u5BF9_u6D4B_u8BD5_u6570_u636E_u96C6_u8FDB_u884C_u5F52_u4E00_u5316"><span class="nav-number">9.</span> <span class="nav-text">如何对测试数据集进行归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u5C01_u88C5_u81EA_u5DF1_u7684_u6570_u636E_u5F52_u4E00_u5316_u65B9_u6CD5"><span class="nav-number">10.</span> <span class="nav-text">封装自己的数据归一化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#u603B_u7ED3"><span class="nav-number">11.</span> <span class="nav-text">总结</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


      
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DevTalking</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  

    <script type="text/javascript">
      var disqus_shortname = 'jacefu';
      var disqus_identifier = '/articles/machine-learning-4/';
      var disqus_title = '机器学习笔记四之kNN算法、超参数、数据归一化';
      var disqus_url = 'http://www.devtalking.com//articles/machine-learning-4/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  


  
  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  

  <script type="text/javascript" src="/lib/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.2" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }

      motionIntegrator.bootstrap();
    });
  </script>

  
  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
